/home/work/miniconda3/lib/python3.12/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.22). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/work/.local/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
Namespace(num_epochs=50, workers=4, resume='', dataset='All', max_videos=-1, config='configs/config.yaml', efficient_net=0, patience=5) 234823489349394
opt : configs/config.yaml
Loaded pretrained weights for efficientnet-b0
config: {'config': 'config.yaml', 'dataset': 'All', 'efficient_net': 0, 'max_videos': -1, 'num_epochs': 50, 'patience': 5, 'resume': '', 'workers': 4, 'training': {'lr': 0.001, 'weight-decay': 1e-07, 'bs': 32, 'scheduler': 'steplr', 'gamma': 0.1, 'step-size': 15, 'rebalancing_fake': 0.3, 'rebalancing_real': 1, 'frames-per-video': 30}, 'model': {'image-size': 224, 'patch-size': 7, 'num-classes': 1, 'dim': 1024, 'depth': 6, 'dim-head': 64, 'heads': 8, 'mlp-dim': 2048, 'emb-dim': 32, 'dropout': 0.15, 'emb-dropout': 0.15}}
optimizer: SGD (
Parameter Group 0
    dampening: 0
    differentiable: False
    foreach: None
    lr: 0.001
    maximize: False
    momentum: 0
    nesterov: False
    weight_decay: 1e-07
)
No checkpoint loaded.
Model Parameters: 72719589
  0%|          | 0/13630 [00:00<?, ?it/s]  0%|          | 1/13630 [00:00<31:46,  7.15it/s]  0%|          | 5/13630 [00:00<09:51, 23.05it/s]  0%|          | 9/13630 [00:00<07:45, 29.24it/s]  0%|          | 14/13630 [00:00<06:32, 34.69it/s]  0%|          | 18/13630 [00:00<06:14, 36.31it/s]  0%|          | 22/13630 [00:00<07:14, 31.35it/s]  0%|          | 26/13630 [00:00<07:23, 30.67it/s]  0%|          | 30/13630 [00:01<09:07, 24.84it/s]  0%|          | 33/13630 [00:01<09:00, 25.17it/s]  0%|          | 38/13630 [00:01<07:41, 29.47it/s]  0%|          | 42/13630 [00:01<08:36, 26.29it/s]  0%|          | 46/13630 [00:01<07:45, 29.17it/s]  0%|          | 50/13630 [00:01<07:17, 31.07it/s]  0%|          | 54/13630 [00:01<06:51, 32.98it/s]  0%|          | 58/13630 [00:01<06:37, 34.13it/s]  0%|          | 62/13630 [00:02<06:29, 34.80it/s]  0%|          | 66/13630 [00:02<07:06, 31.78it/s]  1%|          | 70/13630 [00:02<06:41, 33.80it/s]  1%|          | 74/13630 [00:02<07:06, 31.78it/s]  1%|          | 78/13630 [00:02<06:40, 33.84it/s]  1%|          | 82/13630 [00:02<06:58, 32.35it/s]  1%|          | 86/13630 [00:02<06:59, 32.26it/s]  1%|          | 91/13630 [00:02<06:27, 34.98it/s]  1%|          | 95/13630 [00:03<06:33, 34.44it/s]  1%|          | 99/13630 [00:03<06:28, 34.80it/s]  1%|          | 103/13630 [00:03<06:45, 33.32it/s]  1%|          | 108/13630 [00:03<07:07, 31.66it/s]  1%|          | 113/13630 [00:03<06:57, 32.35it/s]  1%|          | 117/13630 [00:03<07:07, 31.59it/s]  1%|          | 122/13630 [00:03<06:26, 34.97it/s]  1%|          | 126/13630 [00:03<06:27, 34.85it/s]  1%|          | 130/13630 [00:04<06:44, 33.34it/s]  1%|          | 134/13630 [00:04<06:55, 32.48it/s]  1%|          | 138/13630 [00:04<07:14, 31.07it/s]  1%|          | 142/13630 [00:04<07:21, 30.54it/s]  1%|          | 146/13630 [00:04<07:17, 30.79it/s]  1%|          | 150/13630 [00:04<06:50, 32.82it/s]  1%|          | 154/13630 [00:04<07:13, 31.06it/s]  1%|          | 158/13630 [00:05<07:07, 31.54it/s]  1%|          | 162/13630 [00:05<07:27, 30.07it/s]  1%|          | 167/13630 [00:05<06:40, 33.60it/s]  1%|▏         | 171/13630 [00:05<06:25, 34.88it/s]  1%|▏         | 176/13630 [00:05<06:40, 33.56it/s]  1%|▏         | 180/13630 [00:05<06:59, 32.09it/s]  1%|▏         | 184/13630 [00:05<07:07, 31.45it/s]  1%|▏         | 189/13630 [00:05<06:29, 34.54it/s]  1%|▏         | 193/13630 [00:06<06:16, 35.69it/s]  1%|▏         | 198/13630 [00:06<05:52, 38.14it/s]  1%|▏         | 203/13630 [00:06<06:24, 34.91it/s]  2%|▏         | 207/13630 [00:06<06:11, 36.10it/s]  2%|▏         | 211/13630 [00:06<06:29, 34.49it/s]  2%|▏         | 216/13630 [00:06<06:07, 36.49it/s]  2%|▏         | 220/13630 [00:06<06:06, 36.54it/s]  2%|▏         | 224/13630 [00:06<06:30, 34.36it/s]  2%|▏         | 228/13630 [00:07<06:41, 33.41it/s]  2%|▏         | 232/13630 [00:07<06:59, 31.93it/s]  2%|▏         | 236/13630 [00:07<07:29, 29.78it/s]  2%|▏         | 240/13630 [00:07<07:24, 30.10it/s]  2%|▏         | 244/13630 [00:07<06:55, 32.20it/s]  2%|▏         | 248/13630 [00:07<07:17, 30.59it/s]  2%|▏         | 252/13630 [00:07<07:45, 28.74it/s]  2%|▏         | 255/13630 [00:07<07:46, 28.67it/s]  2%|▏         | 258/13630 [00:08<09:51, 22.62it/s]  2%|▏         | 261/13630 [00:08<09:36, 23.21it/s]  2%|▏         | 265/13630 [00:08<08:30, 26.17it/s]  2%|▏         | 268/13630 [00:08<08:16, 26.89it/s]  2%|▏         | 272/13630 [00:08<07:41, 28.92it/s]  2%|▏         | 276/13630 [00:08<08:02, 27.65it/s]  2%|▏         | 279/13630 [00:08<08:12, 27.09it/s]  2%|▏         | 283/13630 [00:09<07:29, 29.71it/s]  2%|▏         | 287/13630 [00:09<07:21, 30.24it/s]  2%|▏         | 291/13630 [00:09<07:46, 28.60it/s]  2%|▏         | 295/13630 [00:09<07:12, 30.84it/s]  2%|▏         | 299/13630 [00:09<06:43, 33.04it/s]  2%|▏         | 303/13630 [00:09<07:02, 31.52it/s]  2%|▏         | 307/13630 [00:09<06:35, 33.66it/s]  2%|▏         | 311/13630 [00:09<07:50, 28.33it/s]  2%|▏         | 316/13630 [00:10<07:00, 31.67it/s]  2%|▏         | 320/13630 [00:10<06:35, 33.64it/s]  2%|▏         | 324/13630 [00:10<07:15, 30.52it/s]  2%|▏         | 329/13630 [00:10<06:34, 33.69it/s]  2%|▏         | 334/13630 [00:10<06:09, 35.99it/s]  2%|▏         | 339/13630 [00:10<05:44, 38.61it/s]  3%|▎         | 344/13630 [00:10<05:38, 39.29it/s]  3%|▎         | 349/13630 [00:10<05:32, 39.91it/s]  3%|▎         | 354/13630 [00:11<05:31, 40.07it/s]  3%|▎         | 359/13630 [00:11<06:08, 36.01it/s]  3%|▎         | 364/13630 [00:11<05:49, 37.91it/s]  3%|▎         | 368/13630 [00:11<05:46, 38.27it/s]  3%|▎         | 372/13630 [00:11<06:07, 36.05it/s]  3%|▎         | 376/13630 [00:11<06:38, 33.26it/s]  3%|▎         | 381/13630 [00:11<06:14, 35.37it/s]  3%|▎         | 385/13630 [00:11<06:20, 34.77it/s]  3%|▎         | 389/13630 [00:12<06:38, 33.25it/s]  3%|▎         | 393/13630 [00:12<06:51, 32.15it/s]  3%|▎         | 397/13630 [00:12<06:56, 31.76it/s]  3%|▎         | 401/13630 [00:12<06:34, 33.53it/s]  3%|▎         | 405/13630 [00:12<06:48, 32.35it/s]  3%|▎         | 409/13630 [00:12<07:03, 31.20it/s]  3%|▎         | 414/13630 [00:12<06:27, 34.14it/s]  3%|▎         | 418/13630 [00:13<07:04, 31.10it/s]  3%|▎         | 422/13630 [00:13<06:54, 31.87it/s]  3%|▎         | 426/13630 [00:13<07:03, 31.21it/s]  3%|▎         | 430/13630 [00:13<08:13, 26.75it/s]  3%|▎         | 434/13630 [00:13<07:25, 29.59it/s]  3%|▎         | 438/13630 [00:13<06:55, 31.76it/s]  3%|▎         | 443/13630 [00:13<06:24, 34.34it/s]  3%|▎         | 447/13630 [00:13<06:10, 35.60it/s]  3%|▎         | 451/13630 [00:14<06:11, 35.48it/s]  3%|▎         | 456/13630 [00:14<05:50, 37.63it/s]  3%|▎         | 461/13630 [00:14<06:06, 35.90it/s]  3%|▎         | 466/13630 [00:14<05:54, 37.13it/s]  3%|▎         | 471/13630 [00:14<05:42, 38.40it/s]  3%|▎         | 475/13630 [00:14<06:41, 32.79it/s]  4%|▎         | 479/13630 [00:14<07:06, 30.80it/s]  4%|▎         | 483/13630 [00:14<07:08, 30.71it/s]  4%|▎         | 488/13630 [00:15<06:24, 34.20it/s]  4%|▎         | 492/13630 [00:15<06:11, 35.32it/s]  4%|▎         | 496/13630 [00:15<06:24, 34.14it/s]  4%|▎         | 500/13630 [00:15<06:49, 32.04it/s]  4%|▎         | 504/13630 [00:15<06:38, 32.94it/s]  4%|▎         | 509/13630 [00:15<06:10, 35.44it/s]  4%|▍         | 513/13630 [00:15<06:09, 35.52it/s]  4%|▍         | 517/13630 [00:15<06:02, 36.18it/s]  4%|▍         | 521/13630 [00:16<06:11, 35.29it/s]  4%|▍         | 526/13630 [00:16<05:50, 37.43it/s]  4%|▍         | 531/13630 [00:16<05:37, 38.78it/s]  4%|▍         | 535/13630 [00:16<05:53, 37.06it/s]  4%|▍         | 540/13630 [00:16<05:36, 38.95it/s]  4%|▍         | 544/13630 [00:16<06:05, 35.77it/s]  4%|▍         | 549/13630 [00:16<05:41, 38.33it/s]  4%|▍         | 553/13630 [00:16<05:46, 37.73it/s]  4%|▍         | 558/13630 [00:16<05:34, 39.06it/s]  4%|▍         | 562/13630 [00:17<05:34, 39.07it/s]  4%|▍         | 566/13630 [00:17<05:45, 37.84it/s]  4%|▍         | 571/13630 [00:17<05:39, 38.47it/s]  4%|▍         | 575/13630 [00:17<06:02, 35.99it/s]  4%|▍         | 579/13630 [00:17<06:19, 34.36it/s]  4%|▍         | 584/13630 [00:17<07:04, 30.73it/s]  4%|▍         | 588/13630 [00:17<06:51, 31.72it/s]  4%|▍         | 592/13630 [00:18<07:20, 29.62it/s]  4%|▍         | 597/13630 [00:18<06:39, 32.59it/s]  4%|▍         | 601/13630 [00:18<07:03, 30.77it/s]  4%|▍         | 605/13630 [00:18<07:26, 29.18it/s]  4%|▍         | 609/13630 [00:18<07:00, 30.98it/s]  4%|▍         | 613/13630 [00:18<06:38, 32.67it/s]  5%|▍         | 617/13630 [00:18<06:48, 31.88it/s]  5%|▍         | 621/13630 [00:18<06:41, 32.37it/s]  5%|▍         | 626/13630 [00:19<06:03, 35.81it/s]  5%|▍         | 630/13630 [00:19<05:52, 36.85it/s]  5%|▍         | 635/13630 [00:19<05:33, 38.98it/s]  5%|▍         | 640/13630 [00:19<05:17, 40.95it/s]  5%|▍         | 645/13630 [00:19<05:39, 38.27it/s]  5%|▍         | 649/13630 [00:19<05:59, 36.08it/s]  5%|▍         | 653/13630 [00:19<06:23, 33.83it/s]  5%|▍         | 658/13630 [00:19<05:59, 36.13it/s]  5%|▍         | 663/13630 [00:20<05:36, 38.56it/s]  5%|▍         | 667/13630 [00:20<06:22, 33.93it/s]  5%|▍         | 671/13630 [00:20<06:18, 34.26it/s]  5%|▍         | 675/13630 [00:20<07:16, 29.69it/s]  5%|▍         | 679/13630 [00:20<07:08, 30.26it/s]  5%|▌         | 683/13630 [00:20<07:04, 30.53it/s]  5%|▌         | 687/13630 [00:20<07:09, 30.15it/s]  5%|▌         | 691/13630 [00:20<06:41, 32.24it/s]  5%|▌         | 696/13630 [00:21<06:01, 35.73it/s]  5%|▌         | 701/13630 [00:21<05:45, 37.46it/s]  5%|▌         | 705/13630 [00:21<06:02, 35.64it/s]  5%|▌         | 709/13630 [00:21<05:51, 36.75it/s]  5%|▌         | 714/13630 [00:21<05:38, 38.17it/s]  5%|▌         | 718/13630 [00:21<05:59, 35.96it/s]  5%|▌         | 723/13630 [00:21<05:46, 37.25it/s]  5%|▌         | 727/13630 [00:21<06:44, 31.92it/s]  5%|▌         | 732/13630 [00:22<06:14, 34.48it/s]  5%|▌         | 736/13630 [00:22<06:18, 34.09it/s]  5%|▌         | 740/13630 [00:22<06:31, 32.94it/s]  5%|▌         | 744/13630 [00:22<06:55, 31.00it/s]  5%|▌         | 748/13630 [00:22<06:30, 33.02it/s]  6%|▌         | 753/13630 [00:22<06:04, 35.28it/s]  6%|▌         | 757/13630 [00:22<05:53, 36.38it/s]  6%|▌         | 762/13630 [00:22<05:40, 37.79it/s]  6%|▌         | 767/13630 [00:23<05:33, 38.53it/s]  6%|▌         | 772/13630 [00:23<05:38, 37.96it/s]  6%|▌         | 776/13630 [00:23<05:59, 35.77it/s]  6%|▌         | 780/13630 [00:23<06:30, 32.90it/s]  6%|▌         | 784/13630 [00:23<06:45, 31.67it/s]  6%|▌         | 788/13630 [00:23<06:28, 33.07it/s]  6%|▌         | 793/13630 [00:23<06:00, 35.58it/s]  6%|▌         | 798/13630 [00:23<05:43, 37.37it/s]  6%|▌         | 803/13630 [00:24<05:32, 38.58it/s]  6%|▌         | 808/13630 [00:24<05:19, 40.08it/s]  6%|▌         | 813/13630 [00:24<05:51, 36.49it/s]  6%|▌         | 817/13630 [00:24<06:11, 34.47it/s]  6%|▌         | 821/13630 [00:24<06:15, 34.11it/s]  6%|▌         | 826/13630 [00:24<05:55, 35.99it/s]  6%|▌         | 831/13630 [00:24<05:39, 37.72it/s]  6%|▌         | 835/13630 [00:24<05:38, 37.78it/s]  6%|▌         | 839/13630 [00:25<06:02, 35.24it/s]  6%|▌         | 844/13630 [00:25<05:41, 37.43it/s]  6%|▌         | 848/13630 [00:25<06:30, 32.69it/s]  6%|▋         | 852/13630 [00:25<06:40, 31.90it/s]  6%|▋         | 857/13630 [00:25<06:03, 35.18it/s]  6%|▋         | 861/13630 [00:25<06:08, 34.67it/s]  6%|▋         | 866/13630 [00:25<05:40, 37.51it/s]  6%|▋         | 870/13630 [00:26<06:06, 34.86it/s]  6%|▋         | 874/13630 [00:26<06:23, 33.27it/s]  6%|▋         | 878/13630 [00:26<06:47, 31.30it/s]  6%|▋         | 883/13630 [00:26<06:15, 33.97it/s]  7%|▋         | 887/13630 [00:26<06:03, 35.04it/s]  7%|▋         | 892/13630 [00:26<06:10, 34.39it/s]  7%|▋         | 896/13630 [00:26<06:04, 34.89it/s]  7%|▋         | 900/13630 [00:26<06:31, 32.50it/s]  7%|▋         | 905/13630 [00:27<06:07, 34.65it/s]  7%|▋         | 909/13630 [00:27<05:58, 35.48it/s]  7%|▋         | 914/13630 [00:27<06:08, 34.52it/s]  7%|▋         | 919/13630 [00:27<05:55, 35.76it/s]  7%|▋         | 923/13630 [00:27<06:27, 32.80it/s]  7%|▋         | 927/13630 [00:27<06:11, 34.19it/s]  7%|▋         | 932/13630 [00:27<05:48, 36.42it/s]  7%|▋         | 937/13630 [00:27<05:45, 36.72it/s]  7%|▋         | 941/13630 [00:28<05:43, 36.92it/s]  7%|▋         | 945/13630 [00:28<05:36, 37.64it/s]  7%|▋         | 950/13630 [00:28<05:22, 39.32it/s]  7%|▋         | 954/13630 [00:28<05:46, 36.60it/s]  7%|▋         | 959/13630 [00:28<05:27, 38.65it/s]  7%|▋         | 963/13630 [00:28<05:27, 38.67it/s]  7%|▋         | 968/13630 [00:28<05:24, 39.05it/s]  7%|▋         | 973/13630 [00:28<05:16, 39.97it/s]  7%|▋         | 978/13630 [00:28<05:13, 40.40it/s]  7%|▋         | 983/13630 [00:29<06:49, 30.85it/s]  7%|▋         | 987/13630 [00:29<06:45, 31.14it/s]  7%|▋         | 991/13630 [00:29<07:35, 27.76it/s]  7%|▋         | 996/13630 [00:29<06:43, 31.32it/s]  7%|▋         | 1001/13630 [00:29<06:09, 34.15it/s]  7%|▋         | 1006/13630 [00:29<05:42, 36.91it/s]  7%|▋         | 1011/13630 [00:29<05:22, 39.15it/s]  7%|▋         | 1016/13630 [00:30<06:12, 33.88it/s]  7%|▋         | 1021/13630 [00:30<05:53, 35.66it/s]  8%|▊         | 1025/13630 [00:30<06:29, 32.40it/s]  8%|▊         | 1029/13630 [00:30<06:31, 32.21it/s]  8%|▊         | 1033/13630 [00:30<06:53, 30.48it/s]  8%|▊         | 1037/13630 [00:30<07:00, 29.98it/s]  8%|▊         | 1042/13630 [00:31<06:21, 33.02it/s]  8%|▊         | 1047/13630 [00:31<05:51, 35.77it/s]  8%|▊         | 1051/13630 [00:31<06:21, 32.96it/s]  8%|▊         | 1055/13630 [00:31<06:51, 30.58it/s]  8%|▊         | 1060/13630 [00:31<06:15, 33.48it/s]  8%|▊         | 1065/13630 [00:31<05:46, 36.22it/s]  8%|▊         | 1070/13630 [00:31<05:31, 37.86it/s]  8%|▊         | 1074/13630 [00:31<05:26, 38.40it/s]  8%|▊         | 1078/13630 [00:31<05:26, 38.41it/s]  8%|▊         | 1082/13630 [00:32<06:06, 34.26it/s]  8%|▊         | 1086/13630 [00:32<06:12, 33.65it/s]  8%|▊         | 1090/13630 [00:32<06:44, 31.03it/s]  8%|▊         | 1094/13630 [00:32<06:44, 31.03it/s]  8%|▊         | 1099/13630 [00:32<06:12, 33.64it/s]  8%|▊         | 1104/13630 [00:32<05:49, 35.83it/s]  8%|▊         | 1109/13630 [00:32<05:31, 37.76it/s]  8%|▊         | 1113/13630 [00:33<06:07, 34.09it/s]  8%|▊         | 1117/13630 [00:33<06:27, 32.27it/s]  8%|▊         | 1121/13630 [00:33<06:29, 32.10it/s]  8%|▊         | 1125/13630 [00:33<08:14, 25.27it/s]  8%|▊         | 1128/13630 [00:33<08:54, 23.41it/s]  8%|▊         | 1131/13630 [00:33<09:02, 23.03it/s]  8%|▊         | 1135/13630 [00:33<07:57, 26.15it/s]  8%|▊         | 1138/13630 [00:34<08:16, 25.16it/s]  8%|▊         | 1141/13630 [00:34<08:46, 23.72it/s]  8%|▊         | 1145/13630 [00:34<07:43, 26.96it/s]  8%|▊         | 1148/13630 [00:34<08:09, 25.51it/s]  8%|▊         | 1151/13630 [00:34<09:07, 22.78it/s]  8%|▊         | 1154/13630 [00:34<08:31, 24.40it/s]  8%|▊         | 1157/13630 [00:34<09:09, 22.71it/s]  9%|▊         | 1160/13630 [00:35<08:38, 24.06it/s]  9%|▊         | 1163/13630 [00:35<08:43, 23.81it/s]  9%|▊         | 1166/13630 [00:35<08:22, 24.78it/s]  9%|▊         | 1169/13630 [00:35<08:28, 24.53it/s]  9%|▊         | 1174/13630 [00:35<06:55, 29.98it/s]  9%|▊         | 1179/13630 [00:35<06:10, 33.60it/s]  9%|▊         | 1183/13630 [00:35<05:53, 35.25it/s]  9%|▊         | 1187/13630 [00:35<05:55, 35.04it/s]  9%|▊         | 1191/13630 [00:35<06:12, 33.38it/s]  9%|▉         | 1196/13630 [00:36<06:08, 33.71it/s]  9%|▉         | 1200/13630 [00:36<06:06, 33.93it/s]  9%|▉         | 1204/13630 [00:36<06:10, 33.51it/s]  9%|▉         | 1208/13630 [00:36<06:24, 32.30it/s]  9%|▉         | 1213/13630 [00:36<05:49, 35.48it/s]  9%|▉         | 1217/13630 [00:36<05:56, 34.86it/s]  9%|▉         | 1222/13630 [00:36<05:35, 36.96it/s]  9%|▉         | 1226/13630 [00:36<05:29, 37.62it/s]  9%|▉         | 1230/13630 [00:37<05:24, 38.23it/s]  9%|▉         | 1234/13630 [00:37<05:48, 35.56it/s]  9%|▉         | 1238/13630 [00:37<05:39, 36.55it/s]  9%|▉         | 1243/13630 [00:37<05:28, 37.73it/s]  9%|▉         | 1247/13630 [00:37<05:26, 37.89it/s]  9%|▉         | 1252/13630 [00:37<05:13, 39.48it/s]  9%|▉         | 1256/13630 [00:37<06:20, 32.54it/s]  9%|▉         | 1260/13630 [00:37<07:08, 28.84it/s]  9%|▉         | 1264/13630 [00:38<07:42, 26.77it/s]  9%|▉         | 1268/13630 [00:38<07:04, 29.15it/s]  9%|▉         | 1272/13630 [00:38<07:14, 28.44it/s]  9%|▉         | 1275/13630 [00:38<07:15, 28.38it/s]  9%|▉         | 1280/13630 [00:38<06:19, 32.55it/s]  9%|▉         | 1284/13630 [00:38<06:16, 32.78it/s]  9%|▉         | 1289/13630 [00:38<06:02, 34.04it/s]  9%|▉         | 1293/13630 [00:39<06:13, 33.02it/s] 10%|▉         | 1298/13630 [00:39<05:47, 35.45it/s] 10%|▉         | 1302/13630 [00:39<06:13, 32.98it/s] 10%|▉         | 1306/13630 [00:39<06:24, 32.07it/s] 10%|▉         | 1310/13630 [00:39<07:25, 27.64it/s] 10%|▉         | 1313/13630 [00:39<08:05, 25.34it/s] 10%|▉         | 1317/13630 [00:39<07:28, 27.46it/s] 10%|▉         | 1320/13630 [00:39<07:19, 28.04it/s] 10%|▉         | 1323/13630 [00:40<07:19, 27.99it/s] 10%|▉         | 1326/13630 [00:40<07:14, 28.31it/s] 10%|▉         | 1330/13630 [00:40<06:39, 30.81it/s] 10%|▉         | 1334/13630 [00:40<06:53, 29.72it/s] 10%|▉         | 1338/13630 [00:40<06:48, 30.09it/s] 10%|▉         | 1343/13630 [00:40<05:58, 34.28it/s] 10%|▉         | 1347/13630 [00:40<06:25, 31.88it/s] 10%|▉         | 1352/13630 [00:40<05:53, 34.74it/s] 10%|▉         | 1356/13630 [00:41<05:57, 34.35it/s] 10%|▉         | 1360/13630 [00:41<06:33, 31.20it/s] 10%|█         | 1365/13630 [00:41<05:58, 34.19it/s] 10%|█         | 1370/13630 [00:41<05:38, 36.18it/s] 10%|█         | 1374/13630 [00:41<05:56, 34.37it/s] 10%|█         | 1379/13630 [00:41<05:33, 36.77it/s] 10%|█         | 1383/13630 [00:41<05:56, 34.32it/s] 10%|█         | 1387/13630 [00:41<05:43, 35.60it/s] 10%|█         | 1392/13630 [00:42<05:24, 37.74it/s] 10%|█         | 1396/13630 [00:42<05:56, 34.30it/s] 10%|█         | 1401/13630 [00:42<05:34, 36.55it/s] 10%|█         | 1406/13630 [00:42<05:23, 37.77it/s] 10%|█         | 1411/13630 [00:42<05:08, 39.57it/s] 10%|█         | 1416/13630 [00:42<06:10, 32.94it/s] 10%|█         | 1420/13630 [00:42<06:23, 31.85it/s] 10%|█         | 1425/13630 [00:43<05:50, 34.84it/s] 10%|█         | 1430/13630 [00:43<05:33, 36.62it/s] 11%|█         | 1435/13630 [00:43<05:21, 37.91it/s] 11%|█         | 1439/13630 [00:43<05:40, 35.78it/s] 11%|█         | 1443/13630 [00:43<06:17, 32.31it/s] 11%|█         | 1448/13630 [00:43<05:44, 35.34it/s] 11%|█         | 1452/13630 [00:43<06:07, 33.18it/s] 11%|█         | 1457/13630 [00:43<05:47, 35.00it/s] 11%|█         | 1461/13630 [00:44<05:51, 34.58it/s] 11%|█         | 1466/13630 [00:44<05:53, 34.45it/s] 11%|█         | 1470/13630 [00:44<05:45, 35.25it/s] 11%|█         | 1474/13630 [00:44<05:50, 34.72it/s] 11%|█         | 1478/13630 [00:44<06:39, 30.46it/s] 11%|█         | 1482/13630 [00:44<06:23, 31.67it/s] 11%|█         | 1487/13630 [00:44<05:46, 35.06it/s] 11%|█         | 1492/13630 [00:44<05:28, 36.93it/s] 11%|█         | 1497/13630 [00:45<05:13, 38.71it/s] 11%|█         | 1502/13630 [00:45<05:18, 38.06it/s] 11%|█         | 1507/13630 [00:45<05:06, 39.57it/s] 11%|█         | 1512/13630 [00:45<04:57, 40.76it/s] 11%|█         | 1517/13630 [00:45<04:59, 40.39it/s] 11%|█         | 1522/13630 [00:45<05:25, 37.23it/s] 11%|█         | 1527/13630 [00:45<05:07, 39.40it/s] 11%|█         | 1532/13630 [00:45<05:32, 36.42it/s] 11%|█▏        | 1536/13630 [00:46<05:28, 36.83it/s] 11%|█▏        | 1541/13630 [00:46<05:13, 38.62it/s] 11%|█▏        | 1545/13630 [00:46<06:08, 32.77it/s] 11%|█▏        | 1550/13630 [00:46<05:41, 35.33it/s] 11%|█▏        | 1554/13630 [00:46<06:30, 30.96it/s] 11%|█▏        | 1558/13630 [00:46<07:21, 27.37it/s] 11%|█▏        | 1563/13630 [00:46<06:23, 31.49it/s] 11%|█▏        | 1567/13630 [00:47<06:22, 31.54it/s] 12%|█▏        | 1572/13630 [00:47<06:14, 32.19it/s] 12%|█▏        | 1577/13630 [00:47<05:48, 34.61it/s] 12%|█▏        | 1582/13630 [00:47<05:28, 36.64it/s] 12%|█▏        | 1587/13630 [00:47<05:24, 37.14it/s] 12%|█▏        | 1592/13630 [00:47<05:10, 38.77it/s] 12%|█▏        | 1597/13630 [00:47<04:57, 40.45it/s] 12%|█▏        | 1602/13630 [00:47<04:55, 40.69it/s] 12%|█▏        | 1607/13630 [00:48<04:53, 40.91it/s] 12%|█▏        | 1612/13630 [00:48<04:48, 41.70it/s] 12%|█▏        | 1617/13630 [00:48<04:45, 42.05it/s] 12%|█▏        | 1622/13630 [00:48<05:40, 35.23it/s] 12%|█▏        | 1626/13630 [00:48<05:43, 34.99it/s] 12%|█▏        | 1630/13630 [00:48<05:58, 33.47it/s] 12%|█▏        | 1634/13630 [00:48<05:52, 33.99it/s] 12%|█▏        | 1639/13630 [00:48<05:27, 36.59it/s] 12%|█▏        | 1643/13630 [00:49<05:51, 34.06it/s] 12%|█▏        | 1648/13630 [00:49<05:31, 36.12it/s] 12%|█▏        | 1653/13630 [00:49<05:33, 35.91it/s] 12%|█▏        | 1658/13630 [00:49<05:14, 38.09it/s] 12%|█▏        | 1662/13630 [00:49<05:27, 36.56it/s] 12%|█▏        | 1667/13630 [00:49<05:30, 36.18it/s] 12%|█▏        | 1672/13630 [00:49<05:19, 37.43it/s] 12%|█▏        | 1677/13630 [00:49<05:02, 39.56it/s] 12%|█▏        | 1681/13630 [00:50<05:21, 37.19it/s] 12%|█▏        | 1685/13630 [00:50<05:29, 36.26it/s] 12%|█▏        | 1690/13630 [00:50<05:10, 38.49it/s] 12%|█▏        | 1695/13630 [00:50<05:01, 39.61it/s] 12%|█▏        | 1700/13630 [00:50<04:48, 41.40it/s] 13%|█▎        | 1705/13630 [00:50<05:25, 36.68it/s] 13%|█▎        | 1709/13630 [00:50<05:46, 34.36it/s] 13%|█▎        | 1714/13630 [00:51<05:26, 36.53it/s] 13%|█▎        | 1719/13630 [00:51<05:29, 36.19it/s] 13%|█▎        | 1724/13630 [00:51<05:14, 37.81it/s] 13%|█▎        | 1728/13630 [00:51<05:32, 35.79it/s] 13%|█▎        | 1733/13630 [00:51<05:13, 37.92it/s] 13%|█▎        | 1737/13630 [00:51<05:32, 35.79it/s] 13%|█▎        | 1742/13630 [00:51<05:13, 37.96it/s] 13%|█▎        | 1746/13630 [00:51<05:23, 36.75it/s] 13%|█▎        | 1751/13630 [00:51<05:12, 38.05it/s] 13%|█▎        | 1755/13630 [00:52<05:44, 34.48it/s] 13%|█▎        | 1759/13630 [00:52<05:54, 33.51it/s] 13%|█▎        | 1763/13630 [00:52<06:31, 30.30it/s] 13%|█▎        | 1768/13630 [00:52<05:59, 33.04it/s] 13%|█▎        | 1773/13630 [00:52<05:36, 35.23it/s] 13%|█▎        | 1778/13630 [00:52<05:15, 37.60it/s] 13%|█▎        | 1782/13630 [00:52<05:19, 37.07it/s] 13%|█▎        | 1787/13630 [00:53<04:59, 39.59it/s] 13%|█▎        | 1792/13630 [00:53<04:54, 40.23it/s] 13%|█▎        | 1797/13630 [00:53<04:50, 40.77it/s] 13%|█▎        | 1802/13630 [00:53<05:01, 39.30it/s] 13%|█▎        | 1806/13630 [00:53<05:18, 37.10it/s] 13%|█▎        | 1811/13630 [00:53<05:02, 39.08it/s] 13%|█▎        | 1815/13630 [00:53<05:29, 35.86it/s] 13%|█▎        | 1819/13630 [00:53<05:34, 35.34it/s] 13%|█▎        | 1823/13630 [00:53<05:29, 35.88it/s] 13%|█▎        | 1827/13630 [00:54<05:38, 34.86it/s] 13%|█▎        | 1831/13630 [00:54<06:46, 29.02it/s] 13%|█▎        | 1835/13630 [00:54<06:24, 30.64it/s] 13%|█▎        | 1839/13630 [00:54<06:16, 31.32it/s] 14%|█▎        | 1843/13630 [00:54<07:23, 26.60it/s] 14%|█▎        | 1847/13630 [00:54<07:10, 27.36it/s] 14%|█▎        | 1851/13630 [00:54<06:45, 29.03it/s] 14%|█▎        | 1855/13630 [00:55<07:55, 24.75it/s] 14%|█▎        | 1859/13630 [00:55<07:21, 26.67it/s] 14%|█▎        | 1863/13630 [00:55<06:46, 28.96it/s] 14%|█▎        | 1867/13630 [00:55<06:22, 30.78it/s] 14%|█▎        | 1871/13630 [00:55<06:00, 32.62it/s] 14%|█▍        | 1875/13630 [00:55<05:45, 33.98it/s] 14%|█▍        | 1879/13630 [00:55<05:35, 34.99it/s] 14%|█▍        | 1883/13630 [00:55<05:41, 34.35it/s] 14%|█▍        | 1887/13630 [00:56<06:37, 29.58it/s] 14%|█▍        | 1892/13630 [00:56<05:53, 33.19it/s] 14%|█▍        | 1897/13630 [00:56<05:33, 35.21it/s] 14%|█▍        | 1902/13630 [00:56<05:15, 37.15it/s] 14%|█▍        | 1906/13630 [00:56<05:33, 35.12it/s] 14%|█▍        | 1910/13630 [00:56<05:31, 35.36it/s] 14%|█▍        | 1915/13630 [00:56<05:15, 37.09it/s] 14%|█▍        | 1919/13630 [00:57<06:07, 31.90it/s] 14%|█▍        | 1923/13630 [00:57<05:56, 32.81it/s] 14%|█▍        | 1927/13630 [00:57<05:40, 34.41it/s] 14%|█▍        | 1931/13630 [00:57<05:44, 33.98it/s] 14%|█▍        | 1935/13630 [00:57<06:14, 31.19it/s] 14%|█▍        | 1940/13630 [00:57<06:07, 31.81it/s] 14%|█▍        | 1945/13630 [00:57<05:38, 34.52it/s] 14%|█▍        | 1949/13630 [00:57<05:56, 32.78it/s] 14%|█▍        | 1953/13630 [00:58<05:52, 33.12it/s] 14%|█▍        | 1958/13630 [00:58<05:25, 35.87it/s] 14%|█▍        | 1962/13630 [00:58<05:30, 35.35it/s] 14%|█▍        | 1966/13630 [00:58<05:47, 33.58it/s] 14%|█▍        | 1970/13630 [00:58<05:47, 33.54it/s] 14%|█▍        | 1974/13630 [00:58<05:53, 33.00it/s] 15%|█▍        | 1979/13630 [00:58<05:23, 36.00it/s] 15%|█▍        | 1983/13630 [00:58<05:29, 35.39it/s] 15%|█▍        | 1987/13630 [00:59<05:37, 34.47it/s] 15%|█▍        | 1991/13630 [00:59<05:57, 32.56it/s] 15%|█▍        | 1995/13630 [00:59<06:02, 32.08it/s] 15%|█▍        | 1999/13630 [00:59<06:21, 30.53it/s] 15%|█▍        | 2003/13630 [00:59<06:22, 30.38it/s] 15%|█▍        | 2008/13630 [00:59<05:47, 33.45it/s] 15%|█▍        | 2012/13630 [00:59<06:41, 28.96it/s] 15%|█▍        | 2017/13630 [01:00<06:02, 32.00it/s] 15%|█▍        | 2022/13630 [01:00<05:33, 34.84it/s] 15%|█▍        | 2026/13630 [01:00<05:44, 33.67it/s] 15%|█▍        | 2031/13630 [01:00<05:18, 36.41it/s] 15%|█▍        | 2035/13630 [01:00<05:35, 34.52it/s] 15%|█▍        | 2039/13630 [01:00<05:59, 32.26it/s] 15%|█▍        | 2043/13630 [01:00<06:01, 32.03it/s] 15%|█▌        | 2048/13630 [01:00<05:28, 35.30it/s] 15%|█▌        | 2053/13630 [01:01<05:08, 37.59it/s] 15%|█▌        | 2057/13630 [01:01<05:04, 37.99it/s] 15%|█▌        | 2061/13630 [01:01<05:25, 35.59it/s] 15%|█▌        | 2066/13630 [01:01<05:02, 38.28it/s] 15%|█▌        | 2070/13630 [01:01<05:14, 36.72it/s] 15%|█▌        | 2075/13630 [01:01<05:00, 38.51it/s] 15%|█▌        | 2079/13630 [01:01<05:33, 34.59it/s] 15%|█▌        | 2084/13630 [01:01<05:17, 36.41it/s] 15%|█▌        | 2089/13630 [01:01<05:00, 38.39it/s] 15%|█▌        | 2093/13630 [01:02<05:02, 38.18it/s] 15%|█▌        | 2098/13630 [01:02<05:31, 34.80it/s] 15%|█▌        | 2102/13630 [01:02<05:42, 33.69it/s] 15%|█▌        | 2106/13630 [01:02<05:39, 33.94it/s] 15%|█▌        | 2111/13630 [01:02<05:15, 36.48it/s] 16%|█▌        | 2115/13630 [01:02<05:44, 33.46it/s] 16%|█▌        | 2120/13630 [01:02<05:21, 35.76it/s] 16%|█▌        | 2124/13630 [01:03<05:49, 32.93it/s] 16%|█▌        | 2129/13630 [01:03<05:25, 35.37it/s] 16%|█▌        | 2133/13630 [01:03<05:57, 32.17it/s] 16%|█▌        | 2137/13630 [01:03<05:57, 32.11it/s] 16%|█▌        | 2141/13630 [01:03<05:48, 32.95it/s] 16%|█▌        | 2145/13630 [01:03<05:58, 32.05it/s] 16%|█▌        | 2149/13630 [01:03<05:55, 32.30it/s] 16%|█▌        | 2153/13630 [01:03<05:54, 32.42it/s] 16%|█▌        | 2157/13630 [01:04<06:07, 31.24it/s] 16%|█▌        | 2161/13630 [01:04<06:09, 31.00it/s] 16%|█▌        | 2165/13630 [01:04<06:15, 30.56it/s] 16%|█▌        | 2169/13630 [01:04<06:14, 30.58it/s] 16%|█▌        | 2173/13630 [01:04<06:31, 29.26it/s] 16%|█▌        | 2177/13630 [01:04<06:04, 31.44it/s] 16%|█▌        | 2182/13630 [01:04<05:30, 34.68it/s] 16%|█▌        | 2186/13630 [01:04<05:30, 34.62it/s] 16%|█▌        | 2190/13630 [01:05<06:00, 31.76it/s] 16%|█▌        | 2195/13630 [01:05<05:26, 35.04it/s] 16%|█▌        | 2199/13630 [01:05<05:23, 35.36it/s] 16%|█▌        | 2204/13630 [01:05<05:01, 37.96it/s] 16%|█▌        | 2209/13630 [01:05<04:47, 39.79it/s] 16%|█▌        | 2214/13630 [01:05<05:08, 36.95it/s] 16%|█▋        | 2218/13630 [01:05<05:17, 35.98it/s] 16%|█▋        | 2223/13630 [01:05<05:04, 37.45it/s] 16%|█▋        | 2227/13630 [01:06<05:36, 33.91it/s] 16%|█▋        | 2231/13630 [01:06<05:45, 33.02it/s] 16%|█▋        | 2235/13630 [01:06<05:48, 32.71it/s] 16%|█▋        | 2240/13630 [01:06<05:21, 35.48it/s] 16%|█▋        | 2244/13630 [01:06<05:27, 34.79it/s] 16%|█▋        | 2248/13630 [01:06<05:15, 36.04it/s] 17%|█▋        | 2252/13630 [01:06<05:26, 34.81it/s] 17%|█▋        | 2256/13630 [01:06<05:33, 34.10it/s] 17%|█▋        | 2261/13630 [01:07<05:12, 36.41it/s] 17%|█▋        | 2266/13630 [01:07<05:16, 35.96it/s] 17%|█▋        | 2271/13630 [01:07<04:58, 38.11it/s] 17%|█▋        | 2275/13630 [01:07<05:34, 33.94it/s] 17%|█▋        | 2280/13630 [01:07<05:12, 36.29it/s] 17%|█▋        | 2285/13630 [01:07<04:58, 38.05it/s] 17%|█▋        | 2290/13630 [01:07<04:51, 38.90it/s] 17%|█▋        | 2295/13630 [01:07<04:47, 39.49it/s] 17%|█▋        | 2300/13630 [01:08<05:02, 37.46it/s] 17%|█▋        | 2305/13630 [01:08<04:52, 38.68it/s] 17%|█▋        | 2310/13630 [01:08<04:45, 39.65it/s] 17%|█▋        | 2314/13630 [01:08<05:36, 33.60it/s] 17%|█▋        | 2319/13630 [01:08<05:11, 36.33it/s] 17%|█▋        | 2323/13630 [01:08<05:17, 35.59it/s] 17%|█▋        | 2328/13630 [01:08<05:00, 37.55it/s] 17%|█▋        | 2333/13630 [01:08<04:51, 38.74it/s] 17%|█▋        | 2338/13630 [01:09<04:45, 39.55it/s] 17%|█▋        | 2342/13630 [01:09<05:33, 33.86it/s] 17%|█▋        | 2347/13630 [01:09<05:09, 36.44it/s] 17%|█▋        | 2351/13630 [01:09<05:51, 32.12it/s] 17%|█▋        | 2356/13630 [01:09<05:25, 34.60it/s] 17%|█▋        | 2361/13630 [01:09<05:06, 36.82it/s] 17%|█▋        | 2366/13630 [01:09<04:50, 38.73it/s] 17%|█▋        | 2370/13630 [01:10<05:03, 37.15it/s] 17%|█▋        | 2374/13630 [01:10<05:48, 32.30it/s] 17%|█▋        | 2378/13630 [01:10<06:03, 30.97it/s] 17%|█▋        | 2382/13630 [01:10<06:11, 30.30it/s] 18%|█▊        | 2386/13630 [01:10<06:58, 26.84it/s] 18%|█▊        | 2391/13630 [01:10<06:08, 30.49it/s] 18%|█▊        | 2395/13630 [01:10<05:49, 32.13it/s] 18%|█▊        | 2399/13630 [01:11<05:37, 33.28it/s] 18%|█▊        | 2404/13630 [01:11<05:14, 35.66it/s] 18%|█▊        | 2408/13630 [01:11<05:43, 32.69it/s] 18%|█▊        | 2412/13630 [01:11<05:33, 33.65it/s] 18%|█▊        | 2416/13630 [01:11<05:52, 31.85it/s] 18%|█▊        | 2421/13630 [01:11<05:41, 32.84it/s] 18%|█▊        | 2426/13630 [01:11<05:13, 35.79it/s] 18%|█▊        | 2430/13630 [01:11<05:58, 31.21it/s] 18%|█▊        | 2434/13630 [01:12<05:42, 32.70it/s] 18%|█▊        | 2438/13630 [01:12<05:44, 32.52it/s] 18%|█▊        | 2442/13630 [01:12<05:39, 32.98it/s] 18%|█▊        | 2446/13630 [01:12<05:22, 34.73it/s] 18%|█▊        | 2451/13630 [01:12<05:05, 36.59it/s] 18%|█▊        | 2455/13630 [01:12<05:05, 36.53it/s] 18%|█▊        | 2459/13630 [01:12<05:01, 37.06it/s] 18%|█▊        | 2464/13630 [01:12<05:12, 35.68it/s] 18%|█▊        | 2468/13630 [01:13<06:15, 29.72it/s] 18%|█▊        | 2473/13630 [01:13<05:38, 32.96it/s] 18%|█▊        | 2478/13630 [01:13<05:15, 35.36it/s] 18%|█▊        | 2482/13630 [01:13<05:31, 33.66it/s] 18%|█▊        | 2486/13630 [01:13<05:17, 35.10it/s] 18%|█▊        | 2490/13630 [01:13<05:14, 35.46it/s] 18%|█▊        | 2494/13630 [01:13<05:36, 33.09it/s] 18%|█▊        | 2498/13630 [01:13<05:27, 33.97it/s] 18%|█▊        | 2503/13630 [01:14<05:05, 36.37it/s] 18%|█▊        | 2508/13630 [01:14<05:06, 36.31it/s] 18%|█▊        | 2512/13630 [01:14<06:02, 30.69it/s] 18%|█▊        | 2516/13630 [01:14<05:52, 31.49it/s] 18%|█▊        | 2520/13630 [01:14<05:35, 33.13it/s] 19%|█▊        | 2524/13630 [01:14<05:37, 32.92it/s] 19%|█▊        | 2528/13630 [01:14<05:49, 31.76it/s] 19%|█▊        | 2533/13630 [01:14<05:18, 34.79it/s] 19%|█▊        | 2537/13630 [01:15<05:31, 33.44it/s] 19%|█▊        | 2541/13630 [01:15<05:49, 31.73it/s] 19%|█▊        | 2546/13630 [01:15<05:19, 34.66it/s] 19%|█▊        | 2550/13630 [01:15<05:29, 33.67it/s] 19%|█▊        | 2554/13630 [01:15<05:17, 34.85it/s] 19%|█▉        | 2559/13630 [01:15<04:58, 37.05it/s] 19%|█▉        | 2563/13630 [01:15<06:18, 29.21it/s] 19%|█▉        | 2567/13630 [01:16<05:50, 31.59it/s] 19%|█▉        | 2572/13630 [01:16<05:25, 34.02it/s] 19%|█▉        | 2576/13630 [01:17<16:08, 11.41it/s] 19%|█▉        | 2580/13630 [01:17<13:09, 14.00it/s] 19%|█▉        | 2583/13630 [01:17<12:14, 15.05it/s] 19%|█▉        | 2586/13630 [01:17<10:47, 17.07it/s] 19%|█▉        | 2591/13630 [01:17<08:22, 21.97it/s] 19%|█▉        | 2596/13630 [01:17<06:58, 26.38it/s] 19%|█▉        | 2600/13630 [01:17<06:32, 28.08it/s] 19%|█▉        | 2605/13630 [01:17<06:18, 29.13it/s] 19%|█▉        | 2609/13630 [01:18<06:00, 30.54it/s] 19%|█▉        | 2613/13630 [01:18<06:37, 27.69it/s] 19%|█▉        | 2618/13630 [01:18<05:45, 31.86it/s] 19%|█▉        | 2622/13630 [01:18<06:04, 30.20it/s] 19%|█▉        | 2627/13630 [01:18<05:31, 33.20it/s] 19%|█▉        | 2631/13630 [01:18<05:26, 33.70it/s] 19%|█▉        | 2635/13630 [01:18<05:38, 32.44it/s] 19%|█▉        | 2639/13630 [01:19<05:33, 32.98it/s] 19%|█▉        | 2643/13630 [01:19<05:29, 33.36it/s] 19%|█▉        | 2647/13630 [01:19<05:15, 34.86it/s] 19%|█▉        | 2652/13630 [01:19<04:57, 36.94it/s] 19%|█▉        | 2656/13630 [01:19<05:18, 34.48it/s] 20%|█▉        | 2661/13630 [01:19<04:56, 37.02it/s] 20%|█▉        | 2666/13630 [01:19<05:10, 35.29it/s] 20%|█▉        | 2670/13630 [01:19<05:13, 34.98it/s] 20%|█▉        | 2674/13630 [01:20<05:37, 32.47it/s] 20%|█▉        | 2678/13630 [01:20<06:01, 30.28it/s] 20%|█▉        | 2682/13630 [01:20<06:36, 27.59it/s] 20%|█▉        | 2687/13630 [01:20<05:50, 31.26it/s] 20%|█▉        | 2692/13630 [01:20<05:18, 34.33it/s] 20%|█▉        | 2696/13630 [01:20<05:27, 33.34it/s] 20%|█▉        | 2700/13630 [01:20<05:39, 32.19it/s] 20%|█▉        | 2704/13630 [01:20<05:38, 32.32it/s] 20%|█▉        | 2708/13630 [01:21<05:20, 34.05it/s] 20%|█▉        | 2712/13630 [01:21<05:21, 33.96it/s] 20%|█▉        | 2717/13630 [01:21<05:01, 36.17it/s] 20%|█▉        | 2721/13630 [01:21<05:23, 33.72it/s] 20%|█▉        | 2725/13630 [01:21<05:33, 32.69it/s] 20%|██        | 2729/13630 [01:21<05:18, 34.22it/s] 20%|██        | 2734/13630 [01:21<04:57, 36.64it/s] 20%|██        | 2739/13630 [01:21<04:42, 38.61it/s] 20%|██        | 2743/13630 [01:22<04:40, 38.86it/s] 20%|██        | 2747/13630 [01:22<05:03, 35.85it/s] 20%|██        | 2752/13630 [01:22<04:50, 37.41it/s] 20%|██        | 2756/13630 [01:22<05:04, 35.71it/s] 20%|██        | 2760/13630 [01:22<05:07, 35.32it/s] 20%|██        | 2765/13630 [01:22<05:18, 34.15it/s] 20%|██        | 2769/13630 [01:22<05:53, 30.72it/s] 20%|██        | 2774/13630 [01:22<05:21, 33.77it/s] 20%|██        | 2779/13630 [01:23<04:58, 36.35it/s] 20%|██        | 2784/13630 [01:23<04:44, 38.07it/s] 20%|██        | 2789/13630 [01:23<04:35, 39.31it/s] 20%|██        | 2794/13630 [01:23<04:26, 40.63it/s] 21%|██        | 2799/13630 [01:23<05:08, 35.06it/s] 21%|██        | 2803/13630 [01:23<05:16, 34.23it/s] 21%|██        | 2807/13630 [01:23<05:30, 32.73it/s] 21%|██        | 2812/13630 [01:24<05:06, 35.30it/s] 21%|██        | 2816/13630 [01:24<05:31, 32.65it/s] 21%|██        | 2821/13630 [01:24<05:01, 35.80it/s] 21%|██        | 2825/13630 [01:24<04:55, 36.62it/s] 21%|██        | 2829/13630 [01:24<05:12, 34.53it/s] 21%|██        | 2833/13630 [01:24<05:14, 34.32it/s] 21%|██        | 2837/13630 [01:24<05:21, 33.57it/s] 21%|██        | 2841/13630 [01:24<05:25, 33.11it/s] 21%|██        | 2845/13630 [01:24<05:11, 34.59it/s] 21%|██        | 2850/13630 [01:25<04:52, 36.85it/s] 21%|██        | 2854/13630 [01:25<05:13, 34.36it/s] 21%|██        | 2859/13630 [01:25<04:59, 36.01it/s] 21%|██        | 2863/13630 [01:25<05:13, 34.35it/s] 21%|██        | 2867/13630 [01:25<05:39, 31.69it/s] 21%|██        | 2871/13630 [01:25<05:28, 32.75it/s] 21%|██        | 2876/13630 [01:25<05:04, 35.34it/s] 21%|██        | 2881/13630 [01:25<04:45, 37.59it/s] 21%|██        | 2885/13630 [01:26<05:00, 35.80it/s] 21%|██        | 2889/13630 [01:26<05:15, 34.03it/s] 21%|██        | 2894/13630 [01:26<05:25, 32.98it/s] 21%|██▏       | 2898/13630 [01:26<05:22, 33.27it/s] 21%|██▏       | 2902/13630 [01:26<05:15, 34.03it/s] 21%|██▏       | 2907/13630 [01:26<04:52, 36.61it/s] 21%|██▏       | 2912/13630 [01:26<05:06, 34.97it/s] 21%|██▏       | 2916/13630 [01:27<05:20, 33.42it/s] 21%|██▏       | 2920/13630 [01:27<05:37, 31.77it/s] 21%|██▏       | 2925/13630 [01:27<05:28, 32.60it/s] 21%|██▏       | 2929/13630 [01:27<05:29, 32.46it/s] 22%|██▏       | 2933/13630 [01:27<05:42, 31.27it/s] 22%|██▏       | 2937/13630 [01:27<05:38, 31.63it/s] 22%|██▏       | 2941/13630 [01:27<05:42, 31.18it/s] 22%|██▏       | 2946/13630 [01:27<05:05, 34.96it/s] 22%|██▏       | 2951/13630 [01:28<04:48, 37.03it/s] 22%|██▏       | 2955/13630 [01:28<05:37, 31.66it/s] 22%|██▏       | 2960/13630 [01:28<05:08, 34.54it/s] 22%|██▏       | 2965/13630 [01:28<04:50, 36.71it/s] 22%|██▏       | 2969/13630 [01:28<04:57, 35.78it/s] 22%|██▏       | 2974/13630 [01:28<04:43, 37.62it/s] 22%|██▏       | 2979/13630 [01:28<04:34, 38.79it/s] 22%|██▏       | 2984/13630 [01:28<04:21, 40.76it/s] 22%|██▏       | 2989/13630 [01:29<04:14, 41.73it/s] 22%|██▏       | 2994/13630 [01:29<04:32, 39.05it/s] 22%|██▏       | 2998/13630 [01:29<04:35, 38.64it/s] 22%|██▏       | 3003/13630 [01:29<04:50, 36.59it/s] 22%|██▏       | 3007/13630 [01:29<05:00, 35.30it/s] 22%|██▏       | 3011/13630 [01:29<05:21, 32.99it/s] 22%|██▏       | 3016/13630 [01:29<04:57, 35.69it/s] 22%|██▏       | 3021/13630 [01:29<05:00, 35.36it/s] 22%|██▏       | 3025/13630 [01:30<05:11, 34.03it/s] 22%|██▏       | 3030/13630 [01:30<04:51, 36.35it/s] 22%|██▏       | 3034/13630 [01:30<05:28, 32.25it/s] 22%|██▏       | 3038/13630 [01:30<05:17, 33.36it/s] 22%|██▏       | 3042/13630 [01:30<05:54, 29.83it/s] 22%|██▏       | 3046/13630 [01:30<06:09, 28.61it/s] 22%|██▏       | 3050/13630 [01:30<05:40, 31.04it/s] 22%|██▏       | 3055/13630 [01:31<05:17, 33.34it/s] 22%|██▏       | 3059/13630 [01:31<05:44, 30.68it/s] 22%|██▏       | 3064/13630 [01:31<05:25, 32.46it/s] 23%|██▎       | 3069/13630 [01:31<05:02, 34.94it/s] 23%|██▎       | 3073/13630 [01:31<05:06, 34.49it/s] 23%|██▎       | 3078/13630 [01:31<04:45, 36.95it/s] 23%|██▎       | 3082/13630 [01:31<05:00, 35.06it/s] 23%|██▎       | 3087/13630 [01:31<04:46, 36.76it/s] 23%|██▎       | 3092/13630 [01:32<04:56, 35.48it/s] 23%|██▎       | 3097/13630 [01:32<05:30, 31.90it/s] 23%|██▎       | 3101/13630 [01:32<06:09, 28.47it/s] 23%|██▎       | 3104/13630 [01:32<07:00, 25.02it/s] 23%|██▎       | 3109/13630 [01:32<06:05, 28.79it/s] 23%|██▎       | 3114/13630 [01:32<05:21, 32.69it/s] 23%|██▎       | 3118/13630 [01:33<05:10, 33.85it/s] 23%|██▎       | 3122/13630 [01:33<05:25, 32.28it/s] 23%|██▎       | 3126/13630 [01:33<05:31, 31.70it/s] 23%|██▎       | 3130/13630 [01:33<05:24, 32.35it/s] 23%|██▎       | 3134/13630 [01:33<05:38, 31.01it/s] 23%|██▎       | 3138/13630 [01:33<05:39, 30.87it/s] 23%|██▎       | 3143/13630 [01:33<05:06, 34.27it/s] 23%|██▎       | 3147/13630 [01:33<05:18, 32.95it/s] 23%|██▎       | 3151/13630 [01:34<06:08, 28.46it/s] 23%|██▎       | 3156/13630 [01:34<05:26, 32.03it/s] 23%|██▎       | 3160/13630 [01:34<06:24, 27.26it/s] 23%|██▎       | 3163/13630 [01:34<07:04, 24.66it/s] 23%|██▎       | 3167/13630 [01:34<06:27, 26.99it/s] 23%|██▎       | 3172/13630 [01:34<05:55, 29.39it/s] 23%|██▎       | 3176/13630 [01:34<05:52, 29.67it/s] 23%|██▎       | 3180/13630 [01:35<05:51, 29.77it/s] 23%|██▎       | 3184/13630 [01:35<05:42, 30.47it/s] 23%|██▎       | 3188/13630 [01:35<06:03, 28.76it/s] 23%|██▎       | 3191/13630 [01:35<06:15, 27.79it/s] 23%|██▎       | 3196/13630 [01:35<05:27, 31.82it/s] 23%|██▎       | 3201/13630 [01:35<04:58, 34.91it/s] 24%|██▎       | 3205/13630 [01:35<05:27, 31.85it/s] 24%|██▎       | 3209/13630 [01:36<05:08, 33.74it/s] 24%|██▎       | 3214/13630 [01:36<04:42, 36.89it/s] 24%|██▎       | 3218/13630 [01:36<04:42, 36.81it/s] 24%|██▎       | 3222/13630 [01:36<05:01, 34.52it/s] 24%|██▎       | 3226/13630 [01:36<05:39, 30.64it/s] 24%|██▎       | 3230/13630 [01:36<06:09, 28.15it/s] 24%|██▎       | 3233/13630 [01:36<06:41, 25.92it/s] 24%|██▎       | 3237/13630 [01:36<06:13, 27.85it/s] 24%|██▍       | 3241/13630 [01:37<06:39, 25.99it/s] 24%|██▍       | 3244/13630 [01:37<06:45, 25.61it/s] 24%|██▍       | 3249/13630 [01:37<05:48, 29.75it/s] 24%|██▍       | 3253/13630 [01:37<05:23, 32.09it/s] 24%|██▍       | 3257/13630 [01:37<06:05, 28.39it/s] 24%|██▍       | 3260/13630 [01:37<06:41, 25.82it/s] 24%|██▍       | 3263/13630 [01:37<07:09, 24.16it/s] 24%|██▍       | 3267/13630 [01:38<06:29, 26.62it/s] 24%|██▍       | 3270/13630 [01:38<07:29, 23.06it/s] 24%|██▍       | 3274/13630 [01:38<06:38, 26.01it/s] 24%|██▍       | 3278/13630 [01:38<06:31, 26.47it/s] 24%|██▍       | 3282/13630 [01:38<06:03, 28.48it/s] 24%|██▍       | 3285/13630 [01:38<06:27, 26.68it/s] 24%|██▍       | 3289/13630 [01:38<05:55, 29.09it/s] 24%|██▍       | 3292/13630 [01:38<06:12, 27.77it/s] 24%|██▍       | 3296/13630 [01:39<05:44, 30.02it/s] 24%|██▍       | 3300/13630 [01:39<05:50, 29.50it/s] 24%|██▍       | 3304/13630 [01:39<05:28, 31.39it/s] 24%|██▍       | 3308/13630 [01:39<05:43, 30.03it/s] 24%|██▍       | 3312/13630 [01:39<05:58, 28.78it/s] 24%|██▍       | 3315/13630 [01:39<06:10, 27.81it/s] 24%|██▍       | 3318/13630 [01:39<06:20, 27.12it/s] 24%|██▍       | 3321/13630 [01:40<06:29, 26.44it/s] 24%|██▍       | 3324/13630 [01:40<06:32, 26.28it/s] 24%|██▍       | 3329/13630 [01:40<06:17, 27.32it/s] 24%|██▍       | 3333/13630 [01:40<05:54, 29.04it/s] 24%|██▍       | 3336/13630 [01:40<06:26, 26.60it/s] 24%|██▍       | 3339/13630 [01:40<06:21, 26.96it/s] 25%|██▍       | 3343/13630 [01:40<05:45, 29.77it/s] 25%|██▍       | 3347/13630 [01:40<06:08, 27.91it/s] 25%|██▍       | 3350/13630 [01:41<06:06, 28.07it/s] 25%|██▍       | 3355/13630 [01:41<05:17, 32.36it/s] 25%|██▍       | 3359/13630 [01:41<05:31, 31.01it/s] 25%|██▍       | 3364/13630 [01:41<05:35, 30.63it/s] 25%|██▍       | 3369/13630 [01:41<05:05, 33.60it/s] 25%|██▍       | 3373/13630 [01:41<05:10, 33.05it/s] 25%|██▍       | 3377/13630 [01:41<04:55, 34.65it/s] 25%|██▍       | 3381/13630 [01:41<05:14, 32.54it/s] 25%|██▍       | 3385/13630 [01:42<05:38, 30.30it/s] 25%|██▍       | 3390/13630 [01:42<05:00, 34.10it/s] 25%|██▍       | 3394/13630 [01:42<05:32, 30.79it/s] 25%|██▍       | 3398/13630 [01:42<05:19, 32.07it/s] 25%|██▍       | 3402/13630 [01:42<05:10, 32.89it/s] 25%|██▍       | 3406/13630 [01:42<05:03, 33.70it/s] 25%|██▌       | 3410/13630 [01:42<05:06, 33.34it/s] 25%|██▌       | 3415/13630 [01:42<04:39, 36.50it/s] 25%|██▌       | 3419/13630 [01:43<05:13, 32.53it/s] 25%|██▌       | 3424/13630 [01:43<05:03, 33.61it/s] 25%|██▌       | 3428/13630 [01:43<05:01, 33.83it/s] 25%|██▌       | 3433/13630 [01:43<04:43, 36.01it/s] 25%|██▌       | 3438/13630 [01:43<04:31, 37.59it/s] 25%|██▌       | 3443/13630 [01:43<04:23, 38.72it/s] 25%|██▌       | 3448/13630 [01:43<04:21, 38.95it/s] 25%|██▌       | 3453/13630 [01:43<04:17, 39.48it/s] 25%|██▌       | 3457/13630 [01:44<04:28, 37.89it/s] 25%|██▌       | 3461/13630 [01:44<04:24, 38.42it/s] 25%|██▌       | 3465/13630 [01:44<04:38, 36.56it/s] 25%|██▌       | 3470/13630 [01:44<04:24, 38.41it/s] 25%|██▌       | 3474/13630 [01:44<05:02, 33.56it/s] 26%|██▌       | 3478/13630 [01:44<05:22, 31.44it/s] 26%|██▌       | 3482/13630 [01:44<05:59, 28.22it/s] 26%|██▌       | 3485/13630 [01:45<06:20, 26.67it/s] 26%|██▌       | 3488/13630 [01:45<06:17, 26.89it/s] 26%|██▌       | 3491/13630 [01:45<06:12, 27.22it/s] 26%|██▌       | 3496/13630 [01:45<05:21, 31.53it/s] 26%|██▌       | 3500/13630 [01:45<05:25, 31.07it/s] 26%|██▌       | 3504/13630 [01:45<05:07, 32.98it/s] 26%|██▌       | 3508/13630 [01:45<05:02, 33.50it/s] 26%|██▌       | 3512/13630 [01:45<05:09, 32.72it/s] 26%|██▌       | 3516/13630 [01:45<05:09, 32.67it/s] 26%|██▌       | 3521/13630 [01:46<04:49, 34.88it/s] 26%|██▌       | 3526/13630 [01:46<04:35, 36.65it/s] 26%|██▌       | 3531/13630 [01:46<05:12, 32.33it/s] 26%|██▌       | 3536/13630 [01:46<04:48, 35.03it/s] 26%|██▌       | 3540/13630 [01:46<05:33, 30.25it/s] 26%|██▌       | 3544/13630 [01:46<05:27, 30.79it/s] 26%|██▌       | 3548/13630 [01:46<05:28, 30.67it/s] 26%|██▌       | 3552/13630 [01:47<05:10, 32.50it/s] 26%|██▌       | 3556/13630 [01:47<05:22, 31.28it/s] 26%|██▌       | 3560/13630 [01:47<07:09, 23.47it/s] 26%|██▌       | 3563/13630 [01:47<07:17, 23.00it/s] 26%|██▌       | 3567/13630 [01:47<06:52, 24.41it/s] 26%|██▌       | 3571/13630 [01:47<06:03, 27.66it/s] 26%|██▌       | 3575/13630 [01:48<06:09, 27.20it/s] 26%|██▋       | 3579/13630 [01:48<05:33, 30.12it/s] 26%|██▋       | 3584/13630 [01:48<05:05, 32.84it/s] 26%|██▋       | 3588/13630 [01:48<04:59, 33.52it/s] 26%|██▋       | 3592/13630 [01:48<05:07, 32.61it/s] 26%|██▋       | 3596/13630 [01:48<05:23, 31.04it/s] 26%|██▋       | 3600/13630 [01:48<05:48, 28.77it/s] 26%|██▋       | 3603/13630 [01:48<06:26, 25.97it/s] 26%|██▋       | 3608/13630 [01:49<05:45, 29.04it/s] 27%|██▋       | 3613/13630 [01:49<05:08, 32.43it/s] 27%|██▋       | 3618/13630 [01:49<04:47, 34.86it/s] 27%|██▋       | 3622/13630 [01:49<05:10, 32.19it/s] 27%|██▋       | 3626/13630 [01:49<05:42, 29.20it/s] 27%|██▋       | 3630/13630 [01:49<05:40, 29.37it/s] 27%|██▋       | 3634/13630 [01:49<05:14, 31.82it/s] 27%|██▋       | 3639/13630 [01:50<04:48, 34.61it/s] 27%|██▋       | 3643/13630 [01:50<05:38, 29.48it/s] 27%|██▋       | 3647/13630 [01:50<05:28, 30.42it/s] 27%|██▋       | 3652/13630 [01:50<05:02, 32.94it/s] 27%|██▋       | 3656/13630 [01:50<04:49, 34.49it/s] 27%|██▋       | 3660/13630 [01:50<05:06, 32.58it/s] 27%|██▋       | 3664/13630 [01:50<05:09, 32.24it/s] 27%|██▋       | 3668/13630 [01:50<04:52, 34.08it/s] 27%|██▋       | 3672/13630 [01:51<04:54, 33.82it/s] 27%|██▋       | 3676/13630 [01:51<04:42, 35.27it/s] 27%|██▋       | 3680/13630 [01:51<05:13, 31.70it/s] 27%|██▋       | 3684/13630 [01:51<05:06, 32.43it/s] 27%|██▋       | 3688/13630 [01:51<05:04, 32.70it/s] 27%|██▋       | 3693/13630 [01:51<04:35, 36.03it/s] 27%|██▋       | 3698/13630 [01:51<04:23, 37.74it/s] 27%|██▋       | 3702/13630 [01:51<04:35, 35.98it/s] 27%|██▋       | 3707/13630 [01:52<04:23, 37.68it/s] 27%|██▋       | 3711/13630 [01:52<04:33, 36.25it/s] 27%|██▋       | 3716/13630 [01:52<04:11, 39.43it/s] 27%|██▋       | 3720/13630 [01:52<04:18, 38.40it/s] 27%|██▋       | 3724/13630 [01:52<04:28, 36.85it/s] 27%|██▋       | 3728/13630 [01:52<04:23, 37.53it/s] 27%|██▋       | 3732/13630 [01:52<04:55, 33.54it/s] 27%|██▋       | 3736/13630 [01:52<05:26, 30.34it/s] 27%|██▋       | 3740/13630 [01:53<05:26, 30.26it/s] 27%|██▋       | 3745/13630 [01:53<04:55, 33.40it/s] 28%|██▊       | 3749/13630 [01:53<05:52, 28.03it/s] 28%|██▊       | 3752/13630 [01:53<05:50, 28.18it/s] 28%|██▊       | 3755/13630 [01:53<06:06, 26.97it/s] 28%|██▊       | 3758/13630 [01:53<06:20, 25.94it/s] 28%|██▊       | 3762/13630 [01:53<05:49, 28.24it/s] 28%|██▊       | 3767/13630 [01:53<05:42, 28.79it/s] 28%|██▊       | 3771/13630 [01:54<05:16, 31.11it/s] 28%|██▊       | 3775/13630 [01:54<05:56, 27.65it/s] 28%|██▊       | 3778/13630 [01:54<05:51, 28.04it/s] 28%|██▊       | 3782/13630 [01:54<05:47, 28.32it/s] 28%|██▊       | 3787/13630 [01:54<05:06, 32.14it/s] 28%|██▊       | 3791/13630 [01:54<04:50, 33.87it/s] 28%|██▊       | 3795/13630 [01:54<05:59, 27.39it/s] 28%|██▊       | 3799/13630 [01:55<06:15, 26.21it/s] 28%|██▊       | 3803/13630 [01:55<05:57, 27.51it/s] 28%|██▊       | 3808/13630 [01:55<05:13, 31.33it/s] 28%|██▊       | 3813/13630 [01:55<04:51, 33.66it/s] 28%|██▊       | 3818/13630 [01:55<04:53, 33.39it/s] 28%|██▊       | 3822/13630 [01:55<04:40, 34.92it/s] 28%|██▊       | 3826/13630 [01:55<04:33, 35.88it/s] 28%|██▊       | 3830/13630 [01:55<04:26, 36.81it/s] 28%|██▊       | 3834/13630 [01:56<05:46, 28.25it/s] 28%|██▊       | 3838/13630 [01:56<05:26, 30.02it/s] 28%|██▊       | 3842/13630 [01:56<05:13, 31.23it/s] 28%|██▊       | 3847/13630 [01:56<04:47, 33.99it/s] 28%|██▊       | 3851/13630 [01:56<04:46, 34.08it/s] 28%|██▊       | 3855/13630 [01:56<04:53, 33.30it/s] 28%|██▊       | 3859/13630 [01:57<06:22, 25.57it/s] 28%|██▊       | 3864/13630 [01:57<05:32, 29.39it/s] 28%|██▊       | 3868/13630 [01:57<06:30, 25.01it/s] 28%|██▊       | 3872/13630 [01:57<06:10, 26.37it/s] 28%|██▊       | 3877/13630 [01:57<05:27, 29.77it/s] 28%|██▊       | 3881/13630 [01:57<05:43, 28.41it/s] 29%|██▊       | 3885/13630 [01:57<06:06, 26.60it/s] 29%|██▊       | 3890/13630 [01:58<05:19, 30.45it/s] 29%|██▊       | 3895/13630 [01:58<04:45, 34.15it/s] 29%|██▊       | 3899/13630 [01:58<06:18, 25.69it/s] 29%|██▊       | 3903/13630 [01:58<05:47, 27.97it/s] 29%|██▊       | 3907/13630 [01:58<06:23, 25.32it/s] 29%|██▊       | 3911/13630 [01:58<05:57, 27.16it/s] 29%|██▊       | 3914/13630 [01:59<07:31, 21.51it/s] 29%|██▉       | 3919/13630 [01:59<06:09, 26.27it/s] 29%|██▉       | 3924/13630 [01:59<05:22, 30.11it/s] 29%|██▉       | 3928/13630 [01:59<05:00, 32.29it/s] 29%|██▉       | 3932/13630 [01:59<06:09, 26.23it/s] 29%|██▉       | 3937/13630 [01:59<05:23, 29.96it/s] 29%|██▉       | 3941/13630 [01:59<05:53, 27.43it/s] 29%|██▉       | 3946/13630 [02:00<05:14, 30.83it/s] 29%|██▉       | 3950/13630 [02:00<05:17, 30.44it/s] 29%|██▉       | 3955/13630 [02:00<04:49, 33.44it/s] 29%|██▉       | 3959/13630 [02:00<04:40, 34.43it/s] 29%|██▉       | 3964/13630 [02:00<04:27, 36.17it/s] 29%|██▉       | 3969/13630 [02:00<04:19, 37.17it/s] 29%|██▉       | 3973/13630 [02:00<04:25, 36.39it/s] 29%|██▉       | 3978/13630 [02:00<04:15, 37.78it/s] 29%|██▉       | 3983/13630 [02:01<04:08, 38.82it/s] 29%|██▉       | 3987/13630 [02:01<04:20, 36.98it/s] 29%|██▉       | 3992/13630 [02:01<04:08, 38.75it/s] 29%|██▉       | 3996/13630 [02:01<04:09, 38.66it/s] 29%|██▉       | 4000/13630 [02:01<04:24, 36.46it/s] 29%|██▉       | 4005/13630 [02:01<04:12, 38.11it/s] 29%|██▉       | 4009/13630 [02:01<05:17, 30.29it/s] 29%|██▉       | 4014/13630 [02:01<05:18, 30.17it/s] 29%|██▉       | 4019/13630 [02:02<04:51, 32.94it/s] 30%|██▉       | 4024/13630 [02:02<04:34, 35.03it/s] 30%|██▉       | 4028/13630 [02:02<05:37, 28.45it/s] 30%|██▉       | 4032/13630 [02:02<05:10, 30.87it/s] 30%|██▉       | 4036/13630 [02:02<05:09, 30.95it/s] 30%|██▉       | 4041/13630 [02:02<04:42, 33.98it/s] 30%|██▉       | 4045/13630 [02:02<04:43, 33.82it/s] 30%|██▉       | 4049/13630 [02:03<04:34, 34.88it/s] 30%|██▉       | 4053/13630 [02:03<04:40, 34.20it/s] 30%|██▉       | 4058/13630 [02:03<04:23, 36.26it/s] 30%|██▉       | 4063/13630 [02:03<04:10, 38.21it/s] 30%|██▉       | 4067/13630 [02:03<04:11, 38.05it/s] 30%|██▉       | 4072/13630 [02:03<04:02, 39.48it/s] 30%|██▉       | 4076/13630 [02:03<04:27, 35.77it/s] 30%|██▉       | 4080/13630 [02:03<04:50, 32.83it/s] 30%|██▉       | 4084/13630 [02:04<05:07, 31.08it/s] 30%|██▉       | 4088/13630 [02:04<05:27, 29.18it/s] 30%|███       | 4091/13630 [02:04<05:39, 28.06it/s] 30%|███       | 4095/13630 [02:04<05:30, 28.86it/s] 30%|███       | 4098/13630 [02:04<05:35, 28.44it/s] 30%|███       | 4101/13630 [02:04<05:52, 27.00it/s] 30%|███       | 4104/13630 [02:04<06:05, 26.06it/s] 30%|███       | 4109/13630 [02:04<05:13, 30.33it/s] 30%|███       | 4113/13630 [02:05<05:06, 31.01it/s] 30%|███       | 4117/13630 [02:05<04:50, 32.77it/s] 30%|███       | 4121/13630 [02:05<04:49, 32.82it/s] 30%|███       | 4125/13630 [02:05<05:13, 30.33it/s] 30%|███       | 4129/13630 [02:05<04:53, 32.41it/s] 30%|███       | 4133/13630 [02:05<04:54, 32.23it/s] 30%|███       | 4138/13630 [02:05<04:26, 35.67it/s] 30%|███       | 4142/13630 [02:05<05:12, 30.38it/s] 30%|███       | 4146/13630 [02:06<04:57, 31.84it/s] 30%|███       | 4151/13630 [02:06<04:34, 34.48it/s] 30%|███       | 4155/13630 [02:06<04:39, 33.93it/s] 31%|███       | 4160/13630 [02:06<04:25, 35.72it/s] 31%|███       | 4164/13630 [02:06<04:29, 35.10it/s] 31%|███       | 4169/13630 [02:06<04:23, 35.86it/s] 31%|███       | 4173/13630 [02:06<04:31, 34.85it/s] 31%|███       | 4177/13630 [02:06<04:22, 36.07it/s] 31%|███       | 4181/13630 [02:07<05:52, 26.78it/s] 31%|███       | 4185/13630 [02:07<05:45, 27.32it/s] 31%|███       | 4188/13630 [02:07<05:53, 26.74it/s] 31%|███       | 4192/13630 [02:07<05:31, 28.49it/s] 31%|███       | 4196/13630 [02:07<05:03, 31.10it/s] 31%|███       | 4201/13630 [02:07<04:33, 34.48it/s] 31%|███       | 4205/13630 [02:07<04:26, 35.33it/s] 31%|███       | 4210/13630 [02:08<04:34, 34.37it/s] 31%|███       | 4214/13630 [02:08<04:40, 33.55it/s] 31%|███       | 4218/13630 [02:08<04:34, 34.28it/s] 31%|███       | 4223/13630 [02:08<04:15, 36.83it/s] 31%|███       | 4227/13630 [02:08<04:30, 34.73it/s] 31%|███       | 4231/13630 [02:08<04:42, 33.32it/s] 31%|███       | 4236/13630 [02:08<04:20, 36.07it/s] 31%|███       | 4240/13630 [02:08<04:41, 33.41it/s] 31%|███       | 4244/13630 [02:09<05:00, 31.28it/s] 31%|███       | 4249/13630 [02:09<04:34, 34.17it/s] 31%|███       | 4254/13630 [02:09<04:17, 36.38it/s] 31%|███       | 4258/13630 [02:09<04:36, 33.90it/s] 31%|███▏      | 4263/13630 [02:09<04:17, 36.39it/s] 31%|███▏      | 4268/13630 [02:09<04:06, 38.03it/s] 31%|███▏      | 4273/13630 [02:09<04:00, 38.88it/s] 31%|███▏      | 4277/13630 [02:09<04:00, 38.96it/s] 31%|███▏      | 4281/13630 [02:10<04:31, 34.37it/s] 31%|███▏      | 4285/13630 [02:10<04:48, 32.43it/s] 31%|███▏      | 4289/13630 [02:10<04:47, 32.48it/s] 31%|███▏      | 4293/13630 [02:10<04:33, 34.17it/s] 32%|███▏      | 4297/13630 [02:10<04:45, 32.70it/s] 32%|███▏      | 4301/13630 [02:10<04:51, 31.97it/s] 32%|███▏      | 4306/13630 [02:10<04:27, 34.84it/s] 32%|███▏      | 4311/13630 [02:10<04:24, 35.20it/s] 32%|███▏      | 4315/13630 [02:11<04:37, 33.55it/s] 32%|███▏      | 4319/13630 [02:11<04:37, 33.50it/s] 32%|███▏      | 4324/13630 [02:11<04:17, 36.09it/s] 32%|███▏      | 4328/13630 [02:11<04:28, 34.71it/s] 32%|███▏      | 4332/13630 [02:11<04:59, 31.05it/s] 32%|███▏      | 4336/13630 [02:11<04:53, 31.66it/s] 32%|███▏      | 4340/13630 [02:11<05:27, 28.37it/s] 32%|███▏      | 4345/13630 [02:12<04:50, 32.00it/s] 32%|███▏      | 4349/13630 [02:12<05:10, 29.92it/s] 32%|███▏      | 4354/13630 [02:12<04:36, 33.59it/s] 32%|███▏      | 4358/13630 [02:12<05:04, 30.42it/s] 32%|███▏      | 4362/13630 [02:12<05:05, 30.36it/s] 32%|███▏      | 4366/13630 [02:12<05:31, 27.97it/s] 32%|███▏      | 4370/13630 [02:12<05:17, 29.18it/s] 32%|███▏      | 4374/13630 [02:13<05:37, 27.46it/s] 32%|███▏      | 4378/13630 [02:13<05:07, 30.05it/s] 32%|███▏      | 4382/13630 [02:13<05:07, 30.11it/s] 32%|███▏      | 4386/13630 [02:13<04:55, 31.31it/s] 32%|███▏      | 4391/13630 [02:13<04:32, 33.95it/s] 32%|███▏      | 4395/13630 [02:13<04:22, 35.23it/s] 32%|███▏      | 4400/13630 [02:13<04:05, 37.64it/s] 32%|███▏      | 4404/13630 [02:13<04:26, 34.56it/s] 32%|███▏      | 4409/13630 [02:13<04:10, 36.78it/s] 32%|███▏      | 4413/13630 [02:14<04:26, 34.54it/s] 32%|███▏      | 4417/13630 [02:14<04:59, 30.80it/s] 32%|███▏      | 4421/13630 [02:14<05:16, 29.07it/s] 32%|███▏      | 4426/13630 [02:14<04:43, 32.47it/s] 33%|███▎      | 4430/13630 [02:14<05:19, 28.84it/s] 33%|███▎      | 4434/13630 [02:14<05:20, 28.66it/s] 33%|███▎      | 4439/13630 [02:15<05:12, 29.46it/s] 33%|███▎      | 4444/13630 [02:15<04:39, 32.89it/s] 33%|███▎      | 4449/13630 [02:15<04:42, 32.52it/s] 33%|███▎      | 4453/13630 [02:15<05:02, 30.35it/s] 33%|███▎      | 4457/13630 [02:15<05:21, 28.56it/s] 33%|███▎      | 4460/13630 [02:15<05:31, 27.63it/s] 33%|███▎      | 4465/13630 [02:15<04:46, 31.93it/s] 33%|███▎      | 4469/13630 [02:15<04:37, 33.00it/s] 33%|███▎      | 4473/13630 [02:16<05:45, 26.49it/s] 33%|███▎      | 4477/13630 [02:16<05:20, 28.56it/s] 33%|███▎      | 4482/13630 [02:16<04:43, 32.28it/s] 33%|███▎      | 4486/13630 [02:16<04:52, 31.31it/s] 33%|███▎      | 4490/13630 [02:16<04:40, 32.60it/s] 33%|███▎      | 4495/13630 [02:16<04:41, 32.43it/s] 33%|███▎      | 4499/13630 [02:16<04:31, 33.63it/s] 33%|███▎      | 4503/13630 [02:17<05:01, 30.31it/s] 33%|███▎      | 4507/13630 [02:17<04:56, 30.80it/s] 33%|███▎      | 4511/13630 [02:17<04:59, 30.40it/s] 33%|███▎      | 4515/13630 [02:17<04:46, 31.82it/s] 33%|███▎      | 4519/13630 [02:17<05:08, 29.53it/s] 33%|███▎      | 4523/13630 [02:17<04:46, 31.76it/s] 33%|███▎      | 4527/13630 [02:17<04:56, 30.65it/s] 33%|███▎      | 4532/13630 [02:18<04:50, 31.28it/s] 33%|███▎      | 4536/13630 [02:18<05:54, 25.68it/s] 33%|███▎      | 4540/13630 [02:18<05:30, 27.51it/s] 33%|███▎      | 4544/13630 [02:18<05:07, 29.51it/s] 33%|███▎      | 4548/13630 [02:18<06:01, 25.09it/s] 33%|███▎      | 4551/13630 [02:18<06:29, 23.31it/s] 33%|███▎      | 4555/13630 [02:19<06:36, 22.89it/s] 33%|███▎      | 4559/13630 [02:19<06:13, 24.32it/s] 33%|███▎      | 4563/13630 [02:19<05:45, 26.27it/s] 33%|███▎      | 4566/13630 [02:19<05:47, 26.09it/s] 34%|███▎      | 4571/13630 [02:19<05:24, 27.91it/s] 34%|███▎      | 4574/13630 [02:19<05:31, 27.30it/s] 34%|███▎      | 4577/13630 [02:19<05:44, 26.30it/s] 34%|███▎      | 4581/13630 [02:19<05:05, 29.62it/s] 34%|███▎      | 4585/13630 [02:20<04:40, 32.27it/s] 34%|███▎      | 4590/13630 [02:20<04:21, 34.63it/s] 34%|███▎      | 4594/13630 [02:20<08:08, 18.48it/s] 34%|███▎      | 4599/13630 [02:20<06:37, 22.74it/s] 34%|███▍      | 4603/13630 [02:20<06:14, 24.10it/s] 34%|███▍      | 4607/13630 [02:21<05:53, 25.53it/s] 34%|███▍      | 4611/13630 [02:21<05:44, 26.14it/s] 34%|███▍      | 4614/13630 [02:21<06:10, 24.36it/s] 34%|███▍      | 4618/13630 [02:21<05:43, 26.27it/s] 34%|███▍      | 4622/13630 [02:21<05:14, 28.61it/s] 34%|███▍      | 4626/13630 [02:21<05:40, 26.46it/s] 34%|███▍      | 4630/13630 [02:21<05:26, 27.57it/s] 34%|███▍      | 4633/13630 [02:22<06:20, 23.61it/s] 34%|███▍      | 4636/13630 [02:22<06:02, 24.82it/s] 34%|███▍      | 4640/13630 [02:22<05:43, 26.18it/s] 34%|███▍      | 4643/13630 [02:22<06:07, 24.42it/s] 34%|███▍      | 4646/13630 [02:22<08:49, 16.97it/s] 34%|███▍      | 4649/13630 [02:22<08:14, 18.16it/s] 34%|███▍      | 4653/13630 [02:22<06:44, 22.17it/s] 34%|███▍      | 4657/13630 [02:23<06:24, 23.35it/s] 34%|███▍      | 4661/13630 [02:23<05:43, 26.12it/s] 34%|███▍      | 4665/13630 [02:23<05:13, 28.56it/s] 34%|███▍      | 4669/13630 [02:23<04:52, 30.59it/s] 34%|███▍      | 4673/13630 [02:23<04:37, 32.27it/s] 34%|███▍      | 4677/13630 [02:23<04:25, 33.66it/s] 34%|███▍      | 4681/13630 [02:23<04:58, 30.02it/s] 34%|███▍      | 4685/13630 [02:24<06:28, 23.05it/s] 34%|███▍      | 4689/13630 [02:24<05:44, 25.98it/s] 34%|███▍      | 4692/13630 [02:24<05:57, 25.03it/s] 34%|███▍      | 4696/13630 [02:24<05:18, 28.07it/s] 34%|███▍      | 4700/13630 [02:24<04:52, 30.51it/s] 35%|███▍      | 4704/13630 [02:24<04:32, 32.77it/s] 35%|███▍      | 4709/13630 [02:24<04:09, 35.79it/s] 35%|███▍      | 4714/13630 [02:24<03:57, 37.56it/s] 35%|███▍      | 4718/13630 [02:25<04:42, 31.55it/s] 35%|███▍      | 4722/13630 [02:25<04:26, 33.37it/s] 35%|███▍      | 4727/13630 [02:25<04:36, 32.23it/s] 35%|███▍      | 4731/13630 [02:25<05:07, 28.94it/s] 35%|███▍      | 4735/13630 [02:25<05:03, 29.32it/s] 35%|███▍      | 4740/13630 [02:25<04:32, 32.64it/s] 35%|███▍      | 4744/13630 [02:25<04:31, 32.74it/s] 35%|███▍      | 4748/13630 [02:26<04:32, 32.55it/s] 35%|███▍      | 4752/13630 [02:26<04:42, 31.46it/s] 35%|███▍      | 4756/13630 [02:26<04:25, 33.39it/s] 35%|███▍      | 4760/13630 [02:26<04:19, 34.18it/s] 35%|███▍      | 4764/13630 [02:26<04:47, 30.85it/s] 35%|███▍      | 4769/13630 [02:26<04:22, 33.77it/s] 35%|███▌      | 4773/13630 [02:26<04:55, 30.01it/s] 35%|███▌      | 4777/13630 [02:26<04:46, 30.92it/s] 35%|███▌      | 4781/13630 [02:27<04:32, 32.42it/s] 35%|███▌      | 4785/13630 [02:27<04:34, 32.23it/s] 35%|███▌      | 4789/13630 [02:27<04:36, 31.93it/s] 35%|███▌      | 4794/13630 [02:27<04:14, 34.68it/s] 35%|███▌      | 4798/13630 [02:27<04:27, 33.00it/s] 35%|███▌      | 4802/13630 [02:27<04:18, 34.20it/s] 35%|███▌      | 4807/13630 [02:27<04:01, 36.47it/s] 35%|███▌      | 4811/13630 [02:27<04:17, 34.21it/s] 35%|███▌      | 4815/13630 [02:28<04:39, 31.57it/s] 35%|███▌      | 4819/13630 [02:28<05:00, 29.31it/s] 35%|███▌      | 4823/13630 [02:28<04:58, 29.48it/s] 35%|███▌      | 4826/13630 [02:28<05:04, 28.92it/s] 35%|███▌      | 4830/13630 [02:28<05:25, 27.04it/s] 35%|███▌      | 4835/13630 [02:28<04:43, 31.01it/s] 36%|███▌      | 4839/13630 [02:28<04:32, 32.23it/s] 36%|███▌      | 4843/13630 [02:29<04:45, 30.75it/s] 36%|███▌      | 4847/13630 [02:29<04:56, 29.61it/s] 36%|███▌      | 4851/13630 [02:29<05:00, 29.23it/s] 36%|███▌      | 4855/13630 [02:29<04:47, 30.54it/s] 36%|███▌      | 4859/13630 [02:29<05:01, 29.10it/s] 36%|███▌      | 4862/13630 [02:29<05:23, 27.13it/s] 36%|███▌      | 4865/13630 [02:29<05:17, 27.61it/s] 36%|███▌      | 4869/13630 [02:29<05:22, 27.13it/s] 36%|███▌      | 4873/13630 [02:30<04:51, 30.07it/s] 36%|███▌      | 4878/13630 [02:30<04:22, 33.31it/s] 36%|███▌      | 4882/13630 [02:30<04:32, 32.06it/s] 36%|███▌      | 4886/13630 [02:30<04:34, 31.87it/s] 36%|███▌      | 4890/13630 [02:30<04:34, 31.89it/s] 36%|███▌      | 4894/13630 [02:30<04:31, 32.22it/s] 36%|███▌      | 4898/13630 [02:30<04:22, 33.33it/s] 36%|███▌      | 4903/13630 [02:30<04:01, 36.19it/s] 36%|███▌      | 4907/13630 [02:31<04:25, 32.90it/s] 36%|███▌      | 4912/13630 [02:31<04:07, 35.26it/s] 36%|███▌      | 4916/13630 [02:31<04:30, 32.23it/s] 36%|███▌      | 4920/13630 [02:31<04:16, 33.99it/s] 36%|███▌      | 4924/13630 [02:31<04:05, 35.46it/s] 36%|███▌      | 4928/13630 [02:31<04:18, 33.63it/s] 36%|███▌      | 4933/13630 [02:31<04:00, 36.10it/s] 36%|███▌      | 4938/13630 [02:31<03:50, 37.72it/s] 36%|███▋      | 4942/13630 [02:32<03:59, 36.32it/s] 36%|███▋      | 4946/13630 [02:32<03:58, 36.43it/s] 36%|███▋      | 4951/13630 [02:32<03:47, 38.18it/s] 36%|███▋      | 4955/13630 [02:32<04:09, 34.79it/s] 36%|███▋      | 4959/13630 [02:32<04:15, 33.88it/s] 36%|███▋      | 4963/13630 [02:32<04:28, 32.23it/s] 36%|███▋      | 4967/13630 [02:32<04:19, 33.44it/s] 36%|███▋      | 4971/13630 [02:32<04:12, 34.23it/s] 37%|███▋      | 4975/13630 [02:33<04:32, 31.73it/s] 37%|███▋      | 4979/13630 [02:33<04:18, 33.50it/s] 37%|███▋      | 4983/13630 [02:33<04:20, 33.25it/s] 37%|███▋      | 4988/13630 [02:33<04:03, 35.42it/s] 37%|███▋      | 4992/13630 [02:33<04:35, 31.41it/s] 37%|███▋      | 4996/13630 [02:33<05:13, 27.53it/s] 37%|███▋      | 5000/13630 [02:33<04:56, 29.13it/s] 37%|███▋      | 5004/13630 [02:33<04:36, 31.17it/s] 37%|███▋      | 5008/13630 [02:34<05:18, 27.04it/s] 37%|███▋      | 5011/13630 [02:34<06:16, 22.90it/s] 37%|███▋      | 5015/13630 [02:34<05:27, 26.31it/s] 37%|███▋      | 5018/13630 [02:34<05:36, 25.61it/s] 37%|███▋      | 5021/13630 [02:34<05:44, 24.99it/s] 37%|███▋      | 5024/13630 [02:34<05:30, 26.05it/s] 37%|███▋      | 5027/13630 [02:34<05:51, 24.45it/s] 37%|███▋      | 5030/13630 [02:35<05:57, 24.07it/s] 37%|███▋      | 5034/13630 [02:35<05:17, 27.07it/s] 37%|███▋      | 5038/13630 [02:35<04:47, 29.87it/s] 37%|███▋      | 5042/13630 [02:35<04:30, 31.73it/s] 37%|███▋      | 5046/13630 [02:35<04:42, 30.43it/s] 37%|███▋      | 5050/13630 [02:35<04:44, 30.20it/s] 37%|███▋      | 5054/13630 [02:35<04:23, 32.59it/s] 37%|███▋      | 5058/13630 [02:36<05:15, 27.20it/s] 37%|███▋      | 5061/13630 [02:36<05:07, 27.83it/s] 37%|███▋      | 5065/13630 [02:36<04:41, 30.48it/s] 37%|███▋      | 5069/13630 [02:36<05:10, 27.53it/s] 37%|███▋      | 5074/13630 [02:36<04:33, 31.24it/s] 37%|███▋      | 5079/13630 [02:36<04:37, 30.87it/s] 37%|███▋      | 5083/13630 [02:36<05:11, 27.43it/s] 37%|███▋      | 5086/13630 [02:36<05:14, 27.15it/s] 37%|███▋      | 5089/13630 [02:37<06:09, 23.14it/s] 37%|███▋      | 5092/13630 [02:37<06:10, 23.04it/s] 37%|███▋      | 5096/13630 [02:37<05:32, 25.70it/s] 37%|███▋      | 5099/13630 [02:37<05:22, 26.43it/s] 37%|███▋      | 5103/13630 [02:37<04:55, 28.86it/s] 37%|███▋      | 5108/13630 [02:37<04:22, 32.51it/s] 38%|███▊      | 5112/13630 [02:37<04:24, 32.21it/s] 38%|███▊      | 5116/13630 [02:38<04:11, 33.85it/s] 38%|███▊      | 5121/13630 [02:38<03:55, 36.09it/s] 38%|███▊      | 5125/13630 [02:38<03:50, 36.87it/s] 38%|███▊      | 5129/13630 [02:38<03:51, 36.65it/s] 38%|███▊      | 5133/13630 [02:38<03:51, 36.72it/s] 38%|███▊      | 5137/13630 [02:38<03:55, 36.04it/s] 38%|███▊      | 5141/13630 [02:38<03:54, 36.21it/s] 38%|███▊      | 5146/13630 [02:38<03:39, 38.58it/s] 38%|███▊      | 5150/13630 [02:38<03:39, 38.58it/s] 38%|███▊      | 5154/13630 [02:39<03:50, 36.80it/s] 38%|███▊      | 5158/13630 [02:39<03:50, 36.81it/s] 38%|███▊      | 5162/13630 [02:39<03:53, 36.33it/s] 38%|███▊      | 5166/13630 [02:39<03:47, 37.13it/s] 38%|███▊      | 5170/13630 [02:39<03:44, 37.75it/s] 38%|███▊      | 5174/13630 [02:39<04:15, 33.11it/s] 38%|███▊      | 5178/13630 [02:39<04:02, 34.82it/s] 38%|███▊      | 5182/13630 [02:39<05:06, 27.56it/s] 38%|███▊      | 5186/13630 [02:40<04:42, 29.93it/s] 38%|███▊      | 5190/13630 [02:40<04:26, 31.62it/s] 38%|███▊      | 5194/13630 [02:40<05:17, 26.57it/s] 38%|███▊      | 5199/13630 [02:40<04:40, 30.08it/s] 38%|███▊      | 5203/13630 [02:40<05:42, 24.59it/s] 38%|███▊      | 5206/13630 [02:40<05:52, 23.89it/s] 38%|███▊      | 5209/13630 [02:40<05:45, 24.34it/s] 38%|███▊      | 5213/13630 [02:41<05:02, 27.86it/s] 38%|███▊      | 5217/13630 [02:41<05:08, 27.28it/s] 38%|███▊      | 5221/13630 [02:41<04:42, 29.76it/s] 38%|███▊      | 5225/13630 [02:41<04:43, 29.60it/s] 38%|███▊      | 5229/13630 [02:41<04:23, 31.92it/s] 38%|███▊      | 5233/13630 [02:41<04:11, 33.45it/s] 38%|███▊      | 5237/13630 [02:41<04:24, 31.72it/s] 38%|███▊      | 5241/13630 [02:41<04:45, 29.42it/s] 38%|███▊      | 5245/13630 [02:42<04:41, 29.76it/s] 39%|███▊      | 5249/13630 [02:42<04:20, 32.12it/s] 39%|███▊      | 5253/13630 [02:42<04:05, 34.14it/s] 39%|███▊      | 5258/13630 [02:42<03:49, 36.40it/s] 39%|███▊      | 5263/13630 [02:42<03:43, 37.48it/s] 39%|███▊      | 5268/13630 [02:42<03:36, 38.62it/s] 39%|███▊      | 5272/13630 [02:42<03:51, 36.07it/s] 39%|███▊      | 5276/13630 [02:42<04:19, 32.17it/s] 39%|███▊      | 5280/13630 [02:43<04:07, 33.72it/s] 39%|███▉      | 5285/13630 [02:43<03:53, 35.73it/s] 39%|███▉      | 5289/13630 [02:43<04:10, 33.30it/s] 39%|███▉      | 5293/13630 [02:43<04:25, 31.44it/s] 39%|███▉      | 5298/13630 [02:43<04:07, 33.72it/s] 39%|███▉      | 5302/13630 [02:43<04:01, 34.54it/s] 39%|███▉      | 5306/13630 [02:43<03:54, 35.47it/s] 39%|███▉      | 5310/13630 [02:43<04:40, 29.69it/s] 39%|███▉      | 5315/13630 [02:44<04:41, 29.58it/s] 39%|███▉      | 5319/13630 [02:44<04:22, 31.60it/s] 39%|███▉      | 5323/13630 [02:44<05:03, 27.34it/s] 39%|███▉      | 5328/13630 [02:44<04:28, 30.94it/s] 39%|███▉      | 5332/13630 [02:44<04:14, 32.60it/s] 39%|███▉      | 5336/13630 [02:44<04:01, 34.31it/s] 39%|███▉      | 5340/13630 [02:45<05:14, 26.36it/s] 39%|███▉      | 5344/13630 [02:45<05:50, 23.66it/s] 39%|███▉      | 5347/13630 [02:45<06:41, 20.63it/s] 39%|███▉      | 5351/13630 [02:45<06:03, 22.76it/s] 39%|███▉      | 5355/13630 [02:45<05:39, 24.36it/s] 39%|███▉      | 5358/13630 [02:45<05:34, 24.71it/s] 39%|███▉      | 5362/13630 [02:45<05:01, 27.40it/s] 39%|███▉      | 5366/13630 [02:46<05:04, 27.15it/s] 39%|███▉      | 5371/13630 [02:46<04:33, 30.19it/s] 39%|███▉      | 5376/13630 [02:46<04:07, 33.38it/s] 39%|███▉      | 5381/13630 [02:46<03:50, 35.78it/s] 40%|███▉      | 5385/13630 [02:46<04:19, 31.80it/s] 40%|███▉      | 5389/13630 [02:46<04:07, 33.27it/s] 40%|███▉      | 5393/13630 [02:46<04:34, 30.01it/s] 40%|███▉      | 5397/13630 [02:47<04:34, 29.95it/s] 40%|███▉      | 5401/13630 [02:47<04:33, 30.09it/s] 40%|███▉      | 5405/13630 [02:47<04:13, 32.42it/s] 40%|███▉      | 5409/13630 [02:47<04:26, 30.80it/s] 40%|███▉      | 5413/13630 [02:47<04:36, 29.74it/s] 40%|███▉      | 5418/13630 [02:47<04:07, 33.13it/s] 40%|███▉      | 5422/13630 [02:47<03:57, 34.57it/s] 40%|███▉      | 5427/13630 [02:47<03:40, 37.14it/s] 40%|███▉      | 5431/13630 [02:48<04:37, 29.52it/s] 40%|███▉      | 5435/13630 [02:48<04:18, 31.76it/s] 40%|███▉      | 5439/13630 [02:48<04:09, 32.85it/s] 40%|███▉      | 5443/13630 [02:48<04:35, 29.73it/s] 40%|███▉      | 5448/13630 [02:48<04:09, 32.85it/s] 40%|████      | 5452/13630 [02:48<04:09, 32.82it/s] 40%|████      | 5457/13630 [02:48<03:52, 35.13it/s] 40%|████      | 5462/13630 [02:48<03:50, 35.47it/s] 40%|████      | 5467/13630 [02:49<03:52, 35.16it/s] 40%|████      | 5471/13630 [02:49<04:02, 33.63it/s] 40%|████      | 5476/13630 [02:49<03:48, 35.75it/s] 40%|████      | 5480/13630 [02:49<03:41, 36.75it/s] 40%|████      | 5484/13630 [02:49<03:42, 36.68it/s] 40%|████      | 5488/13630 [02:49<04:27, 30.47it/s] 40%|████      | 5492/13630 [02:49<04:16, 31.67it/s] 40%|████      | 5497/13630 [02:50<03:54, 34.61it/s] 40%|████      | 5501/13630 [02:50<05:05, 26.58it/s] 40%|████      | 5505/13630 [02:50<05:10, 26.15it/s] 40%|████      | 5509/13630 [02:50<04:39, 29.03it/s] 40%|████      | 5513/13630 [02:50<04:20, 31.20it/s] 40%|████      | 5517/13630 [02:50<04:46, 28.32it/s] 41%|████      | 5521/13630 [02:50<04:47, 28.25it/s] 41%|████      | 5525/13630 [02:51<04:29, 30.09it/s] 41%|████      | 5529/13630 [02:51<04:37, 29.21it/s] 41%|████      | 5533/13630 [02:51<04:37, 29.22it/s] 41%|████      | 5536/13630 [02:51<04:49, 27.99it/s] 41%|████      | 5540/13630 [02:51<04:53, 27.58it/s] 41%|████      | 5545/13630 [02:51<04:42, 28.57it/s] 41%|████      | 5549/13630 [02:51<04:26, 30.32it/s] 41%|████      | 5553/13630 [02:52<04:28, 30.05it/s] 41%|████      | 5557/13630 [02:52<04:10, 32.18it/s] 41%|████      | 5561/13630 [02:52<04:02, 33.29it/s] 41%|████      | 5565/13630 [02:52<04:02, 33.23it/s] 41%|████      | 5570/13630 [02:52<03:49, 35.19it/s] 41%|████      | 5574/13630 [02:52<05:04, 26.47it/s] 41%|████      | 5577/13630 [02:52<05:02, 26.60it/s] 41%|████      | 5581/13630 [02:52<04:39, 28.81it/s] 41%|████      | 5585/13630 [02:53<04:20, 30.89it/s] 41%|████      | 5589/13630 [02:53<04:02, 33.13it/s] 41%|████      | 5593/13630 [02:53<04:15, 31.44it/s] 41%|████      | 5597/13630 [02:53<04:39, 28.73it/s] 41%|████      | 5601/13630 [02:53<04:56, 27.05it/s] 41%|████      | 5605/13630 [02:53<04:35, 29.14it/s] 41%|████      | 5609/13630 [02:54<06:24, 20.84it/s] 41%|████      | 5613/13630 [02:54<05:59, 22.28it/s] 41%|████      | 5617/13630 [02:54<05:17, 25.22it/s] 41%|████      | 5620/13630 [02:54<05:13, 25.56it/s] 41%|████▏     | 5623/13630 [02:54<05:14, 25.50it/s] 41%|████▏     | 5627/13630 [02:54<04:46, 27.94it/s] 41%|████▏     | 5631/13630 [02:54<04:26, 30.00it/s] 41%|████▏     | 5635/13630 [02:54<04:35, 29.02it/s] 41%|████▏     | 5639/13630 [02:55<04:38, 28.65it/s] 41%|████▏     | 5643/13630 [02:55<04:31, 29.39it/s] 41%|████▏     | 5647/13630 [02:55<04:24, 30.22it/s] 41%|████▏     | 5651/13630 [02:55<04:23, 30.28it/s] 41%|████▏     | 5655/13630 [02:55<04:22, 30.33it/s] 42%|████▏     | 5659/13630 [02:55<04:06, 32.29it/s] 42%|████▏     | 5663/13630 [02:55<03:53, 34.18it/s] 42%|████▏     | 5667/13630 [02:55<04:13, 31.38it/s] 42%|████▏     | 5671/13630 [02:56<04:21, 30.45it/s] 42%|████▏     | 5675/13630 [02:56<04:23, 30.14it/s] 42%|████▏     | 5679/13630 [02:56<04:24, 30.04it/s] 42%|████▏     | 5683/13630 [02:56<05:21, 24.73it/s] 42%|████▏     | 5687/13630 [02:56<04:46, 27.68it/s] 42%|████▏     | 5691/13630 [02:56<04:25, 29.91it/s] 42%|████▏     | 5695/13630 [02:56<04:28, 29.60it/s] 42%|████▏     | 5699/13630 [02:57<04:28, 29.50it/s] 42%|████▏     | 5704/13630 [02:57<03:59, 33.07it/s] 42%|████▏     | 5708/13630 [02:57<04:02, 32.65it/s] 42%|████▏     | 5712/13630 [02:57<03:51, 34.14it/s] 42%|████▏     | 5717/13630 [02:57<04:01, 32.79it/s] 42%|████▏     | 5721/13630 [02:57<03:50, 34.38it/s] 42%|████▏     | 5725/13630 [02:57<04:04, 32.35it/s] 42%|████▏     | 5729/13630 [02:57<03:53, 33.78it/s] 42%|████▏     | 5734/13630 [02:58<03:41, 35.62it/s] 42%|████▏     | 5738/13630 [02:58<03:52, 33.90it/s] 42%|████▏     | 5742/13630 [02:58<05:09, 25.46it/s] 42%|████▏     | 5746/13630 [02:58<04:39, 28.24it/s] 42%|████▏     | 5750/13630 [02:58<04:15, 30.87it/s] 42%|████▏     | 5754/13630 [02:58<04:05, 32.09it/s] 42%|████▏     | 5758/13630 [02:58<04:21, 30.08it/s] 42%|████▏     | 5763/13630 [02:59<04:22, 29.93it/s] 42%|████▏     | 5767/13630 [02:59<05:04, 25.85it/s] 42%|████▏     | 5770/13630 [02:59<04:56, 26.48it/s] 42%|████▏     | 5773/13630 [02:59<04:56, 26.46it/s] 42%|████▏     | 5776/13630 [02:59<05:04, 25.77it/s] 42%|████▏     | 5779/13630 [02:59<05:14, 24.94it/s] 42%|████▏     | 5784/13630 [02:59<04:26, 29.39it/s] 42%|████▏     | 5788/13630 [02:59<04:10, 31.30it/s] 42%|████▏     | 5792/13630 [03:00<04:18, 30.30it/s] 43%|████▎     | 5796/13630 [03:00<04:42, 27.78it/s] 43%|████▎     | 5800/13630 [03:00<04:17, 30.38it/s] 43%|████▎     | 5805/13630 [03:00<03:51, 33.78it/s] 43%|████▎     | 5809/13630 [03:00<03:41, 35.30it/s] 43%|████▎     | 5813/13630 [03:00<03:58, 32.83it/s] 43%|████▎     | 5817/13630 [03:00<03:58, 32.73it/s] 43%|████▎     | 5821/13630 [03:01<03:54, 33.27it/s] 43%|████▎     | 5825/13630 [03:01<03:45, 34.63it/s] 43%|████▎     | 5830/13630 [03:01<03:34, 36.35it/s] 43%|████▎     | 5834/13630 [03:01<03:45, 34.57it/s] 43%|████▎     | 5838/13630 [03:01<03:59, 32.54it/s] 43%|████▎     | 5843/13630 [03:01<03:38, 35.68it/s] 43%|████▎     | 5847/13630 [03:01<03:53, 33.37it/s] 43%|████▎     | 5852/13630 [03:01<04:16, 30.34it/s] 43%|████▎     | 5857/13630 [03:02<04:06, 31.55it/s] 43%|████▎     | 5862/13630 [03:02<03:44, 34.54it/s] 43%|████▎     | 5866/13630 [03:02<03:44, 34.56it/s] 43%|████▎     | 5870/13630 [03:02<04:11, 30.89it/s] 43%|████▎     | 5874/13630 [03:02<04:24, 29.37it/s] 43%|████▎     | 5878/13630 [03:02<04:59, 25.85it/s] 43%|████▎     | 5882/13630 [03:02<04:41, 27.56it/s] 43%|████▎     | 5887/13630 [03:03<04:09, 31.05it/s] 43%|████▎     | 5891/13630 [03:03<03:59, 32.27it/s] 43%|████▎     | 5895/13630 [03:03<04:12, 30.63it/s] 43%|████▎     | 5899/13630 [03:03<04:27, 28.90it/s] 43%|████▎     | 5903/13630 [03:03<04:17, 30.00it/s] 43%|████▎     | 5907/13630 [03:03<04:31, 28.49it/s] 43%|████▎     | 5910/13630 [03:03<04:41, 27.43it/s] 43%|████▎     | 5914/13630 [03:04<04:26, 28.92it/s] 43%|████▎     | 5918/13630 [03:04<04:19, 29.72it/s] 43%|████▎     | 5921/13630 [03:04<06:37, 19.39it/s] 43%|████▎     | 5924/13630 [03:04<06:29, 19.77it/s] 43%|████▎     | 5927/13630 [03:04<06:28, 19.81it/s] 44%|████▎     | 5931/13630 [03:04<05:30, 23.27it/s] 44%|████▎     | 5934/13630 [03:04<05:15, 24.42it/s] 44%|████▎     | 5937/13630 [03:05<05:20, 24.03it/s] 44%|████▎     | 5940/13630 [03:05<05:33, 23.07it/s] 44%|████▎     | 5944/13630 [03:05<04:57, 25.85it/s] 44%|████▎     | 5948/13630 [03:05<05:35, 22.90it/s] 44%|████▎     | 5952/13630 [03:05<04:50, 26.42it/s] 44%|████▎     | 5956/13630 [03:05<04:19, 29.54it/s] 44%|████▎     | 5961/13630 [03:05<03:59, 31.97it/s] 44%|████▍     | 5965/13630 [03:06<03:51, 33.16it/s] 44%|████▍     | 5969/13630 [03:06<03:58, 32.09it/s] 44%|████▍     | 5973/13630 [03:06<04:31, 28.18it/s] 44%|████▍     | 5976/13630 [03:06<04:41, 27.23it/s] 44%|████▍     | 5979/13630 [03:06<05:00, 25.48it/s] 44%|████▍     | 5983/13630 [03:06<04:41, 27.16it/s] 44%|████▍     | 5986/13630 [03:06<04:43, 26.96it/s] 44%|████▍     | 5990/13630 [03:06<04:26, 28.68it/s] 44%|████▍     | 5994/13630 [03:07<04:16, 29.79it/s] 44%|████▍     | 5998/13630 [03:07<04:23, 28.94it/s] 44%|████▍     | 6003/13630 [03:07<03:52, 32.86it/s] 44%|████▍     | 6007/13630 [03:07<03:58, 31.98it/s] 44%|████▍     | 6011/13630 [03:07<04:07, 30.78it/s] 44%|████▍     | 6016/13630 [03:07<03:44, 33.88it/s] 44%|████▍     | 6021/13630 [03:07<03:27, 36.66it/s] 44%|████▍     | 6025/13630 [03:08<04:00, 31.63it/s] 44%|████▍     | 6029/13630 [03:08<03:47, 33.46it/s] 44%|████▍     | 6033/13630 [03:08<04:00, 31.59it/s] 44%|████▍     | 6037/13630 [03:08<04:06, 30.80it/s] 44%|████▍     | 6041/13630 [03:08<03:54, 32.37it/s] 44%|████▍     | 6045/13630 [03:08<04:34, 27.61it/s] 44%|████▍     | 6050/13630 [03:08<04:03, 31.11it/s] 44%|████▍     | 6054/13630 [03:08<04:04, 31.04it/s] 44%|████▍     | 6059/13630 [03:09<03:38, 34.69it/s] 44%|████▍     | 6063/13630 [03:09<03:41, 34.17it/s] 45%|████▍     | 6068/13630 [03:09<03:28, 36.19it/s] 45%|████▍     | 6072/13630 [03:09<03:51, 32.65it/s] 45%|████▍     | 6076/13630 [03:09<04:20, 29.02it/s] 45%|████▍     | 6080/13630 [03:09<04:01, 31.29it/s] 45%|████▍     | 6085/13630 [03:09<03:54, 32.16it/s] 45%|████▍     | 6090/13630 [03:10<03:35, 35.04it/s] 45%|████▍     | 6094/13630 [03:10<03:45, 33.39it/s] 45%|████▍     | 6098/13630 [03:10<03:47, 33.08it/s] 45%|████▍     | 6103/13630 [03:10<03:30, 35.70it/s] 45%|████▍     | 6108/13630 [03:10<03:23, 36.92it/s] 45%|████▍     | 6112/13630 [03:10<03:38, 34.42it/s] 45%|████▍     | 6117/13630 [03:10<03:27, 36.15it/s] 45%|████▍     | 6121/13630 [03:10<03:23, 36.88it/s] 45%|████▍     | 6125/13630 [03:11<04:07, 30.35it/s] 45%|████▍     | 6129/13630 [03:11<04:10, 29.93it/s] 45%|████▍     | 6133/13630 [03:11<04:30, 27.68it/s] 45%|████▌     | 6137/13630 [03:11<04:09, 30.08it/s] 45%|████▌     | 6141/13630 [03:11<04:01, 30.96it/s] 45%|████▌     | 6145/13630 [03:11<04:04, 30.67it/s] 45%|████▌     | 6149/13630 [03:11<03:56, 31.58it/s] 45%|████▌     | 6153/13630 [03:11<03:43, 33.38it/s] 45%|████▌     | 6158/13630 [03:12<03:26, 36.16it/s] 45%|████▌     | 6162/13630 [03:12<03:42, 33.56it/s] 45%|████▌     | 6166/13630 [03:12<03:50, 32.44it/s] 45%|████▌     | 6170/13630 [03:12<03:59, 31.13it/s] 45%|████▌     | 6174/13630 [03:12<04:14, 29.25it/s] 45%|████▌     | 6178/13630 [03:12<04:02, 30.74it/s] 45%|████▌     | 6183/13630 [03:12<03:34, 34.66it/s] 45%|████▌     | 6188/13630 [03:13<03:18, 37.52it/s] 45%|████▌     | 6192/13630 [03:13<03:46, 32.77it/s] 45%|████▌     | 6196/13630 [03:13<03:46, 32.79it/s] 45%|████▌     | 6201/13630 [03:13<03:30, 35.29it/s] 46%|████▌     | 6205/13630 [03:13<03:43, 33.16it/s] 46%|████▌     | 6210/13630 [03:13<03:26, 35.86it/s] 46%|████▌     | 6214/13630 [03:13<03:28, 35.56it/s] 46%|████▌     | 6218/13630 [03:13<03:32, 34.89it/s] 46%|████▌     | 6223/13630 [03:14<03:30, 35.17it/s] 46%|████▌     | 6228/13630 [03:14<03:43, 33.07it/s] 46%|████▌     | 6232/13630 [03:14<04:22, 28.17it/s] 46%|████▌     | 6235/13630 [03:14<04:21, 28.30it/s] 46%|████▌     | 6240/13630 [03:14<03:51, 31.90it/s] 46%|████▌     | 6245/13630 [03:14<03:31, 34.99it/s] 46%|████▌     | 6249/13630 [03:14<03:41, 33.39it/s] 46%|████▌     | 6253/13630 [03:15<03:34, 34.36it/s] 46%|████▌     | 6257/13630 [03:15<03:45, 32.69it/s] 46%|████▌     | 6261/13630 [03:15<04:21, 28.20it/s] 46%|████▌     | 6266/13630 [03:15<03:50, 31.88it/s] 46%|████▌     | 6271/13630 [03:15<03:30, 35.03it/s] 46%|████▌     | 6275/13630 [03:15<04:12, 29.12it/s] 46%|████▌     | 6279/13630 [03:15<04:17, 28.60it/s] 46%|████▌     | 6283/13630 [03:16<04:09, 29.41it/s] 46%|████▌     | 6287/13630 [03:16<03:58, 30.81it/s] 46%|████▌     | 6291/13630 [03:16<03:58, 30.73it/s] 46%|████▌     | 6296/13630 [03:16<03:34, 34.12it/s] 46%|████▌     | 6301/13630 [03:16<03:41, 33.10it/s] 46%|████▋     | 6306/13630 [03:16<03:29, 34.95it/s] 46%|████▋     | 6310/13630 [03:16<03:26, 35.38it/s] 46%|████▋     | 6314/13630 [03:16<03:47, 32.11it/s] 46%|████▋     | 6318/13630 [03:17<04:32, 26.80it/s] 46%|████▋     | 6322/13630 [03:17<04:28, 27.25it/s] 46%|████▋     | 6327/13630 [03:17<03:56, 30.87it/s] 46%|████▋     | 6331/13630 [03:17<04:14, 28.63it/s] 46%|████▋     | 6335/13630 [03:17<04:08, 29.32it/s] 47%|████▋     | 6339/13630 [03:17<03:51, 31.47it/s] 47%|████▋     | 6343/13630 [03:17<03:37, 33.49it/s] 47%|████▋     | 6347/13630 [03:18<04:00, 30.33it/s] 47%|████▋     | 6351/13630 [03:18<04:03, 29.91it/s] 47%|████▋     | 6355/13630 [03:18<04:14, 28.53it/s] 47%|████▋     | 6360/13630 [03:18<03:47, 31.92it/s] 47%|████▋     | 6364/13630 [03:18<03:48, 31.78it/s] 47%|████▋     | 6368/13630 [03:18<03:57, 30.55it/s] 47%|████▋     | 6373/13630 [03:18<03:34, 33.85it/s] 47%|████▋     | 6377/13630 [03:19<03:41, 32.69it/s] 47%|████▋     | 6381/13630 [03:19<04:12, 28.68it/s] 47%|████▋     | 6385/13630 [03:19<04:03, 29.77it/s] 47%|████▋     | 6389/13630 [03:19<04:21, 27.73it/s] 47%|████▋     | 6393/13630 [03:19<04:02, 29.85it/s] 47%|████▋     | 6398/13630 [03:19<03:37, 33.22it/s] 47%|████▋     | 6402/13630 [03:19<03:39, 32.96it/s] 47%|████▋     | 6406/13630 [03:20<04:14, 28.41it/s] 47%|████▋     | 6411/13630 [03:20<03:55, 30.61it/s] 47%|████▋     | 6415/13630 [03:20<03:52, 31.06it/s] 47%|████▋     | 6419/13630 [03:20<03:38, 32.96it/s] 47%|████▋     | 6423/13630 [03:20<04:26, 27.09it/s] 47%|████▋     | 6427/13630 [03:20<04:01, 29.84it/s] 47%|████▋     | 6431/13630 [03:20<03:46, 31.83it/s] 47%|████▋     | 6435/13630 [03:20<04:06, 29.23it/s] 47%|████▋     | 6440/13630 [03:21<03:38, 32.95it/s] 47%|████▋     | 6444/13630 [03:21<03:30, 34.16it/s] 47%|████▋     | 6448/13630 [03:21<03:34, 33.50it/s] 47%|████▋     | 6452/13630 [03:21<03:53, 30.80it/s] 47%|████▋     | 6457/13630 [03:21<03:33, 33.61it/s] 47%|████▋     | 6462/13630 [03:21<03:34, 33.34it/s] 47%|████▋     | 6466/13630 [03:21<04:09, 28.73it/s] 47%|████▋     | 6471/13630 [03:22<03:45, 31.80it/s] 48%|████▊     | 6475/13630 [03:22<03:46, 31.52it/s] 48%|████▊     | 6479/13630 [03:22<03:44, 31.89it/s] 48%|████▊     | 6483/13630 [03:22<03:31, 33.77it/s] 48%|████▊     | 6488/13630 [03:22<03:17, 36.14it/s] 48%|████▊     | 6492/13630 [03:22<03:13, 36.97it/s] 48%|████▊     | 6496/13630 [03:22<03:27, 34.38it/s] 48%|████▊     | 6500/13630 [03:22<03:41, 32.15it/s] 48%|████▊     | 6505/13630 [03:23<03:22, 35.19it/s] 48%|████▊     | 6509/13630 [03:23<03:34, 33.21it/s] 48%|████▊     | 6513/13630 [03:23<03:24, 34.73it/s] 48%|████▊     | 6517/13630 [03:23<03:46, 31.46it/s] 48%|████▊     | 6521/13630 [03:23<03:41, 32.08it/s] 48%|████▊     | 6525/13630 [03:23<03:41, 32.11it/s] 48%|████▊     | 6529/13630 [03:23<03:33, 33.29it/s] 48%|████▊     | 6533/13630 [03:23<03:22, 34.99it/s] 48%|████▊     | 6538/13630 [03:24<03:09, 37.43it/s] 48%|████▊     | 6543/13630 [03:24<03:03, 38.60it/s] 48%|████▊     | 6547/13630 [03:24<03:11, 36.99it/s] 48%|████▊     | 6551/13630 [03:24<03:48, 31.02it/s] 48%|████▊     | 6555/13630 [03:24<03:44, 31.54it/s] 48%|████▊     | 6559/13630 [03:24<03:50, 30.65it/s] 48%|████▊     | 6563/13630 [03:24<04:01, 29.21it/s] 48%|████▊     | 6566/13630 [03:24<04:01, 29.21it/s] 48%|████▊     | 6570/13630 [03:25<03:44, 31.45it/s] 48%|████▊     | 6574/13630 [03:25<03:52, 30.39it/s] 48%|████▊     | 6578/13630 [03:25<03:46, 31.07it/s] 48%|████▊     | 6582/13630 [03:25<03:50, 30.61it/s] 48%|████▊     | 6586/13630 [03:25<03:54, 29.99it/s] 48%|████▊     | 6591/13630 [03:25<03:29, 33.57it/s] 48%|████▊     | 6596/13630 [03:25<03:15, 35.93it/s] 48%|████▊     | 6600/13630 [03:25<03:41, 31.67it/s] 48%|████▊     | 6605/13630 [03:26<03:23, 34.50it/s] 48%|████▊     | 6609/13630 [03:26<03:53, 30.01it/s] 49%|████▊     | 6613/13630 [03:26<04:09, 28.14it/s] 49%|████▊     | 6616/13630 [03:26<04:34, 25.52it/s] 49%|████▊     | 6619/13630 [03:26<05:02, 23.20it/s] 49%|████▊     | 6623/13630 [03:26<04:23, 26.62it/s] 49%|████▊     | 6626/13630 [03:27<04:32, 25.75it/s] 49%|████▊     | 6630/13630 [03:27<04:00, 29.09it/s] 49%|████▊     | 6634/13630 [03:27<04:34, 25.52it/s] 49%|████▊     | 6638/13630 [03:27<04:17, 27.16it/s] 49%|████▊     | 6642/13630 [03:27<04:16, 27.20it/s] 49%|████▉     | 6645/13630 [03:27<04:22, 26.62it/s] 49%|████▉     | 6649/13630 [03:27<04:06, 28.27it/s] 49%|████▉     | 6653/13630 [03:27<03:55, 29.64it/s] 49%|████▉     | 6657/13630 [03:28<03:56, 29.50it/s] 49%|████▉     | 6661/13630 [03:28<03:58, 29.28it/s] 49%|████▉     | 6665/13630 [03:28<03:50, 30.28it/s] 49%|████▉     | 6669/13630 [03:28<03:34, 32.44it/s] 49%|████▉     | 6673/13630 [03:28<03:35, 32.26it/s] 49%|████▉     | 6677/13630 [03:28<03:27, 33.47it/s] 49%|████▉     | 6681/13630 [03:28<03:26, 33.59it/s] 49%|████▉     | 6685/13630 [03:28<03:17, 35.15it/s] 49%|████▉     | 6690/13630 [03:29<03:23, 34.07it/s] 49%|████▉     | 6695/13630 [03:29<03:24, 33.97it/s] 49%|████▉     | 6699/13630 [03:29<03:26, 33.53it/s] 49%|████▉     | 6703/13630 [03:29<03:35, 32.10it/s] 49%|████▉     | 6707/13630 [03:29<03:42, 31.12it/s] 49%|████▉     | 6711/13630 [03:29<04:33, 25.26it/s] 49%|████▉     | 6715/13630 [03:29<04:05, 28.16it/s] 49%|████▉     | 6720/13630 [03:30<03:36, 31.93it/s] 49%|████▉     | 6724/13630 [03:30<03:24, 33.77it/s] 49%|████▉     | 6729/13630 [03:30<03:12, 35.79it/s] 49%|████▉     | 6733/13630 [03:30<03:10, 36.28it/s] 49%|████▉     | 6738/13630 [03:30<03:20, 34.31it/s] 49%|████▉     | 6742/13630 [03:30<03:33, 32.21it/s] 49%|████▉     | 6746/13630 [03:30<03:30, 32.67it/s] 50%|████▉     | 6751/13630 [03:30<03:17, 34.78it/s] 50%|████▉     | 6756/13630 [03:31<03:06, 36.88it/s] 50%|████▉     | 6760/13630 [03:31<03:02, 37.67it/s] 50%|████▉     | 6764/13630 [03:31<03:21, 34.03it/s] 50%|████▉     | 6768/13630 [03:31<03:21, 34.14it/s] 50%|████▉     | 6772/13630 [03:31<03:28, 32.92it/s] 50%|████▉     | 6776/13630 [03:31<03:34, 31.97it/s] 50%|████▉     | 6780/13630 [03:31<03:50, 29.76it/s] 50%|████▉     | 6784/13630 [03:31<03:54, 29.24it/s] 50%|████▉     | 6789/13630 [03:32<03:30, 32.57it/s] 50%|████▉     | 6793/13630 [03:32<03:34, 31.82it/s] 50%|████▉     | 6797/13630 [03:32<03:23, 33.56it/s] 50%|████▉     | 6801/13630 [03:32<03:46, 30.16it/s] 50%|████▉     | 6806/13630 [03:32<03:23, 33.53it/s] 50%|████▉     | 6810/13630 [03:32<03:44, 30.36it/s] 50%|████▉     | 6814/13630 [03:32<04:26, 25.56it/s] 50%|█████     | 6817/13630 [03:33<04:38, 24.49it/s] 50%|█████     | 6821/13630 [03:33<04:16, 26.54it/s] 50%|█████     | 6824/13630 [03:33<04:11, 27.03it/s] 50%|█████     | 6828/13630 [03:33<03:49, 29.64it/s] 50%|█████     | 6832/13630 [03:33<04:15, 26.64it/s] 50%|█████     | 6835/13630 [03:33<04:11, 27.02it/s] 50%|█████     | 6840/13630 [03:33<03:42, 30.58it/s] 50%|█████     | 6844/13630 [03:33<03:27, 32.77it/s] 50%|█████     | 6848/13630 [03:34<03:26, 32.82it/s] 50%|█████     | 6852/13630 [03:34<03:40, 30.75it/s] 50%|█████     | 6857/13630 [03:34<03:22, 33.46it/s] 50%|█████     | 6861/13630 [03:34<03:29, 32.24it/s] 50%|█████     | 6866/13630 [03:34<03:12, 35.19it/s] 50%|█████     | 6871/13630 [03:34<03:03, 36.76it/s] 50%|█████     | 6875/13630 [03:34<03:43, 30.18it/s] 50%|█████     | 6879/13630 [03:35<05:10, 21.76it/s] 50%|█████     | 6882/13630 [03:35<04:51, 23.11it/s] 51%|█████     | 6885/13630 [03:35<04:36, 24.39it/s] 51%|█████     | 6890/13630 [03:35<03:51, 29.08it/s] 51%|█████     | 6894/13630 [03:35<04:30, 24.92it/s] 51%|█████     | 6897/13630 [03:35<04:38, 24.15it/s] 51%|█████     | 6902/13630 [03:36<03:51, 29.09it/s] 51%|█████     | 6906/13630 [03:36<04:35, 24.44it/s] 51%|█████     | 6910/13630 [03:36<04:54, 22.81it/s] 51%|█████     | 6915/13630 [03:36<04:07, 27.18it/s] 51%|█████     | 6920/13630 [03:36<03:54, 28.66it/s] 51%|█████     | 6925/13630 [03:36<03:47, 29.53it/s] 51%|█████     | 6929/13630 [03:37<03:46, 29.62it/s] 51%|█████     | 6934/13630 [03:37<03:27, 32.31it/s] 51%|█████     | 6938/13630 [03:37<04:29, 24.79it/s] 51%|█████     | 6942/13630 [03:37<04:13, 26.39it/s] 51%|█████     | 6946/13630 [03:37<04:09, 26.75it/s] 51%|█████     | 6949/13630 [03:37<04:08, 26.85it/s] 51%|█████     | 6954/13630 [03:37<03:37, 30.76it/s] 51%|█████     | 6959/13630 [03:38<03:18, 33.68it/s] 51%|█████     | 6963/13630 [03:38<03:22, 33.00it/s] 51%|█████     | 6967/13630 [03:38<03:24, 32.57it/s] 51%|█████     | 6971/13630 [03:38<03:26, 32.31it/s] 51%|█████     | 6975/13630 [03:38<03:16, 33.88it/s] 51%|█████     | 6979/13630 [03:38<03:34, 31.04it/s] 51%|█████     | 6983/13630 [03:38<03:56, 28.13it/s] 51%|█████▏    | 6986/13630 [03:39<04:24, 25.14it/s] 51%|█████▏    | 6989/13630 [03:39<04:39, 23.79it/s] 51%|█████▏    | 6994/13630 [03:39<04:30, 24.56it/s] 51%|█████▏    | 6999/13630 [03:39<03:49, 28.93it/s] 51%|█████▏    | 7003/13630 [03:39<03:40, 30.10it/s] 51%|█████▏    | 7007/13630 [03:39<03:54, 28.24it/s] 51%|█████▏    | 7010/13630 [03:39<04:18, 25.64it/s] 51%|█████▏    | 7013/13630 [03:40<04:31, 24.38it/s] 51%|█████▏    | 7017/13630 [03:40<04:09, 26.54it/s] 52%|█████▏    | 7021/13630 [03:40<03:46, 29.19it/s] 52%|█████▏    | 7025/13630 [03:40<04:07, 26.65it/s] 52%|█████▏    | 7029/13630 [03:40<03:42, 29.69it/s] 52%|█████▏    | 7033/13630 [03:40<04:09, 26.44it/s] 52%|█████▏    | 7038/13630 [03:40<03:32, 30.98it/s] 52%|█████▏    | 7042/13630 [03:41<04:44, 23.14it/s] 52%|█████▏    | 7047/13630 [03:41<03:58, 27.56it/s] 52%|█████▏    | 7051/13630 [03:41<04:44, 23.12it/s] 52%|█████▏    | 7055/13630 [03:41<04:37, 23.72it/s] 52%|█████▏    | 7058/13630 [03:41<04:28, 24.46it/s] 52%|█████▏    | 7061/13630 [03:41<04:40, 23.39it/s] 52%|█████▏    | 7065/13630 [03:42<04:13, 25.86it/s] 52%|█████▏    | 7069/13630 [03:42<03:47, 28.84it/s] 52%|█████▏    | 7073/13630 [03:42<03:38, 29.99it/s] 52%|█████▏    | 7077/13630 [03:42<03:41, 29.53it/s] 52%|█████▏    | 7081/13630 [03:42<03:45, 29.05it/s] 52%|█████▏    | 7085/13630 [03:42<03:30, 31.12it/s] 52%|█████▏    | 7089/13630 [03:42<03:46, 28.85it/s] 52%|█████▏    | 7094/13630 [03:42<03:20, 32.55it/s] 52%|█████▏    | 7098/13630 [03:43<03:21, 32.49it/s] 52%|█████▏    | 7103/13630 [03:43<03:04, 35.32it/s] 52%|█████▏    | 7107/13630 [03:43<03:01, 35.85it/s] 52%|█████▏    | 7111/13630 [03:43<03:07, 34.70it/s] 52%|█████▏    | 7115/13630 [03:43<03:31, 30.85it/s] 52%|█████▏    | 7120/13630 [03:43<03:13, 33.66it/s] 52%|█████▏    | 7124/13630 [03:43<03:11, 33.97it/s] 52%|█████▏    | 7128/13630 [03:43<03:23, 32.00it/s] 52%|█████▏    | 7132/13630 [03:44<03:46, 28.75it/s] 52%|█████▏    | 7137/13630 [03:44<03:20, 32.38it/s] 52%|█████▏    | 7141/13630 [03:44<03:21, 32.24it/s] 52%|█████▏    | 7145/13630 [03:44<03:19, 32.58it/s] 52%|█████▏    | 7150/13630 [03:44<03:04, 35.14it/s] 52%|█████▏    | 7155/13630 [03:44<02:54, 37.03it/s] 53%|█████▎    | 7159/13630 [03:44<03:05, 34.87it/s] 53%|█████▎    | 7163/13630 [03:44<03:06, 34.74it/s] 53%|█████▎    | 7167/13630 [03:45<03:56, 27.28it/s] 53%|█████▎    | 7172/13630 [03:45<03:30, 30.75it/s] 53%|█████▎    | 7176/13630 [03:45<03:29, 30.74it/s] 53%|█████▎    | 7181/13630 [03:45<03:12, 33.45it/s] 53%|█████▎    | 7186/13630 [03:45<02:59, 35.90it/s] 53%|█████▎    | 7190/13630 [03:45<03:23, 31.71it/s] 53%|█████▎    | 7195/13630 [03:46<03:20, 32.07it/s] 53%|█████▎    | 7199/13630 [03:46<03:42, 28.97it/s] 53%|█████▎    | 7203/13630 [03:46<04:09, 25.79it/s] 53%|█████▎    | 7206/13630 [03:46<04:02, 26.44it/s] 53%|█████▎    | 7209/13630 [03:46<04:15, 25.15it/s] 53%|█████▎    | 7212/13630 [03:46<04:08, 25.80it/s] 53%|█████▎    | 7216/13630 [03:46<04:08, 25.82it/s] 53%|█████▎    | 7220/13630 [03:47<03:56, 27.08it/s] 53%|█████▎    | 7223/13630 [03:47<04:35, 23.26it/s] 53%|█████▎    | 7226/13630 [03:47<04:58, 21.44it/s] 53%|█████▎    | 7229/13630 [03:47<05:01, 21.22it/s] 53%|█████▎    | 7233/13630 [03:47<04:19, 24.64it/s] 53%|█████▎    | 7237/13630 [03:47<03:56, 27.00it/s] 53%|█████▎    | 7240/13630 [03:47<04:31, 23.56it/s] 53%|█████▎    | 7244/13630 [03:48<03:58, 26.78it/s] 53%|█████▎    | 7248/13630 [03:48<03:34, 29.75it/s] 53%|█████▎    | 7252/13630 [03:48<04:31, 23.51it/s] 53%|█████▎    | 7257/13630 [03:48<04:03, 26.22it/s] 53%|█████▎    | 7260/13630 [03:48<04:04, 26.06it/s] 53%|█████▎    | 7263/13630 [03:48<04:08, 25.58it/s] 53%|█████▎    | 7266/13630 [03:48<04:15, 24.87it/s] 53%|█████▎    | 7269/13630 [03:49<04:07, 25.71it/s] 53%|█████▎    | 7274/13630 [03:49<03:29, 30.37it/s] 53%|█████▎    | 7279/13630 [03:49<03:19, 31.86it/s] 53%|█████▎    | 7283/13630 [03:49<03:33, 29.69it/s] 53%|█████▎    | 7287/13630 [03:49<04:40, 22.60it/s] 53%|█████▎    | 7291/13630 [03:49<04:22, 24.11it/s] 54%|█████▎    | 7295/13630 [03:49<03:52, 27.27it/s] 54%|█████▎    | 7299/13630 [03:50<03:51, 27.40it/s] 54%|█████▎    | 7304/13630 [03:50<03:41, 28.60it/s] 54%|█████▎    | 7308/13630 [03:50<03:45, 28.04it/s] 54%|█████▎    | 7313/13630 [03:50<03:41, 28.46it/s] 54%|█████▎    | 7317/13630 [03:50<03:30, 30.00it/s] 54%|█████▎    | 7321/13630 [03:50<03:28, 30.20it/s] 54%|█████▎    | 7325/13630 [03:50<03:27, 30.43it/s] 54%|█████▍    | 7329/13630 [03:51<04:01, 26.08it/s] 54%|█████▍    | 7333/13630 [03:51<03:46, 27.81it/s] 54%|█████▍    | 7336/13630 [03:51<04:02, 25.96it/s] 54%|█████▍    | 7340/13630 [03:51<03:36, 29.01it/s] 54%|█████▍    | 7344/13630 [03:51<03:56, 26.56it/s] 54%|█████▍    | 7347/13630 [03:51<03:57, 26.43it/s] 54%|█████▍    | 7351/13630 [03:51<03:52, 26.97it/s] 54%|█████▍    | 7355/13630 [03:52<03:40, 28.50it/s] 54%|█████▍    | 7358/13630 [03:52<03:38, 28.72it/s] 54%|█████▍    | 7363/13630 [03:52<03:13, 32.40it/s] 54%|█████▍    | 7367/13630 [03:52<03:23, 30.76it/s] 54%|█████▍    | 7371/13630 [03:52<03:20, 31.27it/s] 54%|█████▍    | 7375/13630 [03:52<03:34, 29.12it/s] 54%|█████▍    | 7378/13630 [03:52<04:26, 23.48it/s] 54%|█████▍    | 7382/13630 [03:53<03:53, 26.81it/s] 54%|█████▍    | 7385/13630 [03:53<04:07, 25.27it/s] 54%|█████▍    | 7388/13630 [03:53<05:35, 18.59it/s] 54%|█████▍    | 7392/13630 [03:53<04:39, 22.30it/s] 54%|█████▍    | 7395/13630 [03:53<04:21, 23.80it/s] 54%|█████▍    | 7398/13630 [03:53<04:28, 23.20it/s] 54%|█████▍    | 7401/13630 [03:53<04:59, 20.83it/s] 54%|█████▍    | 7405/13630 [03:54<04:14, 24.43it/s] 54%|█████▍    | 7409/13630 [03:54<03:54, 26.48it/s] 54%|█████▍    | 7412/13630 [03:54<03:59, 25.92it/s] 54%|█████▍    | 7417/13630 [03:54<03:24, 30.40it/s] 54%|█████▍    | 7421/13630 [03:54<03:24, 30.41it/s] 54%|█████▍    | 7425/13630 [03:54<03:22, 30.71it/s] 55%|█████▍    | 7429/13630 [03:54<03:12, 32.16it/s] 55%|█████▍    | 7434/13630 [03:54<02:56, 35.05it/s] 55%|█████▍    | 7439/13630 [03:55<02:48, 36.80it/s] 55%|█████▍    | 7443/13630 [03:55<02:54, 35.50it/s] 55%|█████▍    | 7447/13630 [03:55<03:06, 33.13it/s] 55%|█████▍    | 7451/13630 [03:55<02:57, 34.80it/s] 55%|█████▍    | 7456/13630 [03:55<03:09, 32.66it/s] 55%|█████▍    | 7460/13630 [03:55<03:03, 33.65it/s] 55%|█████▍    | 7464/13630 [03:55<03:29, 29.47it/s] 55%|█████▍    | 7468/13630 [03:56<03:54, 26.28it/s] 55%|█████▍    | 7471/13630 [03:56<03:51, 26.60it/s] 55%|█████▍    | 7476/13630 [03:56<03:22, 30.44it/s] 55%|█████▍    | 7480/13630 [03:56<05:12, 19.70it/s] 55%|█████▍    | 7485/13630 [03:56<04:19, 23.67it/s] 55%|█████▍    | 7490/13630 [03:56<03:42, 27.56it/s] 55%|█████▍    | 7494/13630 [03:57<04:00, 25.47it/s] 55%|█████▌    | 7498/13630 [03:57<03:39, 27.90it/s] 55%|█████▌    | 7502/13630 [03:57<03:23, 30.09it/s] 55%|█████▌    | 7507/13630 [03:57<03:05, 33.01it/s] 55%|█████▌    | 7511/13630 [03:57<03:12, 31.73it/s] 55%|█████▌    | 7515/13630 [03:57<03:30, 29.04it/s] 55%|█████▌    | 7519/13630 [03:57<03:51, 26.35it/s] 55%|█████▌    | 7522/13630 [03:58<04:36, 22.09it/s] 55%|█████▌    | 7526/13630 [03:58<03:58, 25.58it/s] 55%|█████▌    | 7529/13630 [03:58<04:00, 25.39it/s] 55%|█████▌    | 7533/13630 [03:58<03:39, 27.84it/s] 55%|█████▌    | 7536/13630 [03:58<04:04, 24.95it/s] 55%|█████▌    | 7540/13630 [03:58<03:42, 27.35it/s] 55%|█████▌    | 7543/13630 [03:58<04:00, 25.31it/s] 55%|█████▌    | 7548/13630 [03:59<03:42, 27.29it/s] 55%|█████▌    | 7551/13630 [03:59<03:40, 27.52it/s] 55%|█████▌    | 7554/13630 [03:59<03:40, 27.51it/s] 55%|█████▌    | 7559/13630 [03:59<03:11, 31.64it/s] 55%|█████▌    | 7563/13630 [03:59<03:22, 29.89it/s] 56%|█████▌    | 7568/13630 [03:59<03:01, 33.35it/s] 56%|█████▌    | 7573/13630 [03:59<02:47, 36.15it/s] 56%|█████▌    | 7578/13630 [03:59<02:42, 37.34it/s] 56%|█████▌    | 7582/13630 [04:00<02:52, 35.11it/s] 56%|█████▌    | 7586/13630 [04:00<02:46, 36.26it/s] 56%|█████▌    | 7590/13630 [04:00<03:00, 33.38it/s] 56%|█████▌    | 7594/13630 [04:00<03:22, 29.74it/s] 56%|█████▌    | 7598/13630 [04:00<03:48, 26.44it/s] 56%|█████▌    | 7602/13630 [04:00<03:28, 28.93it/s] 56%|█████▌    | 7607/13630 [04:00<03:04, 32.61it/s] 56%|█████▌    | 7611/13630 [04:01<03:15, 30.85it/s] 56%|█████▌    | 7615/13630 [04:01<03:33, 28.13it/s] 56%|█████▌    | 7619/13630 [04:01<03:25, 29.27it/s] 56%|█████▌    | 7623/13630 [04:01<03:30, 28.48it/s] 56%|█████▌    | 7627/13630 [04:01<03:16, 30.51it/s] 56%|█████▌    | 7632/13630 [04:01<02:56, 33.89it/s] 56%|█████▌    | 7636/13630 [04:02<04:31, 22.09it/s] 56%|█████▌    | 7641/13630 [04:02<04:08, 24.08it/s] 56%|█████▌    | 7644/13630 [04:02<04:01, 24.78it/s] 56%|█████▌    | 7647/13630 [04:02<03:52, 25.78it/s] 56%|█████▌    | 7650/13630 [04:02<04:15, 23.36it/s] 56%|█████▌    | 7655/13630 [04:02<03:34, 27.92it/s] 56%|█████▌    | 7660/13630 [04:02<03:11, 31.22it/s] 56%|█████▌    | 7665/13630 [04:02<02:54, 34.22it/s] 56%|█████▋    | 7669/13630 [04:03<02:55, 34.02it/s] 56%|█████▋    | 7673/13630 [04:03<03:08, 31.62it/s] 56%|█████▋    | 7677/13630 [04:03<03:03, 32.47it/s] 56%|█████▋    | 7681/13630 [04:03<02:54, 34.17it/s] 56%|█████▋    | 7685/13630 [04:03<03:20, 29.64it/s] 56%|█████▋    | 7690/13630 [04:03<03:01, 32.72it/s] 56%|█████▋    | 7694/13630 [04:03<03:24, 29.01it/s] 56%|█████▋    | 7698/13630 [04:04<03:13, 30.67it/s] 57%|█████▋    | 7702/13630 [04:04<03:24, 29.06it/s] 57%|█████▋    | 7706/13630 [04:04<03:23, 29.08it/s] 57%|█████▋    | 7711/13630 [04:04<03:29, 28.31it/s] 57%|█████▋    | 7714/13630 [04:04<03:46, 26.15it/s] 57%|█████▋    | 7719/13630 [04:04<03:16, 30.02it/s] 57%|█████▋    | 7723/13630 [04:04<03:50, 25.67it/s] 57%|█████▋    | 7726/13630 [04:05<04:02, 24.34it/s] 57%|█████▋    | 7729/13630 [04:05<03:55, 25.01it/s] 57%|█████▋    | 7734/13630 [04:05<03:19, 29.62it/s] 57%|█████▋    | 7738/13630 [04:05<03:15, 30.16it/s] 57%|█████▋    | 7742/13630 [04:05<03:12, 30.58it/s] 57%|█████▋    | 7747/13630 [04:05<03:04, 31.85it/s] 57%|█████▋    | 7751/13630 [04:05<03:15, 30.09it/s] 57%|█████▋    | 7756/13630 [04:06<02:52, 34.03it/s] 57%|█████▋    | 7760/13630 [04:06<03:19, 29.47it/s] 57%|█████▋    | 7764/13630 [04:06<03:38, 26.90it/s] 57%|█████▋    | 7768/13630 [04:06<03:31, 27.67it/s] 57%|█████▋    | 7772/13630 [04:06<03:14, 30.11it/s] 57%|█████▋    | 7776/13630 [04:06<03:27, 28.21it/s] 57%|█████▋    | 7780/13630 [04:06<03:13, 30.18it/s] 57%|█████▋    | 7784/13630 [04:07<03:10, 30.70it/s] 57%|█████▋    | 7788/13630 [04:07<03:20, 29.19it/s] 57%|█████▋    | 7793/13630 [04:07<02:58, 32.64it/s] 57%|█████▋    | 7798/13630 [04:07<02:46, 35.02it/s] 57%|█████▋    | 7802/13630 [04:07<03:00, 32.22it/s] 57%|█████▋    | 7807/13630 [04:07<03:15, 29.83it/s] 57%|█████▋    | 7811/13630 [04:07<03:20, 28.99it/s] 57%|█████▋    | 7815/13630 [04:08<03:23, 28.54it/s] 57%|█████▋    | 7820/13630 [04:08<03:02, 31.82it/s] 57%|█████▋    | 7824/13630 [04:08<03:09, 30.65it/s] 57%|█████▋    | 7828/13630 [04:08<03:09, 30.63it/s] 57%|█████▋    | 7832/13630 [04:08<03:00, 32.04it/s] 57%|█████▋    | 7836/13630 [04:08<02:56, 32.91it/s] 58%|█████▊    | 7840/13630 [04:08<02:46, 34.69it/s] 58%|█████▊    | 7844/13630 [04:08<02:50, 34.03it/s] 58%|█████▊    | 7848/13630 [04:09<02:50, 33.98it/s] 58%|█████▊    | 7852/13630 [04:09<02:48, 34.24it/s] 58%|█████▊    | 7857/13630 [04:09<02:36, 36.82it/s] 58%|█████▊    | 7862/13630 [04:09<02:29, 38.66it/s] 58%|█████▊    | 7867/13630 [04:09<02:24, 39.89it/s] 58%|█████▊    | 7871/13630 [04:09<02:34, 37.39it/s] 58%|█████▊    | 7875/13630 [04:09<02:32, 37.67it/s] 58%|█████▊    | 7879/13630 [04:09<02:34, 37.26it/s] 58%|█████▊    | 7883/13630 [04:09<02:48, 34.11it/s] 58%|█████▊    | 7887/13630 [04:10<02:54, 32.99it/s] 58%|█████▊    | 7891/13630 [04:10<02:50, 33.64it/s] 58%|█████▊    | 7895/13630 [04:10<03:00, 31.86it/s] 58%|█████▊    | 7899/13630 [04:10<02:52, 33.25it/s] 58%|█████▊    | 7903/13630 [04:10<03:09, 30.27it/s] 58%|█████▊    | 7907/13630 [04:10<02:57, 32.21it/s] 58%|█████▊    | 7911/13630 [04:10<03:21, 28.33it/s] 58%|█████▊    | 7915/13630 [04:11<03:24, 27.94it/s] 58%|█████▊    | 7918/13630 [04:11<03:29, 27.24it/s] 58%|█████▊    | 7922/13630 [04:11<03:13, 29.47it/s] 58%|█████▊    | 7926/13630 [04:11<03:25, 27.74it/s] 58%|█████▊    | 7929/13630 [04:11<03:25, 27.69it/s] 58%|█████▊    | 7932/13630 [04:11<03:28, 27.31it/s] 58%|█████▊    | 7935/13630 [04:11<03:42, 25.55it/s] 58%|█████▊    | 7939/13630 [04:11<03:25, 27.70it/s] 58%|█████▊    | 7944/13630 [04:12<02:59, 31.76it/s] 58%|█████▊    | 7948/13630 [04:12<03:23, 27.97it/s] 58%|█████▊    | 7952/13630 [04:12<03:08, 30.19it/s] 58%|█████▊    | 7956/13630 [04:12<03:25, 27.56it/s] 58%|█████▊    | 7959/13630 [04:12<03:25, 27.63it/s] 58%|█████▊    | 7963/13630 [04:12<03:12, 29.37it/s] 58%|█████▊    | 7967/13630 [04:12<03:08, 30.01it/s] 58%|█████▊    | 7972/13630 [04:12<02:50, 33.11it/s] 59%|█████▊    | 7976/13630 [04:13<03:14, 29.03it/s] 59%|█████▊    | 7980/13630 [04:13<03:06, 30.28it/s] 59%|█████▊    | 7985/13630 [04:13<02:56, 31.98it/s] 59%|█████▊    | 7989/13630 [04:13<03:08, 29.90it/s] 59%|█████▊    | 7993/13630 [04:13<03:10, 29.65it/s] 59%|█████▊    | 7997/13630 [04:13<03:07, 29.99it/s] 59%|█████▊    | 8001/13630 [04:13<03:02, 30.85it/s] 59%|█████▊    | 8005/13630 [04:14<02:57, 31.77it/s] 59%|█████▉    | 8009/13630 [04:14<03:22, 27.70it/s] 59%|█████▉    | 8012/13630 [04:14<03:22, 27.77it/s] 59%|█████▉    | 8017/13630 [04:14<02:56, 31.89it/s] 59%|█████▉    | 8022/13630 [04:14<02:42, 34.47it/s] 59%|█████▉    | 8027/13630 [04:14<02:34, 36.19it/s] 59%|█████▉    | 8032/13630 [04:14<02:39, 35.03it/s] 59%|█████▉    | 8036/13630 [04:15<02:46, 33.66it/s] 59%|█████▉    | 8040/13630 [04:15<03:14, 28.74it/s] 59%|█████▉    | 8045/13630 [04:15<02:52, 32.34it/s] 59%|█████▉    | 8049/13630 [04:15<03:10, 29.34it/s] 59%|█████▉    | 8054/13630 [04:15<02:49, 32.89it/s] 59%|█████▉    | 8058/13630 [04:15<02:52, 32.30it/s] 59%|█████▉    | 8063/13630 [04:15<02:37, 35.42it/s] 59%|█████▉    | 8067/13630 [04:15<02:38, 35.18it/s] 59%|█████▉    | 8071/13630 [04:16<02:38, 34.97it/s] 59%|█████▉    | 8076/13630 [04:16<02:27, 37.66it/s] 59%|█████▉    | 8080/13630 [04:16<02:38, 35.11it/s] 59%|█████▉    | 8084/13630 [04:16<02:43, 33.88it/s] 59%|█████▉    | 8089/13630 [04:16<02:29, 37.01it/s] 59%|█████▉    | 8094/13630 [04:16<02:23, 38.55it/s] 59%|█████▉    | 8098/13630 [04:16<02:24, 38.36it/s] 59%|█████▉    | 8102/13630 [04:16<02:41, 34.31it/s] 59%|█████▉    | 8107/13630 [04:17<02:32, 36.27it/s] 60%|█████▉    | 8111/13630 [04:17<02:53, 31.81it/s] 60%|█████▉    | 8115/13630 [04:17<02:48, 32.72it/s] 60%|█████▉    | 8120/13630 [04:17<02:35, 35.42it/s] 60%|█████▉    | 8124/13630 [04:17<02:39, 34.57it/s] 60%|█████▉    | 8129/13630 [04:17<02:29, 36.69it/s] 60%|█████▉    | 8133/13630 [04:17<02:27, 37.28it/s] 60%|█████▉    | 8137/13630 [04:17<02:43, 33.63it/s] 60%|█████▉    | 8141/13630 [04:18<02:47, 32.82it/s] 60%|█████▉    | 8146/13630 [04:18<02:41, 33.90it/s] 60%|█████▉    | 8150/13630 [04:18<02:40, 34.17it/s] 60%|█████▉    | 8154/13630 [04:18<02:46, 32.95it/s] 60%|█████▉    | 8158/13630 [04:18<02:38, 34.46it/s] 60%|█████▉    | 8162/13630 [04:18<02:34, 35.31it/s] 60%|█████▉    | 8166/13630 [04:18<02:46, 32.72it/s] 60%|█████▉    | 8170/13630 [04:18<02:40, 34.04it/s] 60%|█████▉    | 8175/13630 [04:19<02:48, 32.45it/s] 60%|██████    | 8179/13630 [04:19<02:58, 30.47it/s] 60%|██████    | 8183/13630 [04:19<03:02, 29.82it/s] 60%|██████    | 8187/13630 [04:19<03:18, 27.37it/s] 60%|██████    | 8192/13630 [04:19<02:55, 30.98it/s] 60%|██████    | 8196/13630 [04:19<02:59, 30.24it/s] 60%|██████    | 8201/13630 [04:19<02:41, 33.62it/s] 60%|██████    | 8205/13630 [04:20<02:39, 34.09it/s] 60%|██████    | 8209/13630 [04:20<02:39, 34.01it/s] 60%|██████    | 8213/13630 [04:20<02:33, 35.40it/s] 60%|██████    | 8217/13630 [04:20<02:39, 34.03it/s] 60%|██████    | 8221/13630 [04:20<02:36, 34.64it/s] 60%|██████    | 8226/13630 [04:20<02:27, 36.65it/s] 60%|██████    | 8230/13630 [04:20<02:33, 35.18it/s] 60%|██████    | 8234/13630 [04:20<02:38, 34.05it/s] 60%|██████    | 8238/13630 [04:21<02:43, 32.93it/s] 60%|██████    | 8242/13630 [04:21<02:46, 32.44it/s] 61%|██████    | 8247/13630 [04:21<02:31, 35.58it/s] 61%|██████    | 8251/13630 [04:21<02:32, 35.36it/s] 61%|██████    | 8256/13630 [04:21<02:23, 37.50it/s] 61%|██████    | 8260/13630 [04:21<02:21, 37.92it/s] 61%|██████    | 8264/13630 [04:21<02:31, 35.46it/s] 61%|██████    | 8268/13630 [04:21<02:29, 35.84it/s] 61%|██████    | 8273/13630 [04:21<02:23, 37.45it/s] 61%|██████    | 8277/13630 [04:22<02:23, 37.30it/s] 61%|██████    | 8281/13630 [04:22<02:24, 37.04it/s] 61%|██████    | 8285/13630 [04:22<02:48, 31.76it/s] 61%|██████    | 8289/13630 [04:22<02:38, 33.62it/s] 61%|██████    | 8294/13630 [04:22<02:29, 35.68it/s] 61%|██████    | 8299/13630 [04:22<02:23, 37.06it/s] 61%|██████    | 8304/13630 [04:22<02:18, 38.55it/s] 61%|██████    | 8309/13630 [04:22<02:15, 39.29it/s] 61%|██████    | 8314/13630 [04:23<02:13, 39.92it/s] 61%|██████    | 8319/13630 [04:23<02:21, 37.52it/s] 61%|██████    | 8323/13630 [04:23<02:48, 31.54it/s] 61%|██████    | 8328/13630 [04:23<02:36, 33.98it/s] 61%|██████    | 8332/13630 [04:23<02:29, 35.35it/s] 61%|██████    | 8336/13630 [04:23<02:43, 32.47it/s] 61%|██████    | 8340/13630 [04:23<02:45, 31.97it/s] 61%|██████    | 8344/13630 [04:24<02:53, 30.41it/s] 61%|██████    | 8348/13630 [04:24<02:45, 31.82it/s] 61%|██████▏   | 8352/13630 [04:24<02:37, 33.47it/s] 61%|██████▏   | 8357/13630 [04:24<02:28, 35.58it/s] 61%|██████▏   | 8362/13630 [04:24<02:19, 37.72it/s] 61%|██████▏   | 8366/13630 [04:24<02:35, 33.90it/s] 61%|██████▏   | 8370/13630 [04:24<02:29, 35.17it/s] 61%|██████▏   | 8374/13630 [04:24<02:34, 33.92it/s] 61%|██████▏   | 8378/13630 [04:25<02:41, 32.42it/s] 61%|██████▏   | 8382/13630 [04:25<02:33, 34.17it/s] 62%|██████▏   | 8387/13630 [04:25<02:22, 36.73it/s] 62%|██████▏   | 8391/13630 [04:25<02:44, 31.88it/s] 62%|██████▏   | 8396/13630 [04:25<02:50, 30.67it/s] 62%|██████▏   | 8401/13630 [04:25<02:35, 33.58it/s] 62%|██████▏   | 8405/13630 [04:25<02:30, 34.82it/s] 62%|██████▏   | 8409/13630 [04:25<02:29, 34.83it/s] 62%|██████▏   | 8413/13630 [04:26<02:51, 30.39it/s] 62%|██████▏   | 8417/13630 [04:26<02:52, 30.22it/s] 62%|██████▏   | 8421/13630 [04:26<02:51, 30.40it/s] 62%|██████▏   | 8425/13630 [04:26<02:54, 29.88it/s] 62%|██████▏   | 8429/13630 [04:26<02:53, 29.99it/s] 62%|██████▏   | 8433/13630 [04:26<02:54, 29.71it/s] 62%|██████▏   | 8436/13630 [04:26<02:54, 29.74it/s] 62%|██████▏   | 8439/13630 [04:26<03:03, 28.36it/s] 62%|██████▏   | 8442/13630 [04:27<03:16, 26.45it/s] 62%|██████▏   | 8446/13630 [04:27<03:06, 27.77it/s] 62%|██████▏   | 8449/13630 [04:27<03:05, 27.96it/s] 62%|██████▏   | 8452/13630 [04:27<03:09, 27.33it/s] 62%|██████▏   | 8456/13630 [04:27<02:49, 30.45it/s] 62%|██████▏   | 8460/13630 [04:27<03:29, 24.70it/s] 62%|██████▏   | 8465/13630 [04:27<02:56, 29.26it/s] 62%|██████▏   | 8469/13630 [04:28<02:44, 31.30it/s] 62%|██████▏   | 8473/13630 [04:28<02:58, 28.83it/s] 62%|██████▏   | 8478/13630 [04:28<02:41, 31.99it/s] 62%|██████▏   | 8482/13630 [04:28<02:44, 31.25it/s] 62%|██████▏   | 8486/13630 [04:28<02:36, 32.97it/s] 62%|██████▏   | 8491/13630 [04:28<02:24, 35.68it/s] 62%|██████▏   | 8495/13630 [04:28<02:23, 35.71it/s] 62%|██████▏   | 8499/13630 [04:28<02:35, 32.89it/s] 62%|██████▏   | 8504/13630 [04:29<02:26, 35.00it/s] 62%|██████▏   | 8508/13630 [04:29<02:27, 34.73it/s] 62%|██████▏   | 8513/13630 [04:29<02:20, 36.53it/s] 62%|██████▏   | 8517/13630 [04:29<02:24, 35.32it/s] 63%|██████▎   | 8521/13630 [04:29<02:26, 34.97it/s] 63%|██████▎   | 8525/13630 [04:29<02:32, 33.52it/s] 63%|██████▎   | 8529/13630 [04:29<02:45, 30.79it/s] 63%|██████▎   | 8533/13630 [04:30<03:31, 24.14it/s] 63%|██████▎   | 8536/13630 [04:30<03:37, 23.39it/s] 63%|██████▎   | 8539/13630 [04:30<03:30, 24.20it/s] 63%|██████▎   | 8543/13630 [04:30<03:09, 26.87it/s] 63%|██████▎   | 8547/13630 [04:30<02:52, 29.49it/s] 63%|██████▎   | 8551/13630 [04:30<03:48, 22.18it/s] 63%|██████▎   | 8554/13630 [04:31<04:29, 18.85it/s] 63%|██████▎   | 8558/13630 [04:31<03:47, 22.30it/s] 63%|██████▎   | 8561/13630 [04:31<03:37, 23.30it/s] 63%|██████▎   | 8564/13630 [04:31<03:41, 22.88it/s] 63%|██████▎   | 8568/13630 [04:31<03:15, 25.84it/s] 63%|██████▎   | 8572/13630 [04:31<03:02, 27.76it/s] 63%|██████▎   | 8576/13630 [04:31<03:09, 26.74it/s] 63%|██████▎   | 8580/13630 [04:31<02:51, 29.52it/s] 63%|██████▎   | 8584/13630 [04:32<02:51, 29.34it/s] 63%|██████▎   | 8588/13630 [04:32<02:38, 31.89it/s] 63%|██████▎   | 8592/13630 [04:32<02:42, 31.08it/s] 63%|██████▎   | 8596/13630 [04:32<03:08, 26.75it/s] 63%|██████▎   | 8599/13630 [04:32<03:04, 27.31it/s] 63%|██████▎   | 8603/13630 [04:32<02:45, 30.36it/s] 63%|██████▎   | 8607/13630 [04:32<02:36, 32.14it/s] 63%|██████▎   | 8612/13630 [04:32<02:24, 34.78it/s] 63%|██████▎   | 8617/13630 [04:33<02:17, 36.44it/s] 63%|██████▎   | 8622/13630 [04:33<02:11, 38.17it/s] 63%|██████▎   | 8627/13630 [04:33<02:08, 38.83it/s] 63%|██████▎   | 8631/13630 [04:33<02:18, 36.22it/s] 63%|██████▎   | 8636/13630 [04:33<02:19, 35.67it/s] 63%|██████▎   | 8641/13630 [04:33<02:10, 38.21it/s] 63%|██████▎   | 8646/13630 [04:33<02:06, 39.50it/s] 63%|██████▎   | 8650/13630 [04:33<02:15, 36.64it/s] 63%|██████▎   | 8654/13630 [04:34<02:34, 32.20it/s] 64%|██████▎   | 8658/13630 [04:34<02:35, 32.06it/s] 64%|██████▎   | 8663/13630 [04:34<02:24, 34.29it/s] 64%|██████▎   | 8667/13630 [04:34<02:23, 34.61it/s] 64%|██████▎   | 8671/13630 [04:34<02:21, 35.15it/s] 64%|██████▎   | 8675/13630 [04:34<02:30, 32.92it/s] 64%|██████▎   | 8679/13630 [04:34<02:36, 31.55it/s] 64%|██████▎   | 8683/13630 [04:34<02:29, 33.05it/s] 64%|██████▎   | 8688/13630 [04:35<02:19, 35.54it/s] 64%|██████▍   | 8692/13630 [04:35<02:33, 32.19it/s] 64%|██████▍   | 8696/13630 [04:35<02:34, 31.89it/s] 64%|██████▍   | 8700/13630 [04:35<02:44, 30.02it/s] 64%|██████▍   | 8704/13630 [04:35<02:58, 27.60it/s] 64%|██████▍   | 8707/13630 [04:35<03:04, 26.65it/s] 64%|██████▍   | 8711/13630 [04:35<02:45, 29.65it/s] 64%|██████▍   | 8715/13630 [04:36<02:44, 29.95it/s] 64%|██████▍   | 8719/13630 [04:36<02:37, 31.09it/s] 64%|██████▍   | 8723/13630 [04:36<02:36, 31.29it/s] 64%|██████▍   | 8727/13630 [04:36<02:27, 33.16it/s] 64%|██████▍   | 8731/13630 [04:36<02:33, 31.87it/s] 64%|██████▍   | 8735/13630 [04:36<02:26, 33.43it/s] 64%|██████▍   | 8739/13630 [04:36<02:19, 35.10it/s] 64%|██████▍   | 8743/13630 [04:36<02:24, 33.79it/s] 64%|██████▍   | 8747/13630 [04:36<02:28, 32.90it/s] 64%|██████▍   | 8751/13630 [04:37<02:21, 34.37it/s] 64%|██████▍   | 8755/13630 [04:37<02:16, 35.82it/s] 64%|██████▍   | 8760/13630 [04:37<02:26, 33.31it/s] 64%|██████▍   | 8764/13630 [04:37<02:50, 28.58it/s] 64%|██████▍   | 8768/13630 [04:37<02:58, 27.16it/s] 64%|██████▍   | 8772/13630 [04:37<02:52, 28.10it/s] 64%|██████▍   | 8775/13630 [04:38<03:10, 25.52it/s] 64%|██████▍   | 8778/13630 [04:38<03:56, 20.49it/s] 64%|██████▍   | 8781/13630 [04:38<03:49, 21.15it/s] 64%|██████▍   | 8785/13630 [04:38<03:24, 23.73it/s] 64%|██████▍   | 8788/13630 [04:38<03:24, 23.64it/s] 64%|██████▍   | 8791/13630 [04:38<03:19, 24.27it/s] 65%|██████▍   | 8795/13630 [04:38<03:04, 26.19it/s] 65%|██████▍   | 8798/13630 [04:38<03:04, 26.17it/s] 65%|██████▍   | 8801/13630 [04:39<03:17, 24.48it/s] 65%|██████▍   | 8804/13630 [04:39<03:14, 24.78it/s] 65%|██████▍   | 8809/13630 [04:39<02:41, 29.89it/s] 65%|██████▍   | 8813/13630 [04:39<02:56, 27.26it/s] 65%|██████▍   | 8816/13630 [04:39<03:10, 25.31it/s] 65%|██████▍   | 8820/13630 [04:39<02:53, 27.68it/s] 65%|██████▍   | 8823/13630 [04:39<02:52, 27.93it/s] 65%|██████▍   | 8828/13630 [04:40<02:30, 31.95it/s] 65%|██████▍   | 8832/13630 [04:40<02:21, 34.01it/s] 65%|██████▍   | 8837/13630 [04:40<02:12, 36.28it/s] 65%|██████▍   | 8841/13630 [04:40<02:23, 33.33it/s] 65%|██████▍   | 8846/13630 [04:40<02:13, 35.95it/s] 65%|██████▍   | 8850/13630 [04:40<02:10, 36.73it/s] 65%|██████▍   | 8854/13630 [04:40<02:23, 33.32it/s] 65%|██████▍   | 8858/13630 [04:40<02:29, 32.00it/s] 65%|██████▌   | 8862/13630 [04:41<02:28, 32.13it/s] 65%|██████▌   | 8866/13630 [04:41<02:22, 33.54it/s] 65%|██████▌   | 8870/13630 [04:41<02:18, 34.29it/s] 65%|██████▌   | 8874/13630 [04:41<03:07, 25.34it/s] 65%|██████▌   | 8878/13630 [04:41<02:49, 28.05it/s] 65%|██████▌   | 8882/13630 [04:41<02:40, 29.58it/s] 65%|██████▌   | 8886/13630 [04:41<02:37, 30.09it/s] 65%|██████▌   | 8891/13630 [04:41<02:21, 33.51it/s] 65%|██████▌   | 8895/13630 [04:42<02:52, 27.43it/s] 65%|██████▌   | 8899/13630 [04:42<02:49, 27.90it/s] 65%|██████▌   | 8903/13630 [04:42<02:45, 28.59it/s] 65%|██████▌   | 8907/13630 [04:42<02:32, 30.94it/s] 65%|██████▌   | 8911/13630 [04:42<02:46, 28.38it/s] 65%|██████▌   | 8915/13630 [04:42<02:33, 30.64it/s] 65%|██████▌   | 8919/13630 [04:42<02:36, 30.18it/s] 65%|██████▌   | 8923/13630 [04:43<02:27, 31.82it/s] 65%|██████▌   | 8927/13630 [04:43<02:37, 29.92it/s] 66%|██████▌   | 8931/13630 [04:43<02:36, 30.07it/s] 66%|██████▌   | 8935/13630 [04:43<02:35, 30.19it/s] 66%|██████▌   | 8939/13630 [04:43<02:45, 28.42it/s] 66%|██████▌   | 8942/13630 [04:43<02:56, 26.58it/s] 66%|██████▌   | 8946/13630 [04:43<02:39, 29.32it/s] 66%|██████▌   | 8951/13630 [04:44<02:23, 32.54it/s] 66%|██████▌   | 8955/13630 [04:44<03:15, 23.86it/s] 66%|██████▌   | 8959/13630 [04:44<03:03, 25.47it/s] 66%|██████▌   | 8962/13630 [04:44<03:02, 25.63it/s] 66%|██████▌   | 8965/13630 [04:44<03:09, 24.62it/s] 66%|██████▌   | 8969/13630 [04:44<02:49, 27.46it/s] 66%|██████▌   | 8973/13630 [04:44<02:36, 29.78it/s] 66%|██████▌   | 8977/13630 [04:45<02:58, 26.14it/s] 66%|██████▌   | 8981/13630 [04:45<02:52, 26.97it/s] 66%|██████▌   | 8984/13630 [04:45<02:53, 26.76it/s] 66%|██████▌   | 8988/13630 [04:45<02:38, 29.27it/s] 66%|██████▌   | 8992/13630 [04:45<02:31, 30.54it/s] 66%|██████▌   | 8996/13630 [04:45<02:30, 30.73it/s] 66%|██████▌   | 9000/13630 [04:45<03:01, 25.44it/s] 66%|██████▌   | 9004/13630 [04:46<02:47, 27.54it/s] 66%|██████▌   | 9007/13630 [04:46<02:56, 26.25it/s] 66%|██████▌   | 9012/13630 [04:46<02:53, 26.61it/s] 66%|██████▌   | 9015/13630 [04:46<02:53, 26.54it/s] 66%|██████▌   | 9019/13630 [04:46<02:41, 28.58it/s] 66%|██████▌   | 9022/13630 [04:46<02:40, 28.70it/s] 66%|██████▌   | 9026/13630 [04:46<02:27, 31.17it/s] 66%|██████▋   | 9030/13630 [04:46<02:25, 31.57it/s] 66%|██████▋   | 9034/13630 [04:47<02:31, 30.31it/s] 66%|██████▋   | 9039/13630 [04:47<02:28, 31.00it/s] 66%|██████▋   | 9043/13630 [04:47<02:19, 32.78it/s] 66%|██████▋   | 9047/13630 [04:47<02:23, 32.04it/s] 66%|██████▋   | 9051/13630 [04:47<02:24, 31.69it/s] 66%|██████▋   | 9055/13630 [04:47<02:26, 31.16it/s] 66%|██████▋   | 9059/13630 [04:47<02:52, 26.52it/s] 67%|██████▋   | 9064/13630 [04:48<02:27, 30.91it/s] 67%|██████▋   | 9069/13630 [04:48<02:15, 33.56it/s] 67%|██████▋   | 9073/13630 [04:48<02:26, 31.18it/s] 67%|██████▋   | 9078/13630 [04:48<02:12, 34.35it/s] 67%|██████▋   | 9082/13630 [04:48<02:33, 29.61it/s] 67%|██████▋   | 9086/13630 [04:48<02:26, 30.92it/s] 67%|██████▋   | 9091/13630 [04:48<02:14, 33.86it/s] 67%|██████▋   | 9095/13630 [04:48<02:11, 34.61it/s] 67%|██████▋   | 9099/13630 [04:49<02:24, 31.44it/s] 67%|██████▋   | 9103/13630 [04:49<02:25, 31.20it/s] 67%|██████▋   | 9107/13630 [04:49<02:35, 29.12it/s] 67%|██████▋   | 9111/13630 [04:49<02:26, 30.85it/s] 67%|██████▋   | 9115/13630 [04:49<02:18, 32.58it/s] 67%|██████▋   | 9119/13630 [04:49<02:12, 34.16it/s] 67%|██████▋   | 9123/13630 [04:49<02:21, 31.76it/s] 67%|██████▋   | 9127/13630 [04:50<02:37, 28.60it/s] 67%|██████▋   | 9131/13630 [04:50<02:30, 29.82it/s] 67%|██████▋   | 9135/13630 [04:50<02:24, 31.15it/s] 67%|██████▋   | 9139/13630 [04:50<02:38, 28.35it/s] 67%|██████▋   | 9142/13630 [04:50<02:39, 28.07it/s] 67%|██████▋   | 9147/13630 [04:50<02:20, 31.88it/s] 67%|██████▋   | 9151/13630 [04:50<02:24, 31.08it/s] 67%|██████▋   | 9156/13630 [04:50<02:12, 33.82it/s] 67%|██████▋   | 9160/13630 [04:51<02:25, 30.67it/s] 67%|██████▋   | 9164/13630 [04:51<02:34, 28.84it/s] 67%|██████▋   | 9168/13630 [04:51<02:23, 31.13it/s] 67%|██████▋   | 9172/13630 [04:51<02:58, 25.02it/s] 67%|██████▋   | 9176/13630 [04:51<02:41, 27.63it/s] 67%|██████▋   | 9180/13630 [04:51<02:26, 30.28it/s] 67%|██████▋   | 9184/13630 [04:51<02:44, 26.99it/s] 67%|██████▋   | 9188/13630 [04:52<02:35, 28.61it/s] 67%|██████▋   | 9192/13630 [04:52<02:32, 29.06it/s] 67%|██████▋   | 9196/13630 [04:52<02:27, 30.13it/s] 67%|██████▋   | 9200/13630 [04:52<02:22, 31.20it/s] 68%|██████▊   | 9204/13630 [04:52<02:30, 29.45it/s] 68%|██████▊   | 9208/13630 [04:52<02:25, 30.42it/s] 68%|██████▊   | 9212/13630 [04:52<02:14, 32.75it/s] 68%|██████▊   | 9216/13630 [04:52<02:13, 32.98it/s] 68%|██████▊   | 9220/13630 [04:53<02:06, 34.77it/s] 68%|██████▊   | 9224/13630 [04:53<02:06, 34.95it/s] 68%|██████▊   | 9228/13630 [04:53<02:18, 31.69it/s] 68%|██████▊   | 9232/13630 [04:53<02:17, 32.04it/s] 68%|██████▊   | 9236/13630 [04:53<02:27, 29.80it/s] 68%|██████▊   | 9240/13630 [04:53<02:17, 31.82it/s] 68%|██████▊   | 9244/13630 [04:53<02:29, 29.32it/s] 68%|██████▊   | 9249/13630 [04:53<02:13, 32.92it/s] 68%|██████▊   | 9253/13630 [04:54<02:28, 29.53it/s] 68%|██████▊   | 9258/13630 [04:54<02:13, 32.67it/s] 68%|██████▊   | 9263/13630 [04:54<02:05, 34.79it/s] 68%|██████▊   | 9267/13630 [04:54<02:05, 34.68it/s] 68%|██████▊   | 9272/13630 [04:54<01:58, 36.65it/s] 68%|██████▊   | 9276/13630 [04:54<01:58, 36.67it/s] 68%|██████▊   | 9280/13630 [04:54<02:00, 36.25it/s] 68%|██████▊   | 9284/13630 [04:54<02:02, 35.55it/s] 68%|██████▊   | 9288/13630 [04:55<01:59, 36.35it/s] 68%|██████▊   | 9292/13630 [04:55<02:03, 35.14it/s] 68%|██████▊   | 9297/13630 [04:55<01:54, 37.74it/s] 68%|██████▊   | 9301/13630 [04:55<01:54, 37.81it/s] 68%|██████▊   | 9305/13630 [04:55<01:53, 38.21it/s] 68%|██████▊   | 9309/13630 [04:55<01:58, 36.58it/s] 68%|██████▊   | 9313/13630 [04:55<01:58, 36.50it/s] 68%|██████▊   | 9318/13630 [04:55<02:02, 35.17it/s] 68%|██████▊   | 9322/13630 [04:56<02:02, 35.11it/s] 68%|██████▊   | 9326/13630 [04:56<02:22, 30.28it/s] 68%|██████▊   | 9330/13630 [04:56<02:45, 26.03it/s] 68%|██████▊   | 9333/13630 [04:56<02:43, 26.25it/s] 68%|██████▊   | 9336/13630 [04:56<02:44, 26.08it/s] 69%|██████▊   | 9339/13630 [04:56<03:04, 23.29it/s] 69%|██████▊   | 9344/13630 [04:56<02:31, 28.21it/s] 69%|██████▊   | 9347/13630 [04:57<02:35, 27.56it/s] 69%|██████▊   | 9351/13630 [04:57<02:25, 29.49it/s] 69%|██████▊   | 9355/13630 [04:57<02:14, 31.90it/s] 69%|██████▊   | 9359/13630 [04:57<02:19, 30.60it/s] 69%|██████▊   | 9363/13630 [04:57<02:28, 28.81it/s] 69%|██████▊   | 9368/13630 [04:57<02:25, 29.25it/s] 69%|██████▉   | 9373/13630 [04:57<02:20, 30.20it/s] 69%|██████▉   | 9377/13630 [04:57<02:11, 32.24it/s] 69%|██████▉   | 9381/13630 [04:58<02:05, 33.97it/s] 69%|██████▉   | 9385/13630 [04:58<02:08, 33.02it/s] 69%|██████▉   | 9390/13630 [04:58<01:59, 35.42it/s] 69%|██████▉   | 9394/13630 [04:58<02:04, 34.15it/s] 69%|██████▉   | 9398/13630 [04:58<02:00, 35.03it/s] 69%|██████▉   | 9403/13630 [04:58<01:53, 37.17it/s] 69%|██████▉   | 9407/13630 [04:58<02:12, 31.92it/s] 69%|██████▉   | 9411/13630 [04:58<02:05, 33.62it/s] 69%|██████▉   | 9416/13630 [04:59<01:58, 35.54it/s] 69%|██████▉   | 9420/13630 [04:59<02:04, 33.88it/s] 69%|██████▉   | 9424/13630 [04:59<02:06, 33.13it/s] 69%|██████▉   | 9428/13630 [04:59<02:22, 29.46it/s] 69%|██████▉   | 9432/13630 [04:59<02:23, 29.34it/s] 69%|██████▉   | 9437/13630 [04:59<02:08, 32.75it/s] 69%|██████▉   | 9441/13630 [04:59<02:05, 33.46it/s] 69%|██████▉   | 9445/13630 [05:00<02:03, 34.00it/s] 69%|██████▉   | 9449/13630 [05:00<02:16, 30.57it/s] 69%|██████▉   | 9454/13630 [05:00<02:14, 30.96it/s] 69%|██████▉   | 9458/13630 [05:00<02:06, 32.99it/s] 69%|██████▉   | 9462/13630 [05:00<02:06, 32.98it/s] 69%|██████▉   | 9467/13630 [05:00<01:58, 35.18it/s] 69%|██████▉   | 9471/13630 [05:00<02:00, 34.57it/s] 70%|██████▉   | 9475/13630 [05:00<02:12, 31.25it/s] 70%|██████▉   | 9479/13630 [05:01<02:07, 32.47it/s] 70%|██████▉   | 9483/13630 [05:01<02:16, 30.29it/s] 70%|██████▉   | 9487/13630 [05:01<02:08, 32.14it/s] 70%|██████▉   | 9491/13630 [05:01<02:32, 27.09it/s] 70%|██████▉   | 9494/13630 [05:01<02:53, 23.81it/s] 70%|██████▉   | 9498/13630 [05:01<02:52, 23.90it/s] 70%|██████▉   | 9501/13630 [05:01<02:52, 23.94it/s] 70%|██████▉   | 9506/13630 [05:02<02:23, 28.84it/s] 70%|██████▉   | 9510/13630 [05:02<02:12, 31.03it/s] 70%|██████▉   | 9514/13630 [05:02<02:24, 28.50it/s] 70%|██████▉   | 9518/13630 [05:02<02:14, 30.61it/s] 70%|██████▉   | 9522/13630 [05:02<02:08, 31.98it/s] 70%|██████▉   | 9527/13630 [05:02<02:06, 32.55it/s] 70%|██████▉   | 9531/13630 [05:02<02:10, 31.48it/s] 70%|██████▉   | 9535/13630 [05:03<02:19, 29.28it/s] 70%|██████▉   | 9539/13630 [05:03<02:13, 30.57it/s] 70%|███████   | 9544/13630 [05:03<02:00, 33.89it/s] 70%|███████   | 9549/13630 [05:03<01:52, 36.30it/s] 70%|███████   | 9553/13630 [05:03<01:57, 34.66it/s] 70%|███████   | 9558/13630 [05:03<01:58, 34.30it/s] 70%|███████   | 9562/13630 [05:03<01:54, 35.53it/s] 70%|███████   | 9566/13630 [05:03<02:07, 31.77it/s] 70%|███████   | 9570/13630 [05:04<02:12, 30.69it/s] 70%|███████   | 9575/13630 [05:04<02:00, 33.68it/s] 70%|███████   | 9579/13630 [05:04<02:09, 31.21it/s] 70%|███████   | 9583/13630 [05:04<02:01, 33.25it/s] 70%|███████   | 9588/13630 [05:04<01:52, 35.81it/s] 70%|███████   | 9592/13630 [05:04<01:49, 36.76it/s] 70%|███████   | 9597/13630 [05:04<01:46, 37.79it/s] 70%|███████   | 9602/13630 [05:04<01:43, 38.91it/s] 70%|███████   | 9606/13630 [05:05<01:50, 36.34it/s] 71%|███████   | 9611/13630 [05:05<01:56, 34.38it/s] 71%|███████   | 9615/13630 [05:05<01:54, 35.08it/s] 71%|███████   | 9619/13630 [05:05<02:08, 31.18it/s] 71%|███████   | 9623/13630 [05:05<02:22, 28.06it/s] 71%|███████   | 9627/13630 [05:05<02:18, 28.98it/s] 71%|███████   | 9631/13630 [05:05<02:19, 28.65it/s] 71%|███████   | 9634/13630 [05:06<02:26, 27.24it/s] 71%|███████   | 9637/13630 [05:06<02:29, 26.72it/s] 71%|███████   | 9640/13630 [05:06<03:01, 21.95it/s] 71%|███████   | 9644/13630 [05:06<02:35, 25.60it/s] 71%|███████   | 9648/13630 [05:06<02:18, 28.71it/s] 71%|███████   | 9652/13630 [05:06<02:17, 28.95it/s] 71%|███████   | 9656/13630 [05:06<02:07, 31.28it/s] 71%|███████   | 9661/13630 [05:06<01:56, 34.01it/s] 71%|███████   | 9665/13630 [05:07<02:01, 32.64it/s] 71%|███████   | 9670/13630 [05:07<02:02, 32.36it/s] 71%|███████   | 9674/13630 [05:07<02:05, 31.61it/s] 71%|███████   | 9679/13630 [05:07<01:55, 34.26it/s] 71%|███████   | 9683/13630 [05:07<02:03, 32.00it/s] 71%|███████   | 9687/13630 [05:07<02:02, 32.22it/s] 71%|███████   | 9691/13630 [05:07<02:10, 30.22it/s] 71%|███████   | 9696/13630 [05:08<01:55, 34.09it/s] 71%|███████   | 9700/13630 [05:08<01:54, 34.24it/s] 71%|███████   | 9705/13630 [05:08<01:47, 36.54it/s] 71%|███████   | 9709/13630 [05:08<02:06, 30.95it/s] 71%|███████▏  | 9714/13630 [05:08<01:55, 33.80it/s] 71%|███████▏  | 9718/13630 [05:08<01:53, 34.47it/s] 71%|███████▏  | 9722/13630 [05:08<01:59, 32.65it/s] 71%|███████▏  | 9726/13630 [05:08<02:05, 31.06it/s] 71%|███████▏  | 9730/13630 [05:09<02:33, 25.45it/s] 71%|███████▏  | 9735/13630 [05:09<02:34, 25.22it/s] 71%|███████▏  | 9738/13630 [05:09<02:36, 24.82it/s] 71%|███████▏  | 9742/13630 [05:09<02:23, 27.19it/s] 71%|███████▏  | 9745/13630 [05:09<02:34, 25.10it/s] 72%|███████▏  | 9748/13630 [05:09<02:34, 25.17it/s] 72%|███████▏  | 9752/13630 [05:09<02:15, 28.66it/s] 72%|███████▏  | 9756/13630 [05:10<02:03, 31.32it/s] 72%|███████▏  | 9760/13630 [05:10<02:19, 27.70it/s] 72%|███████▏  | 9764/13630 [05:10<02:08, 30.04it/s] 72%|███████▏  | 9768/13630 [05:10<02:10, 29.52it/s] 72%|███████▏  | 9772/13630 [05:10<02:12, 29.22it/s] 72%|███████▏  | 9776/13630 [05:10<02:15, 28.41it/s] 72%|███████▏  | 9779/13630 [05:10<02:20, 27.39it/s] 72%|███████▏  | 9784/13630 [05:11<02:03, 31.21it/s] 72%|███████▏  | 9788/13630 [05:11<01:56, 32.98it/s] 72%|███████▏  | 9792/13630 [05:11<02:11, 29.23it/s] 72%|███████▏  | 9796/13630 [05:11<02:11, 29.06it/s] 72%|███████▏  | 9800/13630 [05:11<02:02, 31.29it/s] 72%|███████▏  | 9805/13630 [05:11<01:51, 34.25it/s] 72%|███████▏  | 9809/13630 [05:11<01:58, 32.13it/s] 72%|███████▏  | 9814/13630 [05:11<01:49, 34.89it/s] 72%|███████▏  | 9819/13630 [05:12<01:43, 36.87it/s] 72%|███████▏  | 9823/13630 [05:12<01:50, 34.60it/s] 72%|███████▏  | 9827/13630 [05:12<01:53, 33.39it/s] 72%|███████▏  | 9831/13630 [05:12<02:19, 27.32it/s] 72%|███████▏  | 9835/13630 [05:12<02:14, 28.11it/s] 72%|███████▏  | 9838/13630 [05:12<02:28, 25.55it/s] 72%|███████▏  | 9841/13630 [05:12<02:24, 26.15it/s] 72%|███████▏  | 9844/13630 [05:13<02:24, 26.11it/s] 72%|███████▏  | 9848/13630 [05:13<02:31, 25.03it/s] 72%|███████▏  | 9852/13630 [05:13<02:17, 27.44it/s] 72%|███████▏  | 9856/13630 [05:13<02:06, 29.79it/s] 72%|███████▏  | 9860/13630 [05:13<01:59, 31.54it/s] 72%|███████▏  | 9864/13630 [05:13<01:53, 33.07it/s] 72%|███████▏  | 9868/13630 [05:13<02:01, 31.04it/s] 72%|███████▏  | 9872/13630 [05:14<02:18, 27.14it/s] 72%|███████▏  | 9875/13630 [05:14<02:18, 27.04it/s] 72%|███████▏  | 9879/13630 [05:14<02:08, 29.15it/s] 73%|███████▎  | 9883/13630 [05:14<02:30, 24.84it/s] 73%|███████▎  | 9887/13630 [05:14<02:22, 26.29it/s] 73%|███████▎  | 9891/13630 [05:14<02:13, 28.11it/s] 73%|███████▎  | 9894/13630 [05:14<02:22, 26.28it/s] 73%|███████▎  | 9898/13630 [05:14<02:06, 29.47it/s] 73%|███████▎  | 9902/13630 [05:15<02:07, 29.19it/s] 73%|███████▎  | 9906/13630 [05:15<02:07, 29.18it/s] 73%|███████▎  | 9911/13630 [05:15<01:55, 32.28it/s] 73%|███████▎  | 9915/13630 [05:15<02:02, 30.22it/s] 73%|███████▎  | 9919/13630 [05:15<02:00, 30.82it/s] 73%|███████▎  | 9923/13630 [05:15<02:16, 27.19it/s] 73%|███████▎  | 9927/13630 [05:15<02:06, 29.18it/s] 73%|███████▎  | 9931/13630 [05:16<01:59, 30.93it/s] 73%|███████▎  | 9935/13630 [05:16<02:01, 30.43it/s] 73%|███████▎  | 9939/13630 [05:16<01:53, 32.50it/s] 73%|███████▎  | 9943/13630 [05:16<01:51, 32.95it/s] 73%|███████▎  | 9947/13630 [05:16<02:03, 29.76it/s] 73%|███████▎  | 9951/13630 [05:16<02:08, 28.66it/s] 73%|███████▎  | 9956/13630 [05:16<01:54, 32.08it/s] 73%|███████▎  | 9960/13630 [05:16<01:52, 32.61it/s] 73%|███████▎  | 9964/13630 [05:17<02:00, 30.41it/s] 73%|███████▎  | 9968/13630 [05:17<01:53, 32.39it/s] 73%|███████▎  | 9972/13630 [05:17<01:50, 33.14it/s] 73%|███████▎  | 9976/13630 [05:17<01:53, 32.32it/s] 73%|███████▎  | 9980/13630 [05:17<01:47, 34.04it/s] 73%|███████▎  | 9984/13630 [05:17<01:50, 32.98it/s] 73%|███████▎  | 9988/13630 [05:17<01:45, 34.47it/s] 73%|███████▎  | 9992/13630 [05:17<01:52, 32.27it/s] 73%|███████▎  | 9996/13630 [05:18<01:55, 31.42it/s] 73%|███████▎  | 10000/13630 [05:18<01:49, 33.01it/s] 73%|███████▎  | 10004/13630 [05:18<02:00, 30.18it/s] 73%|███████▎  | 10008/13630 [05:18<01:54, 31.53it/s] 73%|███████▎  | 10013/13630 [05:18<01:44, 34.56it/s] 73%|███████▎  | 10017/13630 [05:18<01:47, 33.52it/s] 74%|███████▎  | 10021/13630 [05:18<02:16, 26.38it/s] 74%|███████▎  | 10024/13630 [05:19<02:14, 26.77it/s] 74%|███████▎  | 10028/13630 [05:19<02:09, 27.92it/s] 74%|███████▎  | 10033/13630 [05:19<01:52, 32.07it/s] 74%|███████▎  | 10038/13630 [05:19<01:42, 35.05it/s] 74%|███████▎  | 10042/13630 [05:19<01:51, 32.14it/s] 74%|███████▎  | 10046/13630 [05:19<01:52, 31.90it/s] 74%|███████▎  | 10050/13630 [05:19<02:02, 29.14it/s] 74%|███████▍  | 10054/13630 [05:19<01:59, 29.95it/s] 74%|███████▍  | 10058/13630 [05:20<02:05, 28.45it/s] 74%|███████▍  | 10062/13630 [05:20<01:58, 30.21it/s] 74%|███████▍  | 10066/13630 [05:20<02:02, 29.08it/s] 74%|███████▍  | 10071/13630 [05:20<01:49, 32.36it/s] 74%|███████▍  | 10075/13630 [05:20<01:44, 33.99it/s] 74%|███████▍  | 10079/13630 [05:20<01:41, 35.13it/s] 74%|███████▍  | 10083/13630 [05:20<01:39, 35.57it/s] 74%|███████▍  | 10087/13630 [05:20<01:36, 36.56it/s] 74%|███████▍  | 10091/13630 [05:21<01:35, 36.94it/s] 74%|███████▍  | 10095/13630 [05:21<01:42, 34.62it/s] 74%|███████▍  | 10099/13630 [05:21<02:02, 28.84it/s] 74%|███████▍  | 10103/13630 [05:21<02:12, 26.54it/s] 74%|███████▍  | 10106/13630 [05:21<02:15, 26.10it/s] 74%|███████▍  | 10110/13630 [05:21<02:05, 28.08it/s] 74%|███████▍  | 10114/13630 [05:21<01:55, 30.57it/s] 74%|███████▍  | 10118/13630 [05:22<01:56, 30.22it/s] 74%|███████▍  | 10122/13630 [05:22<01:48, 32.45it/s] 74%|███████▍  | 10126/13630 [05:22<01:51, 31.54it/s] 74%|███████▍  | 10130/13630 [05:22<01:46, 32.99it/s] 74%|███████▍  | 10134/13630 [05:22<01:41, 34.33it/s] 74%|███████▍  | 10139/13630 [05:22<01:38, 35.28it/s] 74%|███████▍  | 10143/13630 [05:22<01:36, 36.25it/s] 74%|███████▍  | 10147/13630 [05:22<01:33, 37.25it/s] 74%|███████▍  | 10151/13630 [05:22<01:41, 34.16it/s] 75%|███████▍  | 10155/13630 [05:23<01:41, 34.20it/s] 75%|███████▍  | 10159/13630 [05:23<01:49, 31.77it/s] 75%|███████▍  | 10163/13630 [05:23<01:48, 31.95it/s] 75%|███████▍  | 10167/13630 [05:23<01:46, 32.38it/s] 75%|███████▍  | 10172/13630 [05:23<01:37, 35.45it/s] 75%|███████▍  | 10177/13630 [05:23<01:32, 37.20it/s] 75%|███████▍  | 10182/13630 [05:23<01:30, 38.22it/s] 75%|███████▍  | 10187/13630 [05:23<01:26, 39.96it/s] 75%|███████▍  | 10192/13630 [05:24<01:35, 36.17it/s] 75%|███████▍  | 10196/13630 [05:24<01:37, 35.06it/s] 75%|███████▍  | 10200/13630 [05:24<01:36, 35.67it/s] 75%|███████▍  | 10204/13630 [05:24<01:41, 33.81it/s] 75%|███████▍  | 10208/13630 [05:24<01:43, 33.01it/s] 75%|███████▍  | 10213/13630 [05:24<01:37, 35.22it/s] 75%|███████▍  | 10217/13630 [05:24<01:34, 36.03it/s] 75%|███████▍  | 10222/13630 [05:24<01:29, 38.04it/s] 75%|███████▌  | 10226/13630 [05:25<01:34, 35.86it/s] 75%|███████▌  | 10231/13630 [05:25<01:29, 37.87it/s] 75%|███████▌  | 10236/13630 [05:25<01:24, 40.04it/s] 75%|███████▌  | 10241/13630 [05:25<01:29, 37.89it/s] 75%|███████▌  | 10245/13630 [05:25<01:31, 36.98it/s] 75%|███████▌  | 10250/13630 [05:25<01:35, 35.42it/s] 75%|███████▌  | 10255/13630 [05:25<01:37, 34.76it/s] 75%|███████▌  | 10260/13630 [05:25<01:30, 37.43it/s] 75%|███████▌  | 10265/13630 [05:26<01:26, 39.12it/s] 75%|███████▌  | 10269/13630 [05:26<01:32, 36.39it/s] 75%|███████▌  | 10273/13630 [05:26<01:35, 35.33it/s] 75%|███████▌  | 10277/13630 [05:26<01:40, 33.53it/s] 75%|███████▌  | 10282/13630 [05:26<01:31, 36.66it/s] 75%|███████▌  | 10286/13630 [05:26<01:37, 34.26it/s] 75%|███████▌  | 10290/13630 [05:26<01:36, 34.62it/s] 76%|███████▌  | 10294/13630 [05:26<01:39, 33.59it/s] 76%|███████▌  | 10299/13630 [05:27<01:31, 36.39it/s] 76%|███████▌  | 10303/13630 [05:27<01:35, 34.73it/s] 76%|███████▌  | 10308/13630 [05:27<01:28, 37.36it/s] 76%|███████▌  | 10312/13630 [05:27<01:47, 30.76it/s] 76%|███████▌  | 10317/13630 [05:27<01:41, 32.72it/s] 76%|███████▌  | 10322/13630 [05:27<01:39, 33.16it/s] 76%|███████▌  | 10326/13630 [05:27<01:39, 33.20it/s] 76%|███████▌  | 10331/13630 [05:28<01:33, 35.32it/s] 76%|███████▌  | 10335/13630 [05:28<01:32, 35.52it/s] 76%|███████▌  | 10339/13630 [05:28<01:34, 34.87it/s] 76%|███████▌  | 10343/13630 [05:28<01:38, 33.37it/s] 76%|███████▌  | 10347/13630 [05:28<01:36, 34.10it/s] 76%|███████▌  | 10352/13630 [05:28<01:29, 36.74it/s] 76%|███████▌  | 10357/13630 [05:28<01:25, 38.49it/s] 76%|███████▌  | 10362/13630 [05:28<01:21, 40.00it/s] 76%|███████▌  | 10367/13630 [05:28<01:25, 38.27it/s] 76%|███████▌  | 10372/13630 [05:29<01:28, 36.67it/s] 76%|███████▌  | 10376/13630 [05:29<01:29, 36.21it/s] 76%|███████▌  | 10381/13630 [05:29<01:25, 37.81it/s] 76%|███████▌  | 10385/13630 [05:29<01:30, 35.68it/s] 76%|███████▌  | 10389/13630 [05:29<01:28, 36.44it/s] 76%|███████▋  | 10394/13630 [05:29<01:24, 38.15it/s] 76%|███████▋  | 10398/13630 [05:29<01:31, 35.27it/s] 76%|███████▋  | 10402/13630 [05:29<01:32, 34.97it/s] 76%|███████▋  | 10407/13630 [05:30<01:34, 33.95it/s] 76%|███████▋  | 10411/13630 [05:30<01:32, 34.65it/s] 76%|███████▋  | 10415/13630 [05:30<01:35, 33.69it/s] 76%|███████▋  | 10419/13630 [05:30<01:40, 31.84it/s] 76%|███████▋  | 10424/13630 [05:30<01:32, 34.64it/s] 77%|███████▋  | 10429/13630 [05:30<01:26, 37.17it/s] 77%|███████▋  | 10433/13630 [05:30<01:31, 35.12it/s] 77%|███████▋  | 10438/13630 [05:30<01:25, 37.34it/s] 77%|███████▋  | 10442/13630 [05:31<01:26, 37.05it/s] 77%|███████▋  | 10447/13630 [05:31<01:22, 38.61it/s] 77%|███████▋  | 10451/13630 [05:31<01:29, 35.53it/s] 77%|███████▋  | 10455/13630 [05:31<01:30, 35.26it/s] 77%|███████▋  | 10460/13630 [05:31<01:25, 36.95it/s] 77%|███████▋  | 10464/13630 [05:31<01:23, 37.72it/s] 77%|███████▋  | 10468/13630 [05:31<01:29, 35.31it/s] 77%|███████▋  | 10473/13630 [05:31<01:24, 37.51it/s] 77%|███████▋  | 10477/13630 [05:32<01:27, 35.89it/s] 77%|███████▋  | 10481/13630 [05:32<01:32, 34.06it/s] 77%|███████▋  | 10485/13630 [05:32<01:33, 33.49it/s] 77%|███████▋  | 10489/13630 [05:32<01:30, 34.84it/s] 77%|███████▋  | 10493/13630 [05:32<01:33, 33.64it/s] 77%|███████▋  | 10497/13630 [05:32<01:33, 33.43it/s] 77%|███████▋  | 10501/13630 [05:32<01:37, 31.99it/s] 77%|███████▋  | 10506/13630 [05:32<01:29, 34.88it/s] 77%|███████▋  | 10510/13630 [05:33<01:37, 32.02it/s] 77%|███████▋  | 10515/13630 [05:33<01:28, 35.07it/s] 77%|███████▋  | 10519/13630 [05:33<01:33, 33.20it/s] 77%|███████▋  | 10524/13630 [05:33<01:25, 36.14it/s] 77%|███████▋  | 10528/13630 [05:33<01:28, 35.20it/s] 77%|███████▋  | 10533/13630 [05:33<01:22, 37.74it/s] 77%|███████▋  | 10537/13630 [05:33<01:24, 36.60it/s] 77%|███████▋  | 10541/13630 [05:33<01:38, 31.34it/s] 77%|███████▋  | 10546/13630 [05:34<01:29, 34.58it/s] 77%|███████▋  | 10551/13630 [05:34<01:23, 36.74it/s] 77%|███████▋  | 10556/13630 [05:34<01:19, 38.62it/s] 77%|███████▋  | 10561/13630 [05:34<01:17, 39.67it/s] 78%|███████▊  | 10566/13630 [05:34<01:16, 40.28it/s] 78%|███████▊  | 10571/13630 [05:34<01:39, 30.74it/s] 78%|███████▊  | 10575/13630 [05:34<01:34, 32.35it/s] 78%|███████▊  | 10580/13630 [05:35<01:27, 34.95it/s] 78%|███████▊  | 10584/13630 [05:35<01:26, 35.27it/s] 78%|███████▊  | 10588/13630 [05:35<01:33, 32.56it/s] 78%|███████▊  | 10594/13630 [05:35<01:18, 38.64it/s] 78%|███████▊  | 10599/13630 [05:35<01:15, 40.30it/s] 78%|███████▊  | 10605/13630 [05:35<01:07, 44.92it/s] 78%|███████▊  | 10614/13630 [05:35<00:54, 55.68it/s] 78%|███████▊  | 10621/13630 [05:35<00:53, 56.39it/s] 78%|███████▊  | 10629/13630 [05:35<00:48, 61.39it/s] 78%|███████▊  | 10636/13630 [05:36<00:54, 55.38it/s] 78%|███████▊  | 10642/13630 [05:36<00:55, 53.97it/s] 78%|███████▊  | 10649/13630 [05:36<00:52, 56.93it/s] 78%|███████▊  | 10658/13630 [05:36<00:46, 64.17it/s] 78%|███████▊  | 10665/13630 [05:36<00:50, 59.24it/s] 78%|███████▊  | 10672/13630 [05:36<00:51, 57.97it/s] 78%|███████▊  | 10681/13630 [05:36<00:45, 64.41it/s] 78%|███████▊  | 10688/13630 [05:36<00:51, 56.76it/s] 78%|███████▊  | 10694/13630 [05:37<00:55, 53.01it/s] 79%|███████▊  | 10701/13630 [05:37<00:52, 55.95it/s] 79%|███████▊  | 10711/13630 [05:37<00:44, 65.70it/s] 79%|███████▊  | 10718/13630 [05:37<00:45, 63.52it/s] 79%|███████▊  | 10725/13630 [05:37<00:50, 57.92it/s] 79%|███████▊  | 10733/13630 [05:37<00:48, 59.54it/s] 79%|███████▉  | 10741/13630 [05:37<00:45, 63.33it/s] 79%|███████▉  | 10748/13630 [05:38<00:56, 50.83it/s] 79%|███████▉  | 10754/13630 [05:38<00:56, 50.65it/s] 79%|███████▉  | 10760/13630 [05:38<01:05, 43.87it/s] 79%|███████▉  | 10765/13630 [05:38<01:16, 37.60it/s] 79%|███████▉  | 10770/13630 [05:38<01:17, 36.95it/s] 79%|███████▉  | 10774/13630 [05:38<01:17, 36.93it/s] 79%|███████▉  | 10782/13630 [05:38<01:02, 45.80it/s] 79%|███████▉  | 10787/13630 [05:39<01:01, 46.08it/s] 79%|███████▉  | 10794/13630 [05:39<00:55, 51.37it/s] 79%|███████▉  | 10800/13630 [05:39<01:01, 46.16it/s] 79%|███████▉  | 10805/13630 [05:39<01:05, 42.93it/s] 79%|███████▉  | 10811/13630 [05:39<01:01, 45.79it/s] 79%|███████▉  | 10817/13630 [05:39<00:59, 47.13it/s] 79%|███████▉  | 10827/13630 [05:39<00:50, 55.04it/s] 79%|███████▉  | 10833/13630 [05:39<00:51, 54.19it/s] 80%|███████▉  | 10839/13630 [05:40<00:50, 55.00it/s] 80%|███████▉  | 10846/13630 [05:40<00:48, 57.39it/s] 80%|███████▉  | 10854/13630 [05:40<00:44, 62.26it/s] 80%|███████▉  | 10863/13630 [05:40<00:40, 68.16it/s] 80%|███████▉  | 10873/13630 [05:40<00:36, 75.25it/s] 80%|███████▉  | 10881/13630 [05:40<00:49, 55.70it/s] 80%|███████▉  | 10888/13630 [05:40<00:53, 50.89it/s] 80%|███████▉  | 10894/13630 [05:40<00:52, 51.97it/s] 80%|███████▉  | 10900/13630 [05:41<00:52, 51.64it/s] 80%|████████  | 10906/13630 [05:41<00:52, 52.11it/s] 80%|████████  | 10912/13630 [05:41<00:55, 49.36it/s] 80%|████████  | 10920/13630 [05:41<00:52, 52.10it/s] 80%|████████  | 10926/13630 [05:41<00:51, 52.36it/s] 80%|████████  | 10934/13630 [05:41<00:46, 58.13it/s] 80%|████████  | 10940/13630 [05:41<00:48, 55.97it/s] 80%|████████  | 10946/13630 [05:41<00:48, 54.86it/s] 80%|████████  | 10952/13630 [05:42<00:50, 52.53it/s] 80%|████████  | 10961/13630 [05:42<00:43, 61.59it/s] 80%|████████  | 10970/13630 [05:42<00:38, 68.49it/s] 81%|████████  | 10977/13630 [05:42<00:44, 59.77it/s] 81%|████████  | 10984/13630 [05:42<00:53, 49.10it/s] 81%|████████  | 10990/13630 [05:42<00:51, 51.37it/s] 81%|████████  | 10996/13630 [05:42<00:52, 50.52it/s] 81%|████████  | 11003/13630 [05:42<00:48, 53.88it/s] 81%|████████  | 11009/13630 [05:43<00:50, 52.33it/s] 81%|████████  | 11015/13630 [05:43<00:55, 46.95it/s] 81%|████████  | 11020/13630 [05:43<00:54, 47.66it/s] 81%|████████  | 11029/13630 [05:43<00:44, 58.33it/s] 81%|████████  | 11036/13630 [05:43<00:45, 57.44it/s] 81%|████████  | 11042/13630 [05:43<00:45, 57.25it/s] 81%|████████  | 11048/13630 [05:43<00:48, 53.15it/s] 81%|████████  | 11054/13630 [05:43<00:48, 53.54it/s] 81%|████████  | 11060/13630 [05:44<00:54, 47.12it/s] 81%|████████  | 11065/13630 [05:44<00:57, 44.79it/s] 81%|████████  | 11072/13630 [05:44<00:51, 49.86it/s] 81%|████████▏ | 11081/13630 [05:44<00:43, 59.25it/s] 81%|████████▏ | 11088/13630 [05:44<00:46, 54.11it/s] 81%|████████▏ | 11096/13630 [05:44<00:45, 55.74it/s] 81%|████████▏ | 11102/13630 [05:44<00:45, 55.45it/s] 81%|████████▏ | 11108/13630 [05:44<00:48, 51.82it/s] 82%|████████▏ | 11114/13630 [05:45<00:50, 49.59it/s] 82%|████████▏ | 11122/13630 [05:45<00:43, 57.03it/s] 82%|████████▏ | 11128/13630 [05:45<00:44, 56.72it/s] 82%|████████▏ | 11134/13630 [05:45<00:51, 48.78it/s] 82%|████████▏ | 11140/13630 [05:45<00:51, 47.93it/s] 82%|████████▏ | 11145/13630 [05:45<00:55, 44.82it/s] 82%|████████▏ | 11150/13630 [05:45<00:57, 43.31it/s] 82%|████████▏ | 11155/13630 [05:45<00:57, 42.79it/s] 82%|████████▏ | 11160/13630 [05:46<00:57, 43.12it/s] 82%|████████▏ | 11165/13630 [05:46<01:02, 39.56it/s] 82%|████████▏ | 11171/13630 [05:46<00:56, 43.53it/s] 82%|████████▏ | 11178/13630 [05:46<00:50, 48.82it/s] 82%|████████▏ | 11184/13630 [05:46<00:58, 42.06it/s] 82%|████████▏ | 11190/13630 [05:46<00:53, 45.71it/s] 82%|████████▏ | 11195/13630 [05:46<00:52, 46.38it/s] 82%|████████▏ | 11202/13630 [05:46<00:47, 51.20it/s] 82%|████████▏ | 11208/13630 [05:47<00:52, 46.04it/s] 82%|████████▏ | 11213/13630 [05:47<00:53, 45.22it/s] 82%|████████▏ | 11218/13630 [05:47<00:52, 45.56it/s] 82%|████████▏ | 11223/13630 [05:47<00:58, 40.90it/s] 82%|████████▏ | 11228/13630 [05:47<00:55, 42.94it/s] 82%|████████▏ | 11237/13630 [05:47<00:44, 54.20it/s] 82%|████████▏ | 11243/13630 [05:47<00:45, 52.08it/s] 83%|████████▎ | 11249/13630 [05:47<00:46, 51.59it/s] 83%|████████▎ | 11258/13630 [05:48<00:39, 59.58it/s] 83%|████████▎ | 11265/13630 [05:48<00:38, 61.10it/s] 83%|████████▎ | 11272/13630 [05:48<00:43, 54.68it/s] 83%|████████▎ | 11278/13630 [05:48<00:44, 52.80it/s] 83%|████████▎ | 11284/13630 [05:48<00:44, 53.01it/s] 83%|████████▎ | 11290/13630 [05:48<01:00, 38.57it/s] 83%|████████▎ | 11295/13630 [05:48<01:00, 38.71it/s] 83%|████████▎ | 11300/13630 [05:49<00:57, 40.39it/s] 83%|████████▎ | 11306/13630 [05:49<00:52, 44.27it/s] 83%|████████▎ | 11313/13630 [05:49<00:49, 46.76it/s] 83%|████████▎ | 11320/13630 [05:49<00:44, 51.78it/s] 83%|████████▎ | 11326/13630 [05:49<00:46, 49.24it/s] 83%|████████▎ | 11334/13630 [05:49<00:41, 55.83it/s] 83%|████████▎ | 11340/13630 [05:49<00:44, 51.01it/s] 83%|████████▎ | 11346/13630 [05:49<00:50, 45.59it/s] 83%|████████▎ | 11354/13630 [05:50<00:42, 53.30it/s] 83%|████████▎ | 11360/13630 [05:50<00:42, 53.90it/s] 83%|████████▎ | 11367/13630 [05:50<00:45, 50.09it/s] 83%|████████▎ | 11373/13630 [05:50<00:46, 48.27it/s] 83%|████████▎ | 11379/13630 [05:50<00:50, 44.90it/s] 84%|████████▎ | 11384/13630 [05:50<00:49, 45.55it/s] 84%|████████▎ | 11389/13630 [05:50<00:48, 46.02it/s] 84%|████████▎ | 11398/13630 [05:50<00:39, 56.03it/s] 84%|████████▎ | 11404/13630 [05:51<00:46, 48.12it/s] 84%|████████▎ | 11410/13630 [05:51<00:54, 40.69it/s] 84%|████████▍ | 11419/13630 [05:51<00:44, 49.94it/s] 84%|████████▍ | 11425/13630 [05:51<00:43, 50.22it/s] 84%|████████▍ | 11431/13630 [05:51<00:57, 38.29it/s] 84%|████████▍ | 11437/13630 [05:51<00:52, 41.41it/s] 84%|████████▍ | 11444/13630 [05:52<00:46, 46.77it/s] 84%|████████▍ | 11450/13630 [05:52<00:55, 39.42it/s] 84%|████████▍ | 11455/13630 [05:52<00:57, 37.52it/s] 84%|████████▍ | 11462/13630 [05:52<00:49, 43.84it/s] 84%|████████▍ | 11470/13630 [05:52<00:42, 51.28it/s] 84%|████████▍ | 11476/13630 [05:52<00:41, 52.02it/s] 84%|████████▍ | 11484/13630 [05:52<00:36, 58.03it/s] 84%|████████▍ | 11491/13630 [05:53<00:43, 48.92it/s] 84%|████████▍ | 11499/13630 [05:53<00:38, 55.66it/s] 84%|████████▍ | 11506/13630 [05:53<00:38, 55.28it/s] 84%|████████▍ | 11513/13630 [05:53<00:37, 55.76it/s] 85%|████████▍ | 11519/13630 [05:53<00:38, 54.58it/s] 85%|████████▍ | 11525/13630 [05:53<00:39, 53.50it/s] 85%|████████▍ | 11534/13630 [05:53<00:33, 62.64it/s] 85%|████████▍ | 11541/13630 [05:53<00:35, 58.24it/s] 85%|████████▍ | 11549/13630 [05:54<00:36, 56.47it/s] 85%|████████▍ | 11556/13630 [05:54<00:38, 54.57it/s] 85%|████████▍ | 11562/13630 [05:54<00:42, 48.12it/s] 85%|████████▍ | 11567/13630 [05:54<00:47, 43.70it/s] 85%|████████▍ | 11572/13630 [05:54<00:51, 39.94it/s] 85%|████████▍ | 11577/13630 [05:54<00:54, 37.73it/s] 85%|████████▍ | 11581/13630 [05:54<00:58, 35.05it/s] 85%|████████▌ | 11586/13630 [05:55<00:53, 37.99it/s] 85%|████████▌ | 11591/13630 [05:55<00:50, 40.12it/s] 85%|████████▌ | 11598/13630 [05:55<00:45, 44.98it/s] 85%|████████▌ | 11605/13630 [05:55<00:39, 51.07it/s] 85%|████████▌ | 11611/13630 [05:55<00:47, 42.77it/s] 85%|████████▌ | 11616/13630 [05:55<00:48, 41.34it/s] 85%|████████▌ | 11622/13630 [05:55<00:44, 45.61it/s] 85%|████████▌ | 11627/13630 [05:55<00:44, 45.38it/s] 85%|████████▌ | 11632/13630 [05:56<00:43, 45.70it/s] 85%|████████▌ | 11637/13630 [05:56<00:55, 35.74it/s] 85%|████████▌ | 11645/13630 [05:56<00:43, 45.61it/s] 85%|████████▌ | 11651/13630 [05:56<00:55, 35.88it/s] 86%|████████▌ | 11657/13630 [05:56<00:49, 40.23it/s] 86%|████████▌ | 11663/13630 [05:56<00:44, 44.24it/s] 86%|████████▌ | 11669/13630 [05:56<00:42, 46.15it/s] 86%|████████▌ | 11675/13630 [05:57<00:43, 45.45it/s] 86%|████████▌ | 11683/13630 [05:57<00:39, 49.64it/s] 86%|████████▌ | 11689/13630 [05:57<00:43, 44.47it/s] 86%|████████▌ | 11694/13630 [05:57<00:42, 45.07it/s] 86%|████████▌ | 11700/13630 [05:57<00:40, 48.01it/s] 86%|████████▌ | 11707/13630 [05:57<00:37, 51.61it/s] 86%|████████▌ | 11715/13630 [05:57<00:32, 58.23it/s] 86%|████████▌ | 11723/13630 [05:57<00:30, 63.32it/s] 86%|████████▌ | 11730/13630 [05:58<00:30, 62.13it/s] 86%|████████▌ | 11737/13630 [05:58<00:30, 62.55it/s] 86%|████████▌ | 11745/13630 [05:58<00:28, 66.67it/s] 86%|████████▌ | 11752/13630 [05:58<00:29, 64.50it/s] 86%|████████▋ | 11760/13630 [05:58<00:27, 67.73it/s] 86%|████████▋ | 11767/13630 [05:58<00:29, 62.46it/s] 86%|████████▋ | 11774/13630 [05:58<00:30, 61.51it/s] 86%|████████▋ | 11781/13630 [05:58<00:31, 58.77it/s] 86%|████████▋ | 11787/13630 [05:59<00:36, 50.63it/s] 87%|████████▋ | 11795/13630 [05:59<00:31, 57.63it/s] 87%|████████▋ | 11802/13630 [05:59<00:31, 57.90it/s] 87%|████████▋ | 11809/13630 [05:59<00:31, 57.44it/s] 87%|████████▋ | 11816/13630 [05:59<00:30, 59.21it/s] 87%|████████▋ | 11823/13630 [05:59<00:31, 57.21it/s] 87%|████████▋ | 11830/13630 [05:59<00:30, 59.96it/s] 87%|████████▋ | 11837/13630 [05:59<00:41, 43.30it/s] 87%|████████▋ | 11843/13630 [06:00<00:39, 45.39it/s] 87%|████████▋ | 11850/13630 [06:00<00:35, 50.33it/s] 87%|████████▋ | 11858/13630 [06:00<00:32, 53.81it/s] 87%|████████▋ | 11864/13630 [06:00<00:32, 54.90it/s] 87%|████████▋ | 11872/13630 [06:00<00:31, 55.40it/s] 87%|████████▋ | 11879/13630 [06:00<00:30, 57.60it/s] 87%|████████▋ | 11886/13630 [06:00<00:28, 60.70it/s] 87%|████████▋ | 11893/13630 [06:00<00:29, 57.91it/s] 87%|████████▋ | 11899/13630 [06:01<00:30, 56.82it/s] 87%|████████▋ | 11905/13630 [06:01<00:34, 50.71it/s] 87%|████████▋ | 11911/13630 [06:01<00:36, 47.42it/s] 87%|████████▋ | 11916/13630 [06:01<00:36, 46.95it/s] 87%|████████▋ | 11921/13630 [06:01<00:35, 47.59it/s] 87%|████████▋ | 11926/13630 [06:01<00:35, 47.72it/s] 88%|████████▊ | 11934/13630 [06:01<00:30, 55.85it/s] 88%|████████▊ | 11943/13630 [06:01<00:28, 58.76it/s] 88%|████████▊ | 11949/13630 [06:01<00:29, 56.26it/s] 88%|████████▊ | 11955/13630 [06:02<00:29, 57.06it/s] 88%|████████▊ | 11962/13630 [06:02<00:28, 57.75it/s] 88%|████████▊ | 11968/13630 [06:02<00:34, 48.18it/s] 88%|████████▊ | 11975/13630 [06:02<00:31, 52.15it/s] 88%|████████▊ | 11981/13630 [06:02<00:30, 53.89it/s] 88%|████████▊ | 11989/13630 [06:02<00:27, 60.04it/s] 88%|████████▊ | 11998/13630 [06:02<00:24, 66.44it/s] 88%|████████▊ | 12005/13630 [06:02<00:25, 64.87it/s] 88%|████████▊ | 12012/13630 [06:03<00:29, 53.94it/s] 88%|████████▊ | 12020/13630 [06:03<00:26, 59.78it/s] 88%|████████▊ | 12027/13630 [06:03<00:29, 54.26it/s] 88%|████████▊ | 12035/13630 [06:03<00:26, 60.01it/s] 88%|████████▊ | 12042/13630 [06:03<00:26, 60.31it/s] 88%|████████▊ | 12049/13630 [06:03<00:26, 59.23it/s] 88%|████████▊ | 12057/13630 [06:03<00:24, 63.66it/s] 89%|████████▊ | 12066/13630 [06:03<00:22, 68.29it/s] 89%|████████▊ | 12073/13630 [06:04<00:24, 64.39it/s] 89%|████████▊ | 12082/13630 [06:04<00:24, 63.51it/s] 89%|████████▊ | 12091/13630 [06:04<00:22, 68.91it/s] 89%|████████▉ | 12100/13630 [06:04<00:20, 73.43it/s] 89%|████████▉ | 12108/13630 [06:04<00:22, 68.13it/s] 89%|████████▉ | 12115/13630 [06:04<00:24, 60.73it/s] 89%|████████▉ | 12122/13630 [06:04<00:30, 48.77it/s] 89%|████████▉ | 12129/13630 [06:05<00:28, 52.74it/s] 89%|████████▉ | 12135/13630 [06:05<00:29, 50.11it/s] 89%|████████▉ | 12141/13630 [06:05<00:30, 49.25it/s] 89%|████████▉ | 12148/13630 [06:05<00:28, 52.40it/s] 89%|████████▉ | 12156/13630 [06:05<00:25, 58.91it/s] 89%|████████▉ | 12163/13630 [06:05<00:24, 60.08it/s] 89%|████████▉ | 12170/13630 [06:05<00:29, 49.11it/s] 89%|████████▉ | 12178/13630 [06:05<00:26, 54.71it/s] 89%|████████▉ | 12185/13630 [06:06<00:24, 58.27it/s] 89%|████████▉ | 12193/13630 [06:06<00:22, 63.46it/s] 90%|████████▉ | 12200/13630 [06:06<00:29, 48.45it/s] 90%|████████▉ | 12206/13630 [06:06<00:31, 44.69it/s] 90%|████████▉ | 12214/13630 [06:06<00:27, 52.15it/s] 90%|████████▉ | 12220/13630 [06:06<00:30, 46.73it/s] 90%|████████▉ | 12228/13630 [06:06<00:26, 53.77it/s] 90%|████████▉ | 12234/13630 [06:07<00:27, 51.50it/s] 90%|████████▉ | 12240/13630 [06:07<00:28, 49.30it/s] 90%|████████▉ | 12246/13630 [06:07<00:29, 47.11it/s] 90%|████████▉ | 12252/13630 [06:07<00:27, 50.19it/s] 90%|████████▉ | 12260/13630 [06:07<00:23, 57.41it/s] 90%|█████████ | 12267/13630 [06:07<00:24, 55.44it/s] 90%|█████████ | 12273/13630 [06:07<00:26, 50.40it/s] 90%|█████████ | 12279/13630 [06:07<00:26, 50.70it/s] 90%|█████████ | 12288/13630 [06:08<00:22, 58.77it/s] 90%|█████████ | 12295/13630 [06:08<00:21, 61.09it/s] 90%|█████████ | 12302/13630 [06:08<00:22, 60.04it/s] 90%|█████████ | 12309/13630 [06:08<00:24, 52.88it/s] 90%|█████████ | 12316/13630 [06:08<00:25, 52.49it/s] 90%|█████████ | 12322/13630 [06:08<00:24, 52.34it/s] 90%|█████████ | 12328/13630 [06:08<00:24, 53.84it/s] 90%|█████████ | 12335/13630 [06:08<00:23, 56.21it/s] 91%|█████████ | 12341/13630 [06:09<00:23, 55.67it/s] 91%|█████████ | 12350/13630 [06:09<00:20, 63.41it/s] 91%|█████████ | 12357/13630 [06:09<00:23, 54.38it/s] 91%|█████████ | 12363/13630 [06:09<00:25, 49.95it/s] 91%|█████████ | 12369/13630 [06:09<00:31, 40.63it/s] 91%|█████████ | 12376/13630 [06:09<00:27, 45.02it/s] 91%|█████████ | 12381/13630 [06:09<00:27, 44.86it/s] 91%|█████████ | 12390/13630 [06:09<00:22, 54.75it/s] 91%|█████████ | 12396/13630 [06:10<00:25, 49.18it/s] 91%|█████████ | 12402/13630 [06:10<00:26, 45.60it/s] 91%|█████████ | 12410/13630 [06:10<00:22, 53.46it/s] 91%|█████████ | 12417/13630 [06:10<00:22, 54.02it/s] 91%|█████████ | 12423/13630 [06:10<00:24, 48.78it/s] 91%|█████████ | 12431/13630 [06:10<00:21, 55.77it/s] 91%|█████████ | 12437/13630 [06:10<00:21, 55.35it/s] 91%|█████████▏| 12444/13630 [06:11<00:21, 54.87it/s] 91%|█████████▏| 12450/13630 [06:11<00:23, 49.90it/s] 91%|█████████▏| 12457/13630 [06:11<00:21, 53.80it/s] 91%|█████████▏| 12463/13630 [06:11<00:21, 54.02it/s] 92%|█████████▏| 12472/13630 [06:11<00:18, 62.52it/s] 92%|█████████▏| 12480/13630 [06:11<00:17, 66.86it/s] 92%|█████████▏| 12487/13630 [06:11<00:19, 58.74it/s] 92%|█████████▏| 12494/13630 [06:12<00:26, 42.10it/s] 92%|█████████▏| 12500/13630 [06:12<00:26, 43.41it/s] 92%|█████████▏| 12505/13630 [06:12<00:26, 42.50it/s] 92%|█████████▏| 12511/13630 [06:12<00:24, 45.32it/s] 92%|█████████▏| 12519/13630 [06:12<00:21, 52.26it/s] 92%|█████████▏| 12525/13630 [06:12<00:20, 53.45it/s] 92%|█████████▏| 12531/13630 [06:12<00:21, 51.37it/s] 92%|█████████▏| 12537/13630 [06:12<00:21, 50.45it/s] 92%|█████████▏| 12543/13630 [06:13<00:22, 48.10it/s] 92%|█████████▏| 12548/13630 [06:13<00:23, 45.35it/s] 92%|█████████▏| 12553/13630 [06:13<00:23, 45.51it/s] 92%|█████████▏| 12558/13630 [06:13<00:23, 45.52it/s] 92%|█████████▏| 12565/13630 [06:13<00:20, 50.76it/s] 92%|█████████▏| 12571/13630 [06:13<00:21, 49.31it/s] 92%|█████████▏| 12577/13630 [06:13<00:21, 48.14it/s] 92%|█████████▏| 12584/13630 [06:13<00:19, 53.03it/s] 92%|█████████▏| 12592/13630 [06:13<00:17, 58.45it/s] 92%|█████████▏| 12598/13630 [06:14<00:18, 55.79it/s] 92%|█████████▏| 12604/13630 [06:14<00:18, 56.12it/s] 93%|█████████▎| 12610/13630 [06:14<00:18, 55.53it/s] 93%|█████████▎| 12616/13630 [06:14<00:19, 52.89it/s] 93%|█████████▎| 12622/13630 [06:14<00:23, 42.28it/s] 93%|█████████▎| 12627/13630 [06:14<00:24, 40.84it/s] 93%|█████████▎| 12632/13630 [06:14<00:25, 39.15it/s] 93%|█████████▎| 12637/13630 [06:15<00:24, 41.14it/s] 93%|█████████▎| 12642/13630 [06:15<00:27, 36.31it/s] 93%|█████████▎| 12649/13630 [06:15<00:23, 41.98it/s] 93%|█████████▎| 12655/13630 [06:15<00:21, 44.71it/s] 93%|█████████▎| 12660/13630 [06:15<00:22, 42.50it/s] 93%|█████████▎| 12665/13630 [06:15<00:26, 36.63it/s] 93%|█████████▎| 12671/13630 [06:15<00:23, 41.20it/s] 93%|█████████▎| 12679/13630 [06:15<00:19, 48.93it/s] 93%|█████████▎| 12687/13630 [06:16<00:17, 55.20it/s] 93%|█████████▎| 12694/13630 [06:16<00:16, 58.13it/s] 93%|█████████▎| 12701/13630 [06:16<00:15, 58.26it/s] 93%|█████████▎| 12709/13630 [06:16<00:14, 63.02it/s] 93%|█████████▎| 12716/13630 [06:16<00:16, 55.64it/s] 93%|█████████▎| 12722/13630 [06:16<00:19, 47.71it/s] 93%|█████████▎| 12729/13630 [06:16<00:17, 51.69it/s] 93%|█████████▎| 12736/13630 [06:16<00:16, 55.62it/s] 93%|█████████▎| 12742/13630 [06:17<00:19, 45.02it/s] 94%|█████████▎| 12747/13630 [06:17<00:20, 43.98it/s] 94%|█████████▎| 12756/13630 [06:17<00:16, 53.44it/s] 94%|█████████▎| 12762/13630 [06:17<00:16, 52.81it/s] 94%|█████████▎| 12770/13630 [06:17<00:16, 51.99it/s] 94%|█████████▍| 12779/13630 [06:17<00:14, 59.21it/s] 94%|█████████▍| 12786/13630 [06:17<00:14, 58.38it/s] 94%|█████████▍| 12793/13630 [06:18<00:13, 59.92it/s] 94%|█████████▍| 12800/13630 [06:18<00:15, 53.89it/s] 94%|█████████▍| 12806/13630 [06:18<00:15, 54.05it/s] 94%|█████████▍| 12812/13630 [06:18<00:15, 53.30it/s] 94%|█████████▍| 12818/13630 [06:18<00:15, 53.85it/s] 94%|█████████▍| 12826/13630 [06:18<00:13, 59.19it/s] 94%|█████████▍| 12833/13630 [06:18<00:13, 58.33it/s] 94%|█████████▍| 12842/13630 [06:18<00:13, 58.23it/s] 94%|█████████▍| 12848/13630 [06:19<00:13, 56.17it/s] 94%|█████████▍| 12855/13630 [06:19<00:13, 58.43it/s] 94%|█████████▍| 12861/13630 [06:19<00:14, 52.96it/s] 94%|█████████▍| 12870/13630 [06:19<00:12, 60.93it/s] 94%|█████████▍| 12877/13630 [06:19<00:13, 57.78it/s] 95%|█████████▍| 12883/13630 [06:19<00:24, 30.33it/s] 95%|█████████▍| 12888/13630 [06:20<00:23, 31.52it/s] 95%|█████████▍| 12893/13630 [06:20<00:21, 34.46it/s] 95%|█████████▍| 12898/13630 [06:20<00:20, 36.51it/s] 95%|█████████▍| 12906/13630 [06:20<00:16, 45.12it/s] 95%|█████████▍| 12912/13630 [06:20<00:15, 45.58it/s] 95%|█████████▍| 12918/13630 [06:20<00:14, 47.52it/s] 95%|█████████▍| 12924/13630 [06:20<00:15, 45.00it/s] 95%|█████████▍| 12932/13630 [06:20<00:13, 53.08it/s] 95%|█████████▍| 12938/13630 [06:21<00:13, 52.40it/s] 95%|█████████▍| 12945/13630 [06:21<00:12, 55.71it/s] 95%|█████████▌| 12953/13630 [06:21<00:10, 61.68it/s] 95%|█████████▌| 12961/13630 [06:21<00:10, 66.21it/s] 95%|█████████▌| 12969/13630 [06:21<00:09, 69.42it/s] 95%|█████████▌| 12977/13630 [06:21<00:10, 62.25it/s] 95%|█████████▌| 12984/13630 [06:21<00:10, 63.40it/s] 95%|█████████▌| 12991/13630 [06:21<00:10, 63.39it/s] 95%|█████████▌| 12998/13630 [06:22<00:11, 53.70it/s] 95%|█████████▌| 13006/13630 [06:22<00:10, 59.48it/s] 95%|█████████▌| 13015/13630 [06:22<00:10, 61.34it/s] 96%|█████████▌| 13022/13630 [06:22<00:09, 61.56it/s] 96%|█████████▌| 13029/13630 [06:22<00:12, 48.11it/s] 96%|█████████▌| 13035/13630 [06:22<00:13, 45.48it/s] 96%|█████████▌| 13041/13630 [06:22<00:12, 47.27it/s] 96%|█████████▌| 13047/13630 [06:23<00:13, 42.40it/s] 96%|█████████▌| 13052/13630 [06:23<00:14, 39.78it/s] 96%|█████████▌| 13059/13630 [06:23<00:12, 45.90it/s] 96%|█████████▌| 13068/13630 [06:23<00:10, 55.18it/s] 96%|█████████▌| 13075/13630 [06:23<00:09, 57.48it/s] 96%|█████████▌| 13082/13630 [06:23<00:11, 46.17it/s] 96%|█████████▌| 13090/13630 [06:23<00:10, 53.46it/s] 96%|█████████▌| 13097/13630 [06:23<00:09, 55.87it/s] 96%|█████████▌| 13104/13630 [06:24<00:09, 53.70it/s] 96%|█████████▌| 13110/13630 [06:24<00:10, 51.79it/s] 96%|█████████▌| 13118/13630 [06:24<00:08, 57.00it/s] 96%|█████████▋| 13125/13630 [06:24<00:08, 60.19it/s] 96%|█████████▋| 13133/13630 [06:24<00:08, 59.94it/s] 96%|█████████▋| 13141/13630 [06:24<00:07, 64.06it/s] 96%|█████████▋| 13149/13630 [06:24<00:07, 67.15it/s] 97%|█████████▋| 13156/13630 [06:24<00:08, 57.39it/s] 97%|█████████▋| 13163/13630 [06:25<00:08, 57.55it/s] 97%|█████████▋| 13169/13630 [06:25<00:08, 53.29it/s] 97%|█████████▋| 13177/13630 [06:25<00:08, 53.89it/s] 97%|█████████▋| 13183/13630 [06:25<00:08, 51.98it/s] 97%|█████████▋| 13189/13630 [06:25<00:08, 52.94it/s] 97%|█████████▋| 13195/13630 [06:25<00:08, 52.17it/s] 97%|█████████▋| 13201/13630 [06:25<00:08, 50.47it/s] 97%|█████████▋| 13209/13630 [06:25<00:08, 52.51it/s] 97%|█████████▋| 13215/13630 [06:26<00:07, 52.07it/s] 97%|█████████▋| 13225/13630 [06:26<00:06, 58.07it/s] 97%|█████████▋| 13231/13630 [06:26<00:07, 53.43it/s] 97%|█████████▋| 13237/13630 [06:26<00:07, 52.78it/s] 97%|█████████▋| 13244/13630 [06:26<00:06, 56.15it/s] 97%|█████████▋| 13252/13630 [06:26<00:06, 56.33it/s] 97%|█████████▋| 13260/13630 [06:26<00:06, 61.62it/s] 97%|█████████▋| 13268/13630 [06:26<00:05, 66.16it/s] 97%|█████████▋| 13275/13630 [06:27<00:06, 54.33it/s] 97%|█████████▋| 13281/13630 [06:27<00:06, 54.51it/s] 97%|█████████▋| 13287/13630 [06:27<00:06, 54.46it/s] 98%|█████████▊| 13293/13630 [06:27<00:07, 46.38it/s] 98%|█████████▊| 13300/13630 [06:27<00:06, 50.71it/s] 98%|█████████▊| 13307/13630 [06:27<00:05, 53.97it/s] 98%|█████████▊| 13315/13630 [06:27<00:05, 60.52it/s] 98%|█████████▊| 13324/13630 [06:27<00:04, 67.20it/s] 98%|█████████▊| 13331/13630 [06:28<00:04, 63.90it/s] 98%|█████████▊| 13338/13630 [06:28<00:04, 64.95it/s] 98%|█████████▊| 13347/13630 [06:28<00:04, 70.00it/s] 98%|█████████▊| 13355/13630 [06:28<00:04, 66.90it/s] 98%|█████████▊| 13362/13630 [06:28<00:04, 63.02it/s] 98%|█████████▊| 13369/13630 [06:28<00:04, 60.35it/s] 98%|█████████▊| 13376/13630 [06:28<00:04, 54.39it/s] 98%|█████████▊| 13382/13630 [06:29<00:05, 49.02it/s] 98%|█████████▊| 13391/13630 [06:29<00:04, 57.87it/s] 98%|█████████▊| 13399/13630 [06:29<00:03, 62.70it/s] 98%|█████████▊| 13406/13630 [06:29<00:03, 64.38it/s] 98%|█████████▊| 13415/13630 [06:29<00:03, 69.56it/s] 98%|█████████▊| 13424/13630 [06:29<00:03, 68.17it/s] 99%|█████████▊| 13431/13630 [06:29<00:03, 53.71it/s] 99%|█████████▊| 13439/13630 [06:29<00:03, 55.05it/s] 99%|█████████▊| 13445/13630 [06:30<00:03, 55.25it/s] 99%|█████████▊| 13451/13630 [06:30<00:03, 55.90it/s] 99%|█████████▊| 13458/13630 [06:30<00:02, 57.61it/s] 99%|█████████▉| 13464/13630 [06:30<00:02, 56.68it/s] 99%|█████████▉| 13470/13630 [06:30<00:02, 57.54it/s] 99%|█████████▉| 13477/13630 [06:30<00:02, 57.41it/s] 99%|█████████▉| 13483/13630 [06:30<00:04, 32.25it/s] 99%|█████████▉| 13488/13630 [06:31<00:04, 35.26it/s] 99%|█████████▉| 13493/13630 [06:31<00:03, 36.21it/s] 99%|█████████▉| 13498/13630 [06:31<00:04, 31.97it/s] 99%|█████████▉| 13503/13630 [06:31<00:03, 35.05it/s] 99%|█████████▉| 13510/13630 [06:31<00:03, 38.18it/s] 99%|█████████▉| 13516/13630 [06:31<00:03, 37.16it/s] 99%|█████████▉| 13520/13630 [06:31<00:02, 36.74it/s] 99%|█████████▉| 13525/13630 [06:32<00:02, 38.27it/s] 99%|█████████▉| 13531/13630 [06:32<00:02, 38.58it/s] 99%|█████████▉| 13537/13630 [06:32<00:02, 42.77it/s] 99%|█████████▉| 13543/13630 [06:32<00:02, 41.74it/s] 99%|█████████▉| 13549/13630 [06:32<00:01, 45.63it/s] 99%|█████████▉| 13556/13630 [06:32<00:01, 51.38it/s]100%|█████████▉| 13562/13630 [06:32<00:01, 49.16it/s]100%|█████████▉| 13568/13630 [06:32<00:01, 47.52it/s]100%|█████████▉| 13573/13630 [06:33<00:01, 47.95it/s]100%|█████████▉| 13581/13630 [06:33<00:00, 54.97it/s]100%|█████████▉| 13589/13630 [06:33<00:00, 59.02it/s]100%|█████████▉| 13595/13630 [06:33<00:00, 58.82it/s]100%|█████████▉| 13601/13630 [06:33<00:00, 51.44it/s]100%|█████████▉| 13607/13630 [06:33<00:00, 52.53it/s]100%|█████████▉| 13613/13630 [06:33<00:00, 53.18it/s]100%|█████████▉| 13621/13630 [06:33<00:00, 59.33it/s]100%|█████████▉| 13628/13630 [06:34<00:00, 53.53it/s]100%|██████████| 13630/13630 [06:34<00:00, 34.59it/s]
Train images: 34522 Validation images: 9914
__TRAINING STATS__
Counter({1.0: 17842, 0.0: 16680})
Weights 0.9348727721107499
__VALIDATION STATS__
Counter({1: 5124, 0: 4790})
___________________
=-=-=-=-=-=
EfficientNextViT(
  (efficient_net): EfficientNet(
    (_conv_stem): Conv2dStaticSamePadding(
      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False
      (static_padding): ZeroPad2d((0, 1, 0, 1))
    )
    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
    (_blocks): ModuleList(
      (0): MBConvBlock(
        (_depthwise_conv): Conv2dStaticSamePadding(
          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False
          (static_padding): ZeroPad2d((1, 1, 1, 1))
        )
        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          32, 8, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          8, 32, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (1): MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False
          (static_padding): ZeroPad2d((0, 1, 0, 1))
        )
        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          96, 4, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          4, 96, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (2): MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False
          (static_padding): ZeroPad2d((1, 1, 1, 1))
        )
        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          144, 6, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          6, 144, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (3): MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False
          (static_padding): ZeroPad2d((1, 2, 1, 2))
        )
        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          144, 6, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          6, 144, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (4): MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False
          (static_padding): ZeroPad2d((2, 2, 2, 2))
        )
        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          240, 10, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          10, 240, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (5): MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False
          (static_padding): ZeroPad2d((0, 1, 0, 1))
        )
        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          240, 10, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          10, 240, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (6-7): 2 x MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False
          (static_padding): ZeroPad2d((1, 1, 1, 1))
        )
        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          480, 20, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          20, 480, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (8): MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False
          (static_padding): ZeroPad2d((2, 2, 2, 2))
        )
        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          480, 20, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          20, 480, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (9-10): 2 x MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False
          (static_padding): ZeroPad2d((2, 2, 2, 2))
        )
        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          672, 28, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          28, 672, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (11): MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False
          (static_padding): ZeroPad2d((1, 2, 1, 2))
        )
        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          672, 28, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          28, 672, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (12-14): 3 x MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False
          (static_padding): ZeroPad2d((2, 2, 2, 2))
        )
        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          1152, 48, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          48, 1152, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
      (15): MBConvBlock(
        (_expand_conv): Conv2dStaticSamePadding(
          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_depthwise_conv): Conv2dStaticSamePadding(
          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False
          (static_padding): ZeroPad2d((1, 1, 1, 1))
        )
        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_se_reduce): Conv2dStaticSamePadding(
          1152, 48, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_se_expand): Conv2dStaticSamePadding(
          48, 1152, kernel_size=(1, 1), stride=(1, 1)
          (static_padding): Identity()
        )
        (_project_conv): Conv2dStaticSamePadding(
          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False
          (static_padding): Identity()
        )
        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
        (_swish): MemoryEfficientSwish()
      )
    )
    (_conv_head): Conv2dStaticSamePadding(
      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False
      (static_padding): Identity()
    )
    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)
    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)
    (_dropout): Dropout(p=0.2, inplace=False)
    (_fc): Linear(in_features=1280, out_features=1000, bias=True)
    (_swish): MemoryEfficientSwish()
  )
  (next_vit): NextViT(
    (stem): Sequential(
      (0): Conv2d(1280, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (features): ModuleList(
      (0-1): 2 x Sequential(
        (0): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
        (1): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
      )
      (2): Sequential(
        (0): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
        (1): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
        (2): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
        (3): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
        (4): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
        (5): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
      )
      (3): Sequential(
        (0): NCB(
          (patch_embed): PatchEmbed(
            (avgpool): Identity()
            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (drop_path1): Identity()
          (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(640, 2560, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path2): Identity()
        )
        (1): NTB(
          (conv1): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (e_mhsa): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=640, out_features=640, bias=True)
          )
          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)
          (conv2): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (mhca): MHCA(
            (group_conv3x3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)
            (norm): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (act): ReLU(inplace=True)
            (projection): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
          (norm3): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (mlp): Mlp(
            (fc1): Conv2d(1280, 5120, kernel_size=(1, 1), stride=(1, 1))
            (act): ReLU(inplace=True)
            (fc2): Conv2d(5120, 640, kernel_size=(1, 1), stride=(1, 1))
            (drop): Dropout(p=0.0, inplace=False)
          )
          (drop_path): Identity()
        )
      )
    )
    (head): Sequential(
      (0): AdaptiveAvgPool2d(output_size=1)
      (1): Flatten(start_dim=1, end_dim=-1)
      (2): Linear(in_features=640, out_features=1, bias=True)
    )
  )
)
=-=-=-=-=-=
  0%|          | 0/50 [00:00<?, ?it/s]/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
  2%|▏         | 1/50 [02:03<1:40:39, 123.25s/it]#1/50 loss:0.6264 accuracy:0.638 val_loss:0.5843 val_accuracy:0.6885 val_0s:4710/4790 val_1s:5204/5124
train_loss_list: [0.6264]
val_loss_list: [0.5843]
lr_list: [0.001]
train_f1_score_list: [0.6601]
val_f1_score_list: [0.701]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
  4%|▍         | 2/50 [04:09<1:39:54, 124.89s/it]#2/50 loss:0.5705 accuracy:0.6917 val_loss:0.5559 val_accuracy:0.7175 val_0s:4859/4790 val_1s:5055/5124
train_loss_list: [0.6264, 0.5705]
val_loss_list: [0.5843, 0.5559]
lr_list: [0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098]
val_f1_score_list: [0.701, 0.7248]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
  6%|▌         | 3/50 [06:16<1:38:29, 125.74s/it]#3/50 loss:0.5459 accuracy:0.7104 val_loss:0.5369 val_accuracy:0.7354 val_0s:4405/4790 val_1s:5509/5124
train_loss_list: [0.6264, 0.5705, 0.5459]
val_loss_list: [0.5843, 0.5559, 0.5369]
lr_list: [0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257]
val_f1_score_list: [0.701, 0.7248, 0.7533]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
  8%|▊         | 4/50 [08:21<1:36:12, 125.49s/it]#4/50 loss:0.5279 accuracy:0.7221 val_loss:0.5355 val_accuracy:0.7314 val_0s:4587/4790 val_1s:5327/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355]
lr_list: [0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 10%|█         | 5/50 [10:26<1:33:58, 125.30s/it]#5/50 loss:0.5106 accuracy:0.7319 val_loss:0.5135 val_accuracy:0.7448 val_0s:4654/4790 val_1s:5260/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 12%|█▏        | 6/50 [12:32<1:32:11, 125.72s/it]#6/50 loss:0.4927 accuracy:0.7459 val_loss:0.5038 val_accuracy:0.7551 val_0s:4370/4790 val_1s:5544/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 14%|█▍        | 7/50 [14:40<1:30:29, 126.27s/it]#7/50 loss:0.4848 accuracy:0.7478 val_loss:0.4916 val_accuracy:0.7594 val_0s:4447/4790 val_1s:5467/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 16%|█▌        | 8/50 [16:46<1:28:26, 126.35s/it]Validation loss did not improved
#8/50 loss:0.4756 accuracy:0.7556 val_loss:0.4951 val_accuracy:0.7574 val_0s:4659/4790 val_1s:5255/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 18%|█▊        | 9/50 [18:53<1:26:29, 126.57s/it]#9/50 loss:0.4654 accuracy:0.7597 val_loss:0.4938 val_accuracy:0.7623 val_0s:4549/4790 val_1s:5365/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 20%|██        | 10/50 [21:00<1:24:24, 126.61s/it]#10/50 loss:0.4602 accuracy:0.7656 val_loss:0.4863 val_accuracy:0.768 val_0s:4484/4790 val_1s:5430/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 22%|██▏       | 11/50 [23:07<1:22:23, 126.75s/it]#11/50 loss:0.4511 accuracy:0.7686 val_loss:0.4707 val_accuracy:0.7758 val_0s:4525/4790 val_1s:5389/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 24%|██▍       | 12/50 [25:14<1:20:17, 126.77s/it]#12/50 loss:0.4448 accuracy:0.7735 val_loss:0.4579 val_accuracy:0.7798 val_0s:4657/4790 val_1s:5257/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 26%|██▌       | 13/50 [27:20<1:18:08, 126.70s/it]Validation loss did not improved
#13/50 loss:0.4347 accuracy:0.7802 val_loss:0.4618 val_accuracy:0.7843 val_0s:4410/4790 val_1s:5504/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 28%|██▊       | 14/50 [29:24<1:15:23, 125.66s/it]#14/50 loss:0.4263 accuracy:0.7846 val_loss:0.4506 val_accuracy:0.7887 val_0s:4655/4790 val_1s:5259/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 30%|███       | 15/50 [31:18<1:11:23, 122.40s/it]#15/50 loss:0.4234 accuracy:0.7865 val_loss:0.4499 val_accuracy:0.7885 val_0s:4697/4790 val_1s:5217/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 32%|███▏      | 16/50 [33:13<1:08:04, 120.14s/it]#16/50 loss:0.4204 accuracy:0.7892 val_loss:0.4456 val_accuracy:0.7895 val_0s:4635/4790 val_1s:5279/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 34%|███▍      | 17/50 [35:09<1:05:17, 118.70s/it]#17/50 loss:0.4116 accuracy:0.7935 val_loss:0.4446 val_accuracy:0.7932 val_0s:4596/4790 val_1s:5318/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 36%|███▌      | 18/50 [37:04<1:02:44, 117.65s/it]#18/50 loss:0.4102 accuracy:0.7932 val_loss:0.4419 val_accuracy:0.7925 val_0s:4545/4790 val_1s:5369/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 38%|███▊      | 19/50 [38:59<1:00:28, 117.04s/it]Validation loss did not improved
#19/50 loss:0.4029 accuracy:0.7965 val_loss:0.4539 val_accuracy:0.7818 val_0s:5079/4790 val_1s:4835/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 40%|████      | 20/50 [40:55<58:19, 116.65s/it]  #20/50 loss:0.4003 accuracy:0.7978 val_loss:0.4362 val_accuracy:0.7983 val_0s:4688/4790 val_1s:5226/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 42%|████▏     | 21/50 [42:50<56:09, 116.18s/it]#21/50 loss:0.3976 accuracy:0.8004 val_loss:0.4327 val_accuracy:0.7995 val_0s:4752/4790 val_1s:5162/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 44%|████▍     | 22/50 [44:47<54:16, 116.31s/it]#22/50 loss:0.3922 accuracy:0.8033 val_loss:0.4315 val_accuracy:0.7998 val_0s:4631/4790 val_1s:5283/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 46%|████▌     | 23/50 [46:54<53:49, 119.59s/it]Validation loss did not improved
#23/50 loss:0.3886 accuracy:0.8044 val_loss:0.4348 val_accuracy:0.7962 val_0s:4558/4790 val_1s:5356/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 48%|████▊     | 24/50 [49:14<54:31, 125.83s/it]#24/50 loss:0.3858 accuracy:0.8066 val_loss:0.429 val_accuracy:0.8002 val_0s:4647/4790 val_1s:5267/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 50%|█████     | 25/50 [51:37<54:29, 130.79s/it]#25/50 loss:0.3841 accuracy:0.8088 val_loss:0.4201 val_accuracy:0.8046 val_0s:4715/4790 val_1s:5199/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 52%|█████▏    | 26/50 [54:05<54:25, 136.06s/it]Validation loss did not improved
#26/50 loss:0.3764 accuracy:0.8111 val_loss:0.4266 val_accuracy:0.8025 val_0s:4518/4790 val_1s:5396/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 54%|█████▍    | 27/50 [56:33<53:31, 139.63s/it]#27/50 loss:0.3742 accuracy:0.8098 val_loss:0.4213 val_accuracy:0.8054 val_0s:4661/4790 val_1s:5253/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 56%|█████▌    | 28/50 [59:00<51:56, 141.65s/it]Validation loss did not improved
#28/50 loss:0.373 accuracy:0.8114 val_loss:0.424 val_accuracy:0.8051 val_0s:4782/4790 val_1s:5132/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 58%|█████▊    | 29/50 [1:01:56<53:13, 152.07s/it]#29/50 loss:0.3677 accuracy:0.8162 val_loss:0.4194 val_accuracy:0.8068 val_0s:4743/4790 val_1s:5171/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 60%|██████    | 30/50 [1:05:03<54:10, 162.55s/it]Validation loss did not improved
#30/50 loss:0.3698 accuracy:0.8108 val_loss:0.4195 val_accuracy:0.8031 val_0s:4526/4790 val_1s:5388/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 62%|██████▏   | 31/50 [1:08:50<57:35, 181.87s/it]Validation loss did not improved
#31/50 loss:0.3676 accuracy:0.8144 val_loss:0.4217 val_accuracy:0.808 val_0s:4753/4790 val_1s:5161/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 64%|██████▍   | 32/50 [1:12:23<57:21, 191.19s/it]#32/50 loss:0.3623 accuracy:0.8185 val_loss:0.4075 val_accuracy:0.8115 val_0s:4691/4790 val_1s:5223/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 66%|██████▌   | 33/50 [1:15:05<51:42, 182.49s/it]Validation loss did not improved
#33/50 loss:0.3604 accuracy:0.8178 val_loss:0.4155 val_accuracy:0.8066 val_0s:4505/4790 val_1s:5409/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 68%|██████▊   | 34/50 [1:17:40<46:28, 174.29s/it]#34/50 loss:0.3568 accuracy:0.8215 val_loss:0.3977 val_accuracy:0.821 val_0s:4553/4790 val_1s:5361/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 70%|███████   | 35/50 [1:20:13<41:56, 167.79s/it]Validation loss did not improved
#35/50 loss:0.3515 accuracy:0.8244 val_loss:0.4202 val_accuracy:0.8093 val_0s:4501/4790 val_1s:5413/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 72%|███████▏  | 36/50 [1:22:45<38:02, 163.03s/it]#36/50 loss:0.3562 accuracy:0.8203 val_loss:0.4119 val_accuracy:0.8132 val_0s:4578/4790 val_1s:5336/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 74%|███████▍  | 37/50 [1:25:20<34:50, 160.80s/it]#37/50 loss:0.3525 accuracy:0.8229 val_loss:0.3977 val_accuracy:0.8202 val_0s:4507/4790 val_1s:5407/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 76%|███████▌  | 38/50 [1:27:52<31:38, 158.21s/it]Validation loss did not improved
#38/50 loss:0.3504 accuracy:0.8248 val_loss:0.4055 val_accuracy:0.8192 val_0s:4560/4790 val_1s:5354/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 78%|███████▊  | 39/50 [1:30:37<29:22, 160.23s/it]Validation loss did not improved
#39/50 loss:0.3488 accuracy:0.8245 val_loss:0.4095 val_accuracy:0.817 val_0s:4394/4790 val_1s:5520/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 80%|████████  | 40/50 [1:33:14<26:30, 159.01s/it]#40/50 loss:0.3446 accuracy:0.8262 val_loss:0.4035 val_accuracy:0.8122 val_0s:4602/4790 val_1s:5312/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 82%|████████▏ | 41/50 [1:35:59<24:08, 160.94s/it]Validation loss did not improved
#41/50 loss:0.344 accuracy:0.8276 val_loss:0.4084 val_accuracy:0.8143 val_0s:4585/4790 val_1s:5329/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 84%|████████▍ | 42/50 [1:38:48<21:46, 163.27s/it]#42/50 loss:0.3421 accuracy:0.8283 val_loss:0.4064 val_accuracy:0.812 val_0s:4600/4790 val_1s:5314/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 86%|████████▌ | 43/50 [1:41:17<18:32, 158.92s/it]#43/50 loss:0.3408 accuracy:0.8309 val_loss:0.4016 val_accuracy:0.8221 val_0s:4772/4790 val_1s:5142/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421, 0.3408]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064, 0.4016]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378, 0.84]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214, 0.8282]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 88%|████████▊ | 44/50 [1:43:48<15:39, 156.57s/it]Validation loss did not improved
#44/50 loss:0.3395 accuracy:0.8302 val_loss:0.4074 val_accuracy:0.8164 val_0s:4698/4790 val_1s:5216/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421, 0.3408, 0.3395]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064, 0.4016, 0.4074]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378, 0.84, 0.8395]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214, 0.8282, 0.824]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 90%|█████████ | 45/50 [1:46:23<13:00, 156.18s/it]#45/50 loss:0.3382 accuracy:0.8303 val_loss:0.3853 val_accuracy:0.8303 val_0s:4612/4790 val_1s:5302/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421, 0.3408, 0.3395, 0.3382]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064, 0.4016, 0.4074, 0.3853]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378, 0.84, 0.8395, 0.8395]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214, 0.8282, 0.824, 0.8387]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 92%|█████████▏| 46/50 [1:48:56<10:20, 155.21s/it]Validation loss did not improved
#46/50 loss:0.3367 accuracy:0.8313 val_loss:0.3857 val_accuracy:0.8264 val_0s:4555/4790 val_1s:5359/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421, 0.3408, 0.3395, 0.3382, 0.3367]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064, 0.4016, 0.4074, 0.3853, 0.3857]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378, 0.84, 0.8395, 0.8395, 0.8406]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214, 0.8282, 0.824, 0.8387, 0.8358]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 94%|█████████▍| 47/50 [1:51:28<07:43, 154.37s/it]Validation loss did not improved
#47/50 loss:0.3352 accuracy:0.8328 val_loss:0.3912 val_accuracy:0.8227 val_0s:4702/4790 val_1s:5212/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421, 0.3408, 0.3395, 0.3382, 0.3367, 0.3352]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064, 0.4016, 0.4074, 0.3853, 0.3857, 0.3912]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378, 0.84, 0.8395, 0.8395, 0.8406, 0.8417]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214, 0.8282, 0.824, 0.8387, 0.8358, 0.8299]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 96%|█████████▌| 48/50 [1:53:40<04:55, 147.62s/it]#48/50 loss:0.3287 accuracy:0.8356 val_loss:0.3909 val_accuracy:0.8208 val_0s:4555/4790 val_1s:5359/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421, 0.3408, 0.3395, 0.3382, 0.3367, 0.3352, 0.3287]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064, 0.4016, 0.4074, 0.3853, 0.3857, 0.3912, 0.3909]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378, 0.84, 0.8395, 0.8395, 0.8406, 0.8417, 0.8444]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214, 0.8282, 0.824, 0.8387, 0.8358, 0.8299, 0.8305]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
 98%|█████████▊| 49/50 [1:55:41<02:19, 139.55s/it]Validation loss did not improved
#49/50 loss:0.3271 accuracy:0.8356 val_loss:0.3938 val_accuracy:0.8258 val_0s:4793/4790 val_1s:5121/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421, 0.3408, 0.3395, 0.3382, 0.3367, 0.3352, 0.3287, 0.3271]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064, 0.4016, 0.4074, 0.3853, 0.3857, 0.3912, 0.3909, 0.3938]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378, 0.84, 0.8395, 0.8395, 0.8406, 0.8417, 0.8444, 0.8445]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214, 0.8282, 0.824, 0.8387, 0.8358, 0.8299, 0.8305, 0.8314]
/home/work/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
100%|██████████| 50/50 [1:57:41<00:00, 133.61s/it]100%|██████████| 50/50 [1:57:41<00:00, 141.22s/it]
#50/50 loss:0.3262 accuracy:0.839 val_loss:0.39 val_accuracy:0.8271 val_0s:4506/4790 val_1s:5408/5124
train_loss_list: [0.6264, 0.5705, 0.5459, 0.5279, 0.5106, 0.4927, 0.4848, 0.4756, 0.4654, 0.4602, 0.4511, 0.4448, 0.4347, 0.4263, 0.4234, 0.4204, 0.4116, 0.4102, 0.4029, 0.4003, 0.3976, 0.3922, 0.3886, 0.3858, 0.3841, 0.3764, 0.3742, 0.373, 0.3677, 0.3698, 0.3676, 0.3623, 0.3604, 0.3568, 0.3515, 0.3562, 0.3525, 0.3504, 0.3488, 0.3446, 0.344, 0.3421, 0.3408, 0.3395, 0.3382, 0.3367, 0.3352, 0.3287, 0.3271, 0.3262]
val_loss_list: [0.5843, 0.5559, 0.5369, 0.5355, 0.5135, 0.5038, 0.4916, 0.4951, 0.4938, 0.4863, 0.4707, 0.4579, 0.4618, 0.4506, 0.4499, 0.4456, 0.4446, 0.4419, 0.4539, 0.4362, 0.4327, 0.4315, 0.4348, 0.429, 0.4201, 0.4266, 0.4213, 0.424, 0.4194, 0.4195, 0.4217, 0.4075, 0.4155, 0.3977, 0.4202, 0.4119, 0.3977, 0.4055, 0.4095, 0.4035, 0.4084, 0.4064, 0.4016, 0.4074, 0.3853, 0.3857, 0.3912, 0.3909, 0.3938, 0.39]
lr_list: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]
train_f1_score_list: [0.6601, 0.7098, 0.7257, 0.7386, 0.7481, 0.7609, 0.7635, 0.771, 0.7741, 0.7802, 0.782, 0.7867, 0.7926, 0.7968, 0.7997, 0.8008, 0.8056, 0.8049, 0.8081, 0.8091, 0.8109, 0.8136, 0.8159, 0.8181, 0.8203, 0.8215, 0.8202, 0.8217, 0.826, 0.8208, 0.8246, 0.8278, 0.8277, 0.8312, 0.834, 0.8302, 0.832, 0.8338, 0.8344, 0.8356, 0.8371, 0.8378, 0.84, 0.8395, 0.8395, 0.8406, 0.8417, 0.8444, 0.8445, 0.8476]
val_f1_score_list: [0.701, 0.7248, 0.7533, 0.7452, 0.7564, 0.7724, 0.7748, 0.7683, 0.7753, 0.7821, 0.7885, 0.7897, 0.7988, 0.7982, 0.7972, 0.7994, 0.8037, 0.804, 0.7828, 0.8068, 0.8067, 0.8093, 0.8073, 0.8094, 0.8124, 0.8139, 0.8141, 0.8116, 0.814, 0.8143, 0.815, 0.8194, 0.818, 0.8307, 0.8205, 0.8229, 0.8307, 0.829, 0.8296, 0.8216, 0.8239, 0.8214, 0.8282, 0.824, 0.8387, 0.8358, 0.8299, 0.8305, 0.8314, 0.8373]
